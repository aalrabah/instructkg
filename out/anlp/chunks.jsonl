{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture1_Introduction__0001"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Dilek HakkaniT√ºr\n- -Prof @ UIUC and Amazon Scholar\n- -Senior Principal Scientist at Amazon Alexa AI and Visiting Professor at UC Santa Cruz\n- -Google Research, Microsoft Research, ICSI @ UC Berkeley and AT&T Research\n- -Office Hours: Fri, 9-11am\n- -Please get an appointment from: https://calendly.com/dilek-b2w\n- -Will also stay after the class for questions\n- Email: dilek@illinois.edu", "prev_chunk_id": "ALecture1_Introduction__0000", "next_chunk_id": "ALecture1_Introduction__0002"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Enabling conversational machines\n- -Complete tasks collaboratively by interacting with machines in natural language\n- -Safe and accurate interactions\n- -Interactions with web and embodied agents\n- -More information is available on the UIUC ConvAI web pages:\nhttps://uiuc-conversational-ai-lab.github.io/", "prev_chunk_id": "ALecture1_Introduction__0001", "next_chunk_id": "ALecture1_Introduction__0003"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0003", "chunk_index": 3, "page_numbers": [5], "text": "- -Ishika Agarwal ishikaa2@illinois.edu\n- -Janvijay Singh jvsingh2@illinois.edu\n- -Sagnik Mukherjee sagnikm3@illinois.edu Office Hours: Friday 1-3PM Book\nOffice hours: Friday 10am-12pm near Siebel 1129 (sign up\nhere: https://docs.google.com/spreadsheets/d/1p2lsCKnjQn6mPRSacTKekwhlD3IYhFU5fd DtL7Ur9PM/edit?usp=sharing)\nOffice Hours: Wednesday 10am-12pm Zoom/Seibel 1129 Book\nhere: https://outlook.office.com/book/CS546OfficeHours@uillinoisedu.onmicrosoft.com/\nhere: https://outlook.office.com/book/SagnikOH@uillinoisedu.onmicrosoft.com/?ismsaljsa uthenabled (edited)", "prev_chunk_id": "ALecture1_Introduction__0002", "next_chunk_id": "ALecture1_Introduction__0004"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0004", "chunk_index": 4, "page_numbers": [6], "text": "- -You are a PhD student\n- -You are a Masters student\n- -You are an Undergraduate student\n- -You are enrolled in Siebel School of Computing and Data Science\n- -You trained a deep learning model\n- -You fine-tuned an LLM\n- -You have published any papers\n- -You have published papers at NLP conferences (ACL, EMNLP, NAACL, ACL, COLING, LREC,‚Ä¶)\n- -This is your first year at UIUC", "prev_chunk_id": "ALecture1_Introduction__0003", "next_chunk_id": "ALecture1_Introduction__0005"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0005", "chunk_index": 5, "page_numbers": [7], "text": "- -a two-minute video available online\n- -Interactive Building Maps\n- -Students with disabilities\nTo obtain disability-related academic adjustments and/or auxiliary aids, students with disabilities must contact the course instructor as soon as possible and provide the instructor with a Letter of Academic Accommodations from Disability Resources and Educational Services (DRES). To contact DRES, you may visit 1207 S. Oak St., Champaign, call 217-333-1970, e-mail disability@illinois.edu or visit the DRES website. Here is the direct link to apply for services at DRES, https://www.disability.illinois.edu/applying-services.", "prev_chunk_id": "ALecture1_Introduction__0004", "next_chunk_id": "ALecture1_Introduction__0006"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0006", "chunk_index": 6, "page_numbers": [8], "text": "- -Canvas: https://canvas.illinois.edu/courses/59757\n- -Piazza for offline discussions:\n- -Sign up link: https://piazza.com/illinois/fall2025/cs546\n- -Access code: u66756wqqwm", "prev_chunk_id": "ALecture1_Introduction__0005", "next_chunk_id": "ALecture1_Introduction__0007"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0007", "chunk_index": 7, "page_numbers": [9], "text": "- -Cover advanced and recent research topics about natural language processing (NLP)\n- -Mainly based on deep learning and large language models (LLMs)\n- -Note that this is not an introductory level course!\n- -Review related founding papers\n- -The target audience is graduate students who plan to or are already working on these topics.", "prev_chunk_id": "ALecture1_Introduction__0006", "next_chunk_id": "ALecture1_Introduction__0008"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0008", "chunk_index": 8, "page_numbers": [10], "text": "- -You have basic knowledge of:\n- -Natural language processing\n- -Machine learning and deep learning\n- -Math, probability, and statistics\n- -Your knowledge of Python is already pretty solid.", "prev_chunk_id": "ALecture1_Introduction__0007", "next_chunk_id": "ALecture1_Introduction__0009"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0009", "chunk_index": 9, "page_numbers": [11], "text": "- -The classes will mainly be in lecture mode\n- -There will be\n- -2 homeworks, 2 midterms, and one final project (4-6 people team project)\n- -NO final exam!", "prev_chunk_id": "ALecture1_Introduction__0008", "next_chunk_id": "ALecture1_Introduction__0010"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0010", "chunk_index": 10, "page_numbers": [12], "text": "- -I expect you to attend class!\n- -Asking questions is encouraged.\n- -Not to each other!\n- -Please ask clarification questions immediately and save broader discussion topics, at the end of the class.", "prev_chunk_id": "ALecture1_Introduction__0009", "next_chunk_id": "ALecture1_Introduction__0011"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0011", "chunk_index": 11, "page_numbers": [13], "text": "- -Academic integrity: Students should pay particular attention to Article 1, Part 4: Academic Integrity. Read the Code at the following URL: http://studentcode.illinois.edu/.\n- -Late assignments will not be accepted.\n- -No cell phones and internet surfing are allowed in the lecture room.\n- -Do not be late to the class.", "prev_chunk_id": "ALecture1_Introduction__0010", "next_chunk_id": "ALecture1_Introduction__0012"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0012", "chunk_index": 12, "page_numbers": [14], "text": "- -2 midterms: 30%\n- -Homeworks (2): 30%\n- -Final Project: 40%\n- -Proposal write-up: 5%\n- -Final project write-up: 15%\n- -Code & Demo: 10%\n- -Presentations: 10% (2 presentations in total: proposal and final)\n- -Proportions subject to change: Check canvas for the latest info.\n- -(TBD) Possible to get extra credits from homeworks and the final project.", "prev_chunk_id": "ALecture1_Introduction__0011", "next_chunk_id": "ALecture1_Introduction__0013"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0013", "chunk_index": 13, "page_numbers": [15], "text": "- -Will take place on September and October 30th, 2025.\n- -Will be in the class.\n- -Will mainly include multiple choice and T/F questions.\n- -Will be on topics we have covered to date.", "prev_chunk_id": "ALecture1_Introduction__0012", "next_chunk_id": "ALecture1_Introduction__0014"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0014", "chunk_index": 14, "page_numbers": [16], "text": "- -Two coding homeworks with experimental results.\n- -TBD If you get one of the top N highest results on experiments, you will get up to 3 grade points bonus .\n- -You will submit\n- -running code that we can test on our test set and see the performance.\n- -a report describing the approach and results on the validation set.", "prev_chunk_id": "ALecture1_Introduction__0013", "next_chunk_id": "ALecture1_Introduction__0015"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0015", "chunk_index": 15, "page_numbers": [17], "text": "- -Aim to have publishable work from the class!\n- -Group projects of 4-6 students\n- -Can also reach out to TAs or PhD students from the UIUC ConvAI lab for:\n- -Ideas and mentoring\n- -I can also make suggestions once I see the proposals\n- -They may reach out to you based on your idea or you can pick their idea as the project topic (more on this in a few slides)", "prev_chunk_id": "ALecture1_Introduction__0014", "next_chunk_id": "ALecture1_Introduction__0016"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0016", "chunk_index": 16, "page_numbers": [18, 19, 20, 21], "text": "- -September 23: Finalize and sign-up project team\n- -OK to use piazza to search for project partners.\n- -Ask the TAs for help if you have difficulty finding a team\n- -\n- Spreadsheet to sign up project teams: https://docs.google.com/spreadsheets/d/1EJ_5Xby0mRhHFmSRSmxlv6Gws4Q s5T8P5JUiKYKAZcA/edit?usp=sharing\n- -PLEASE MAKE SURE TO NOT ERASE OTHER's ENTRIES!\nB\nProjectTitle(can changelater)\nC\nBriefDescription\nD\nTeamLead(Nameandemail)\nE\nTeamMembers(Namesand Emails)\nA\nTeam ID\nTA(orConvAl\nPhD student)\nMentor\nNotes\nG\nH\n- -Project proposal presentations and reports are due on October 7, 9am\n- -Presentation and report content should include:\n- -Problem description and motivation\n- -Related work (2-3 papers)\n- -Proposed approach(es)\n- -Datasets and metrics\n- -Proposal presentations will be on zoom (traveling to COLM that week)\n- -~2-3 minutes per team, 1 presenter is ok, but all team members should contribute & be present)\n- -Final projects presentations are due on December 2. (TBD: Poster versus presentation)\n- -Posters will be presented by student teams on December 2, 4, or 9.\n- -Final Project Reports are due on December 12.\n- -Should be in ACL paper submission format (8 pages) & be like a regular paper submission.\n- -In addition to the content in the proposal, they should include:\n- -Contributions of the work and main findings.\n- -Experiments and results, as well as an analysis of results.\n- -Limitations of the approach\n- -Start thinking about the proposal soon and reach out to me if you'd like to discuss ideas. Example topics:\n- -NL understanding\n- -E.g., Converting NL to logical form or converting NL Math Theorems to structured form (i.e., Lean)\n- -Domain adaptation\n- -E.g., Improving LLM performance in a new domain (without forgetting earlier knowledge)\n- -Creative idea generation with LLMs\n- -E.g., Math Theorem Proving\n- -TAs and ConvAI lab PhD students are also sharing a list of high-level ideas that we will share soon.", "prev_chunk_id": "ALecture1_Introduction__0015", "next_chunk_id": "ALecture1_Introduction__0017"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0017", "chunk_index": 17, "page_numbers": [22], "text": "- -For general NLP background:\n- -Daniel Jurafsky and James Martin. Speech and Language Processing (3rd draft)\n- -Christopher D. Manning and Hinrich Schutze. Foundations of Statistical Natural Language Processing. MIT Press.", "prev_chunk_id": "ALecture1_Introduction__0016", "next_chunk_id": "ALecture1_Introduction__0018"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0018", "chunk_index": 18, "page_numbers": [23, 24, 25], "text": "- -For deep learning background (and many code examples):\n- -Aston Zhang, Zack Lipton, Mu Li, and Alex Smola. Dive Into Deep Learning. https://d2l.ai/\n- -Also freely available online.\n- -Delip Rao and Brian McMahan, Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning. https://www.oreilly.com/library/view/natural-languageprocessing/9781491978221/\n- -I will prepare most of the content based on these books and papers.\n- -Will share:\n- -a list of related papers on my slides (on canvas too)\n- -my lecture slides on canvas (right before each class)", "prev_chunk_id": "ALecture1_Introduction__0017", "next_chunk_id": "ALecture1_Introduction__0019"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0019", "chunk_index": 19, "page_numbers": [26], "text": "- -Introduction to class\n- -Class Goals and Logistics\n- -Introduction to NLP\n- -Why study NLP?\n- -Success of LLMs and what is still missing?\n- -An overview of the topics we will cover", "prev_chunk_id": "ALecture1_Introduction__0018", "next_chunk_id": "ALecture1_Introduction__0020"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0020", "chunk_index": 20, "page_numbers": [27], "text": "- -Field at the intersection of many disciplines, such as\n- -computer science, linguistics, artificial intelligence, cognitive science, and psychology\n- -Aims to enable computers to understand, interpret, and generate natural language, leading to improvements in areas, such as\n- -summarization, machine translation, question answering, dialogue systems, and so on\n- -Has applications across several industries, such as\n- -virtual personal assistants, customer service, healthcare, data analysis, and so on", "prev_chunk_id": "ALecture1_Introduction__0019", "next_chunk_id": "ALecture1_Introduction__0021"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0021", "chunk_index": 21, "page_numbers": [28, 29], "text": "- -Claude Shannon, A Mathematical Theory of Communication, The Bell System Technical Journal, 1948.\n- -Noam Chomsky, Syntactic Structures, 1957.\n- -Symbolic/Rule based methods\n- -i.e., ELIZA (Weizenbaum, 1966)\n1950               1960               1970               1980                1990               2000                2010\n2020", "prev_chunk_id": "ALecture1_Introduction__0020", "next_chunk_id": "ALecture1_Introduction__0022"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0022", "chunk_index": 22, "page_numbers": [30], "text": "- -Jelinek, F.; Bahl, L.; Mercer, R. (1975). \"Design of a linguistic statistical decoder for the recognition of continuous speech\". IEEE Transactions on Information Theory.\n- -Baum, L. E.; Petrie, T. (1966). \"Statistical Inference for Probabilistic Functions of Finite State Markov Chains\". The Annals of Mathematical Statistics. 37 (6): 1554 -1563.\n- -First Neural Network Architectures\n- -LSTM (Hochreiter and Schmidhuber, 1995)\n- -Recurrent Neural Networks (Jordan, 1986 and Elman, 1990 Networks)", "prev_chunk_id": "ALecture1_Introduction__0021", "next_chunk_id": "ALecture1_Introduction__0023"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0023", "chunk_index": 23, "page_numbers": [31, 32, 33], "text": "- -Classification\n- -E.g., Support Vector Machines (Cortes and Vapnik, 1995)\n- -Sequence Tagging\n- -E.g., Conditional Random Fields (Lafferty et al., 2001)\n- -Re-rise of deep neural networks and language models\n- -Word embeddings (word2vec, Mikolov et al., 2013)\n- -Transformers (Vaswani et al., 2017)\n- -BERT (Devlin et al., 2018)\n- -Large Language Models (LLMs)\n- -OpenAI's GPT3 ( Brown et al, 2020)\n- -ChatGPT\n-\n‚Ä¶", "prev_chunk_id": "ALecture1_Introduction__0022", "next_chunk_id": "ALecture1_Introduction__0024"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0024", "chunk_index": 24, "page_numbers": [34], "text": "- -Introduction to class\n- -Class Goals and Logistics\n- -Introduction to NLP\n- -Why study NLP?\n- -Success of LLMs and what is still missing?\n- -An overview of the topics we will cover", "prev_chunk_id": "ALecture1_Introduction__0023", "next_chunk_id": "ALecture1_Introduction__0025"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0025", "chunk_index": 25, "page_numbers": [35], "text": "- -Search engines\n- -Virtual personal assistants\n- -Chatbots\n- -Grammar checkers\n- -Writing assistants\n- -Question answering\n- -Email filtering\n- -Summarization\n- -Embodied agents", "prev_chunk_id": "ALecture1_Introduction__0024", "next_chunk_id": "ALecture1_Introduction__0026"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0026", "chunk_index": 26, "page_numbers": [36], "text": "- -Tech companies\n- -Research labs\n- -Healthcare (medical transcription, converting interactions to electronic health records, clinical data analysis)\n- -Law (legal document processing)\n- -Finance (analyzing reports or news)\nOther news (July 1 st , 2025): https://www.wired.com/story/markzuckerberg-meta-offer-top-ai-talent-300million/", "prev_chunk_id": "ALecture1_Introduction__0025", "next_chunk_id": "ALecture1_Introduction__0027"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0027", "chunk_index": 27, "page_numbers": [36], "text": "Software Development (25)", "prev_chunk_id": "ALecture1_Introduction__0026", "next_chunk_id": "ALecture1_Introduction__0028"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0028", "chunk_index": 28, "page_numbers": [37, 38], "text": "- -Language is inherently complex and ambiguous .\n- -Studying NLP involves solving fascinating problems, such as:\n- -How to model and represent meaning?\n- -How to deal with ambiguity?\n- -How to represent and model context, emotions, tone, or sarcasm?\n- -NLP is central to modern AI development.\n- -Innovations in LLMs (like OpenAI's GPTs and Anthropic's Claude models, etc.) are transforming how we think about\n- -intelligence, creativity, and automation.", "prev_chunk_id": "ALecture1_Introduction__0027", "next_chunk_id": "ALecture1_Introduction__0029"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0029", "chunk_index": 29, "page_numbers": [39], "text": "Figure 1: Distribution of number of papers per year in the ACL Anthology from 1952 to 2022.\nFigure from Schopf et al., 2023.", "prev_chunk_id": "ALecture1_Introduction__0028", "next_chunk_id": "ALecture1_Introduction__0030"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0030", "chunk_index": 30, "page_numbers": [40], "text": "- -Introduction to class\n- -Class Goals and Logistics\n- -Introduction to NLP\n- -Why study NLP?\n- -Success of LLMs and what is still missing?\n- -An overview of the topics we will cover", "prev_chunk_id": "ALecture1_Introduction__0029", "next_chunk_id": "ALecture1_Introduction__0031"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0031", "chunk_index": 31, "page_numbers": [41], "text": "- -Natural language understanding and generation . LLMs can:\n- -Answer questions with context-aware accuracy\n- -Generate fluent, coherent text (stories, essays, reports)\n- -Summarize long documents\n- -Translate languages with near-human quality", "prev_chunk_id": "ALecture1_Introduction__0030", "next_chunk_id": "ALecture1_Introduction__0032"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0032", "chunk_index": 32, "page_numbers": [42], "text": "- -Code generation and debugging. LLMs can understand and generate computer code , and help with:\n- -Writing functions\n- -Debugging errors\n- -Explaining code\n- -Completing programs", "prev_chunk_id": "ALecture1_Introduction__0031", "next_chunk_id": "ALecture1_Introduction__0033"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0033", "chunk_index": 33, "page_numbers": [43], "text": "- -LLMs have powered a new generation of AI chatbots and assistants , and are able to:\n- -Produce very natural sounding and engaging responses\n- -Hold multi-turn conversations\n- -Remember context (to some degree)\n- -Adjust tone and formality", "prev_chunk_id": "ALecture1_Introduction__0032", "next_chunk_id": "ALecture1_Introduction__0034"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0034", "chunk_index": 34, "page_numbers": [44], "text": "- -Pretrained language models are known to hallucinate (Ji et al., ACM Comput. Surv., 2022)\n- -Even for summarization, where the content is given, hallucination rates are around 1-2%(e.g., OpenAI GPT-4o 1.5%)\n(https://huggingface.co/spaces/vectara/Hallucinat ion-evaluation-leaderboard)\n- -And can be higher for other tasks\n- -Hallucinations are not acceptable for real applications!", "prev_chunk_id": "ALecture1_Introduction__0033", "next_chunk_id": "ALecture1_Introduction__0035"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0035", "chunk_index": 35, "page_numbers": [44], "text": "Which school did Dilek work at?\nDilek Hakkani-Tur has worked at several academic institutions throughout her career. She is currently a professor in the Department of Electrical and Computer Engineering at the University of Arizona,where she leads the Natural Language Processing group.\nPrior to her current position, Dr. Hakkani-Tur held several research and academic positions, including working as a research scientist at AT&T Labs, aseniorresearch manager at Microsoft Research,and aprofessor at theUniversityofPittsburgh.\nThroughout her career,Dr.Hakkani-Tur hasmade significant contributions to the field of modal communication. Her work has been recognized with several awards and honors, including the IEEE Speech and Language Processing Technical Committee (SLTC) Award for Distinguished Technical Contributions to the field of Automatic Speech Recognition (ASR) in 2020.", "prev_chunk_id": "ALecture1_Introduction__0034", "next_chunk_id": "ALecture1_Introduction__0036"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0036", "chunk_index": 36, "page_numbers": [45], "text": "- Retrieving and selecting knowledge to ground on (Eric et al., INLG 2021)\n- Generating a response, given the conversation context and selected knowledge\n- -Unstructured,\n- -Text segments are modelled independently\n- -Content is assumed to be reliable\nPros: Text resources may already be available Cons:", "prev_chunk_id": "ALecture1_Introduction__0035", "next_chunk_id": "ALecture1_Introduction__0037"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0037", "chunk_index": 37, "page_numbers": [46], "text": "- -Large language models (like GPTs) are great at generating natural and accurate sounding text, but they don't always remember specific facts or stay up to date!\n- -Retrieval-Augmented Generation (RAG) solves this by combining two ideas:\n- -Retrieval : Searching a knowledge source (such as Wikipedia or company-internal databases) to find relevant information.\n- -Generation : Using a language model to produce an answer that incorporates that information.\n- -Instead of making up answers based on model weights, the model looks things up first, then explains them.", "prev_chunk_id": "ALecture1_Introduction__0036", "next_chunk_id": "ALecture1_Introduction__0038"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0038", "chunk_index": 38, "page_numbers": [47], "text": "- Earlier datasets use wikipedia (e.g.,WoW, TopicalChat), news (e.g., TopicalChat)\n- Internet augmented generation (Komeli et al., ACL 2022)\n- Queries are formulated based on the conversation context\n- Knowledge candidates are retrieved by a search engine\nFigure is from (Komeli et al, ACL 2022).", "prev_chunk_id": "ALecture1_Introduction__0037", "next_chunk_id": "ALecture1_Introduction__0039"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0039", "chunk_index": 39, "page_numbers": [48], "text": "- Large language models can be trained to learn to call tools\n- These provide additional knowledge for task completion\n- LAMDA (Thoppilan et al., arXiv:2201.08239 2022)\n- ToolFormer (Schick et al., arXiv:2302.04761 2023)\n- LangChain (https://python.langchain.com/en/latest/)", "prev_chunk_id": "ALecture1_Introduction__0038", "next_chunk_id": "ALecture1_Introduction__0040"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0040", "chunk_index": 40, "page_numbers": [49], "text": "ChatGPT\nhow many s's in assistant\n```\nScreenshot from 7/28/2025 Tried again last night‚Ä¶ The word \"assistant\" has 2 S's:\n```\n'Thinking longer for a better answer'\n```\na S‚Üê1 S2 1 s ‚Üê (no, this is i) t a n Total: 2 S's.\n```\nX", "prev_chunk_id": "ALecture1_Introduction__0039", "next_chunk_id": "ALecture1_Introduction__0041"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0041", "chunk_index": 41, "page_numbers": [50], "text": "- -LLMs capture, reflect and amplify:\n- -Social biases (gender, race, ethnicity)\n- -Cultural or political viewpoints\n- -Toxic or offensive content\n- -They also may unintentionally capture and leak private information.", "prev_chunk_id": "ALecture1_Introduction__0040", "next_chunk_id": "ALecture1_Introduction__0042"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0042", "chunk_index": 42, "page_numbers": [51], "text": "- Unsafe responses are not acceptable!\n- Detecting and filtering unsafe content from the training data or outputs (Dinan et al., EMNLP 2019)\n- Preventing toxic generations during decoding (Arora et al., ACL 2022)\n- Reinforcement learning from human feedback (Bai et al., arXiv:2204.05862, 2022)\n- Detecting and re-writing unsafe responses (Bauer et al., EMNLP Findings, 2022)\n- In-context learning to steer models towards safer outputs (Meade et al., arXiv:2302.00871, 2023)\n- -Quickly recovering from issues (Gupta et al., arXiv:2212.10557, 2022)", "prev_chunk_id": "ALecture1_Introduction__0041", "next_chunk_id": "ALecture1_Introduction__0043"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0043", "chunk_index": 43, "page_numbers": [52], "text": "- -Lack of full consistency and limited long-term memory:\n- -The same instruction can result in different answers each time or contradictions within a single answer or throughout the same conversation or across multiple sessions.\n- -AI Overreliance:\n- -AI sycophancy\n- -Users may be misled by plausible sounding mistakes in the outputs or\n- -Environmental concerns:\n- -Training and inference with LLMs requires massive computational resources and consumes significant energy and raises concerns about carbon footprint.\n- -Opaque model decisions:\n- -Models are often black boxes, and it is hard to explain their decisions.", "prev_chunk_id": "ALecture1_Introduction__0042", "next_chunk_id": "ALecture1_Introduction__0044"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0044", "chunk_index": 44, "page_numbers": [53], "text": "- -Introduction to class\n- -Class Goals and Logistics\n- -Introduction to NLP\n- -Why study NLP?\n- -Success of LLMs and what is still missing?\n- -An overview of the topics we will cover", "prev_chunk_id": "ALecture1_Introduction__0043", "next_chunk_id": "ALecture1_Introduction__0045"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0045", "chunk_index": 45, "page_numbers": [54], "text": "- -an introduction to NLP and deep learning for NLP\n- -Neural networks\n- -Language Modeling and Word Representations\n- -Sequence Modeling\n- -Model Architectures and Contextual Embeddings\n- -Decoding, Attention\n- -Transformers\n- -Pre-training and Fine-tuning", "prev_chunk_id": "ALecture1_Introduction__0044", "next_chunk_id": "ALecture1_Introduction__0046"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0046", "chunk_index": 46, "page_numbers": [55], "text": "- -Following weeks will introduce newer research areas and provide background on them, e.g.,\n- -Prompting and Instruction Tuning\n- -Reasoning\n- -Alignment and Preference Learning\n- -Theory of Mind\n- -Agentic Systems\n- -And so on‚Ä¶", "prev_chunk_id": "ALecture1_Introduction__0045", "next_chunk_id": "ALecture1_Introduction__0047"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0047", "chunk_index": 47, "page_numbers": [56], "text": "- -Vivian (Yun-Nung) Chen, NTU (especially for the graphics, for several lectures)\n- -Heng Ji, UIUC\n- -Marilyn Walker, UCSC\n- -And others", "prev_chunk_id": "ALecture1_Introduction__0046", "next_chunk_id": "ALecture1_Introduction__0048"}
{"lecture_id": "ALecture1_Introduction", "source_pdf": "data/anlp/ALecture1_Introduction.pdf", "chunk_id": "ALecture1_Introduction__0048", "chunk_index": 48, "page_numbers": [57], "text": "- -Machine Learning (ML) for NLP\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study\n- ML Examples: NLP Tasks", "prev_chunk_id": "ALecture1_Introduction__0047", "next_chunk_id": null}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0001"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Machine Learning (ML) for NLP\n- -ML Examples: NLP Tasks\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0000", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0002"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Machine learning (ML) is the field of artificial intelligence that develops algorithms and statistical models that computers use to perform a specific task\n- -without using explicit instructions (i.e., coding the specific steps for performing the task),\n- -relying on patterns and inference instead.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0001", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0003"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0003", "chunk_index": 3, "page_numbers": [4], "text": "Some tasks are complex, and we don't know how to write a program to solve them Example: speech recognition, summarization, ‚Ä¶\n?", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0002", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0004"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0004", "chunk_index": 4, "page_numbers": [5], "text": "- -Learning ‚âà Looking for a Function\nInput:\n'I love this camera!'\nf\n+\n'It's a very bulky.' ? f\n-\nGiven a large amount of data, the machine learns what the function f should be.\n'It makes too much noise.'\nf", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0003", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0005"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0005", "chunk_index": 5, "page_numbers": [6], "text": "- Speech Recognition\n- Handwriting Recognition\nf(\n- Weather forecast\nThursday\n- Playing video games\n<!-- formula-not-decoded -->\n'\nSaturday'\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0004", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0006"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -Machine Learning (ML) for NLP\n- -ML Examples: NLP Tasks\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0005", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0007"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0007", "chunk_index": 7, "page_numbers": [8], "text": "- -Sentence Segmentation\n- -Tokenization", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0006", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0008"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0008", "chunk_index": 8, "page_numbers": [11], "text": "They hid the letter on the shelf.\nnsubj csubj dobj iobj pobj, Description = nominal subject clausal subject direct object indirect object object of preposition. pou det, Description = temporal modifier determiner. Modifier Dependencies, Description = Description. appos prep, Description = appositional modifier prepositional modifier", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0007", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0009"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0009", "chunk_index": 9, "page_numbers": [13], "text": "But the little prince could not restrain admiration:\n\"Oh! How beautiful you are!\"\n\"Am I not?\" the flower responded, sweetly. \"And I was born at the same moment as the sun . . .\"\nThe little prince could guess easily enough that she was not any too modest-but how moving--and exciting--she was!\n\"I think it is time for breakfast,\" she added an instant later. \"If you would have the kindness to think of my needs--\"\nAnd the little prince, completely abashed, went to look for a sprinkling-can of fresh water. So, he tended the flower.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0008", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0010"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0010", "chunk_index": 10, "page_numbers": [14], "text": "- -Machine Translation 'The book is on the table.' 'Kitap masada.' ‰∏Ä\n- -Question Answering\n- -Summarization\n```\n'Paul Auster' 'Who wrote Leviathan?' ‰∫å\n```", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0009", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0011"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0011", "chunk_index": 11, "page_numbers": [15], "text": "- -Machine Learning (ML) for NLP\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study\n- ML Examples: NLP Tasks", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0010", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0012"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0012", "chunk_index": 12, "page_numbers": [16], "text": "- -Supervised learning\n- -Self-supervised learning\n- -Unsupervised learning\n- -Reinforcement learning", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0011", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0013"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0013", "chunk_index": 13, "page_numbers": [17], "text": "- -Learning functions for predicting targets from given inputs, using pairs of inputs and targets.\n- -The targets, which we often call labels , are generally denoted by y .\n- -The input data, also called the features or covariates, are typically denoted ùê± .", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0012", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0014"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0014", "chunk_index": 14, "page_numbers": [18, 19], "text": "```\n'It makes too much noise.' -(negative) function input y : x\n```\nfunction output\nTraining :\nTesting :\n- -Each (input, target) pair is called an example or an instance .\n- -Supervised learning learns the model/function from examples.\n- -For example, machine translation models are trained from pairs of source and target language sentences.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0013", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0015"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0015", "chunk_index": 15, "page_numbers": [20], "text": "- -Whether an apartment will be preferred by a user or not?\n- -Features:\n- -Number of rooms, e.g., 1, 2, 3\n- -Whether the apartment has a dish washer or not, e.g. yes, no\n- -Style of the apartment building, e.g., high-rise, low-rise, duplex,\n-\n‚Ä¶", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0014", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0016"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0016", "chunk_index": 16, "page_numbers": [21], "text": "w, b are the parameters of this neuron", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0015", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0017"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0017", "chunk_index": 17, "page_numbers": [22], "text": "- -Observations : inputs, x .\n- -Model : Function that takes an observation and predicts the value of its target label.\n- -Targets : labels, y , corresponding to the observations (also called ground truth sometimes).", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0016", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0018"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0018", "chunk_index": 18, "page_numbers": [23], "text": "- Parameters:weights (for deep learning), w, that parameterize the model.\n- Predictions: Also called estimates, targetvalues estimated by the model, y.\n- Loss Function:a function(denoted L) that compares how far off a prediction is from its target.The lowertheloss,thebetteristhe modelatpredictingthetarget.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0017", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0019"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0019", "chunk_index": 19, "page_numbers": [24], "text": "- -When the targets take arbitrary values in a range (instead of specific categories such as +/- for sentiment classification), we call it a regression problem.\n- -Estimating the price of a house given attributes such as location, square footage, number of rooms, etc.\n- -Examples:\n- -Predicting how many miles can a dog run given attributes such as its breed, age, size, etc.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0018", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0020"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0020", "chunk_index": 20, "page_numbers": [25], "text": "- -When the targets take a limited number of pre-defined categories, we call it a classification problem.\n- -Positive/negative sentiment classification\n- -Binary classification: only two values\n- -Sentence segmentation from speech: each word boundary is classified into sentence boundary versus not a sentence boundary.\n- -Multi-class classification: more than two values\n- -Sentence segmentation for text: each '.' is classified into two classes similarly.\n- -Digit recognition: each digit is classified into one of 10 values for 0,‚Ä¶,9.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0019", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0021"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0021", "chunk_index": 21, "page_numbers": [26], "text": "- -Some tasks are not simply binary or multi-class classification tasks, but we need to\n- -Tagging tokens/words in natural language utterances\n- -Tagging images with multiple objects", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0020", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0022"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0022", "chunk_index": 22, "page_numbers": [26], "text": "I want to fly from Boston to Seattle via Chicago. => I want to fly from Boston to Seattle via Chicago.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0021", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0023"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0023", "chunk_index": 23, "page_numbers": [27], "text": "- -Supervised learning\n- -Self-supervised learning\n- -Unsupervised learning\n- -Reinforcement learning", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0022", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0024"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0024", "chunk_index": 24, "page_numbers": [28], "text": "- -Finding previously unknown patterns in data set without pre-existing labels. Urlsbeled data Cluslered data\n- -Zero-Shot Learning\n- -Clustering\n- -Few-Shot Learning\n- -Weakly-supervised Learning\n- -Semi-supervised Learning\n- -Transfer Learning", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0023", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0025"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0025", "chunk_index": 25, "page_numbers": [29], "text": "- -Supervised learning\n- -Self-supervised learning\n- -Unsupervised learning\n- -Reinforcement learning", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0024", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0026"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0026", "chunk_index": 26, "page_numbers": [30], "text": "- -Benefiting from enormous amount of data available on the web and other resources.\n- -Examples:\n- -For text, masking tokens and predicting them or predicting the next token,\n- -For image, image reconstruction, and\n- -For videos, predicting video frames.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0025", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0027"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0027", "chunk_index": 27, "page_numbers": [31], "text": "- -Supervised learning\n- -Self-supervised learning\n- -Unsupervised learning\n- -Reinforcement learning", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0026", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0028"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0028", "chunk_index": 28, "page_numbers": [32], "text": "- -Learning action prediction policies through interactions with an environment over a series of timesteps.\n- -Finally, the agent receives a reward ùëü ùë° from the environment.\n- -At each timestep ùë° , the learning agent receives some observation ùëúùë° from the environment and must choose an action ùëéùë° that is subsequently transmitted back to the environment via some mechanism.\n- -The agent then receives a subsequent observation, and chooses a subsequent action, and so on.\n- -Apolicy is a function that maps from observations to actions.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0027", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0029"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0029", "chunk_index": 29, "page_numbers": [33], "text": "- No labels, just a reward\n- Agent can exploit and explore", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0028", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0030"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0030", "chunk_index": 30, "page_numbers": [34], "text": "- -Machine Learning (ML) for NLP\n- -ML Examples: NLP Tasks\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0029", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0031"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0031", "chunk_index": 31, "page_numbers": [35], "text": "- -Training deep learning models: updating them successively so that they get better as they see more data\n- -Getting better: minimizing a loss function\n- -A crucial step in nearly all deep learning optimization algorithms.\n- -Loss functions that are differentiable with respect to our model's parameters.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0030", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0032"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0032", "chunk_index": 32, "page_numbers": [36], "text": "- -Suppose we have function ùëì : ‚Ñù‚Üí‚Ñù with scalar input and outputs.\n- -The derivative of ùëì is defined as:\n<!-- formula-not-decoded -->\n- -If ùëì ‚Ä≤ ( ùëé ) exists, ùëì is said to be differentiable at ùëé .\n- -If ùëì is differentiable at every number of an interval, then this function is differentiable on this interval.\n- -Alternative notations for derivatives:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0031", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0033"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0033", "chunk_index": 33, "page_numbers": [37, 38], "text": "- DC = 0 (C is a constant),\n- Dxn = nxn-1 (the power rule, n is any real number),\n- Dex: = ex\n- Dln(x) = 1/x.\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nConstant multiple rule\nSum rule\nProduct rule\nQuotient rule", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0032", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0034"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0034", "chunk_index": 34, "page_numbers": [39], "text": "- -Given function,\n- -We can apply the previous set of rules to get its derivative:\n<!-- formula-not-decoded -->\n- -For ùë• =1, we have ùë¢ ‚Ä≤ =2.\n- -This derivative is also the slope of the tangent line to the curve ùë¢ = ùëì ( ùë• ) when ùë• =1.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0033", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0035"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0035", "chunk_index": 35, "page_numbers": [40], "text": "- -If ùë¶ is a function with ùëõ variables, i.e.,\n<!-- formula-not-decoded -->\n- -The partial derivative of ùë¶ with respect to its ùëñ th parameter ùë•ùëñ is:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0034", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0036"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0036", "chunk_index": 36, "page_numbers": [41], "text": "- -For functions with multiple variables, we obtain the gradient vector of the function by concatenating partial derivatives of that function w.r.t. all its variables.\n- -For example, for ùëì : ‚Ñù ùëõ ‚Üí ‚Ñù is a function with an ùëõ -dimensional vector ùê± =[ ùë• 1 , ùë• 2 ,‚Ä¶, ùë•ùëõ ] ‚ä§ input and a scalar output. The gradient of the function ùëì ( ùê± ) w.r.t. ùê± is a vector of ùëõ partial derivatives:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0035", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0037"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0037", "chunk_index": 37, "page_numbers": [42], "text": "- -Suppose that functions ùë¶ = ùëì ( ùë¢ ) and ùë¢ = ùëî ( ùë• ) are both differentiable, then,\n<!-- formula-not-decoded -->\n- -More general scenario, where a differentiable function ùë¶ has variables ùë¢ 1 , ùë¢ 2 ,‚Ä¶, ùë¢ùëö , where each differentiable function ùë¢ùëñ has variables ùë• 1 , ùë• 2 ,‚Ä¶, ùë•ùëõ . Then, for any ùëñ =1,2,‚Ä¶, ùëõ ,\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0036", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0038"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0038", "chunk_index": 38, "page_numbers": [43], "text": "- -Machine Learning (ML) for NLP\n- -ML Examples: NLP Tasks\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0037", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0039"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0039", "chunk_index": 39, "page_numbers": [44], "text": "End-to-end training: what each function should do is learned automatically\nDeep learning usually refers to neural network -based model", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0038", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0040"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0040", "chunk_index": 40, "page_numbers": [45], "text": "vector x 'I love this product.'\nRepresentation Learning attempts to learn good features/representations\nDeep Learning attempts to learn (multiple levels of) representations and an output", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0039", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0041"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0041", "chunk_index": 41, "page_numbers": [46], "text": "Each box is a simple function in the production line:\n:hand-crafted\n:learned from data", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0040", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0042"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0042", "chunk_index": 42, "page_numbers": [47], "text": "Less engineering labor, but machine learns more", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0041", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0043"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0043", "chunk_index": 43, "page_numbers": [52], "text": "- Cascading the neurons to form a neural network\nInput\nLayer 1\nLayer 2\nLayer L\nOutput\nEach layer is a simple function in the production line\nA neural network is a complex function:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0042", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0044"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0044", "chunk_index": 44, "page_numbers": [53], "text": "- -Machine Learning (ML) for NLP\n- -ML Examples: NLP Tasks\n- -ML Basics\n- -Calculus Reminders\n- -Deep Learning\n- -Linear Regression -Case Study", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0043", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0045"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0045", "chunk_index": 45, "page_numbers": [54], "text": "- -Speech Recognition ( ) = f 'the book'\n- -Handwriting Recognition\n- -Weather forecast ( f\n- -Play video games ( f\nf\n(\n<!-- formula-not-decoded -->\n- ) = '                Saturday' Thursday\n) =\n'move left'", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0044", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0046"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0046", "chunk_index": 46, "page_numbers": [55], "text": "Training aims to pick the best function given the observed examples Testing predicts the label using the learned function", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0045", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0047"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0047", "chunk_index": 47, "page_numbers": [56], "text": "- -Regression : modeling the relationship between data points ùê± and corresponding real-valued targets ùë¶\n- -Example: Predicting prices of homes\n- -Label: Price, ùë¶ ( ùëñ )\n- -Features: Size, Age\n- -ùë• ( ùëñ ) =[ ùë• ( ùëñ ) 1 , ùë• ( ùëñ ) 2 ]\nSize (sqft), 1 = Price. 2100, 1 = $1.90M. 1600, 1 = $1.56M. 3200, 1 = $2.40M. ‚Ä¶, 1 = ‚Ä¶\n2019 Sales:", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0046", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0048"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0048", "chunk_index": 48, "page_numbers": [57], "text": "- -the relationship between the features ùê± and targets ùë¶ is linear,\n- -i.e., ùë¶ can be expressed as a weighted sum of the inputs ùê± , give or take some noise on the observations", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0047", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0049"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0049", "chunk_index": 49, "page_numbers": [58], "text": "- Bivariate data: (x 1 ,y 1 ), (x2 , y 2 ), ‚Ä¶, ( xn ,y n )\n- Model: y i = f(x i ) + e i Random error\n- -Supervised approach!\n- -Model allows us to predict the value of y for any given value of x.\n- -x is called the independent or predictor variable.\n- -y is the dependent or response variable.", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0048", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0050"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0050", "chunk_index": 50, "page_numbers": [59], "text": "- lines: f(x) = mx + b\n- polynomials: f(x) = ax 2 + bx + c\n- others: f(x) = a/x + b\n$$f(x) = a sin(x) + b$$\n$$f(x) = a sqrt(x)$$", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0049", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0051"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0051", "chunk_index": 51, "page_numbers": [60], "text": "- -The relationship between x and y is linear\n- -y is distributed normally at each value of x, and the variance of y at every value of x is the same\n- -The observations are independent", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0050", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0052"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0052", "chunk_index": 52, "page_numbers": [61], "text": "- -ùë¶ùëñ is the actual value, ùëì ùë•ùëñ is the prediction, and their difference is the error.\n- -Commonly used function, (mean) squared error:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0051", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0053"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0053", "chunk_index": 53, "page_numbers": [62, 63, 64, 65, 66, 67, 68], "text": "- -(Mean) squared error:\n<!-- formula-not-decoded -->\n- -To estimate the values that minimize the mean squared error, we need to take the derivative with respect to the two variables ùëö and ùëè :\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -b is called bias.\n- -m is the slope of f ( x )\n- -Every time x moves by 1, y moves by m\n- -We need to solve these two equations to find ùëö and ùëè , to be able to find the function ùëì ùë•ùëñ .\n- -Beginning with the first one, for ùëè :\n<!-- formula-not-decoded -->\n- -We can use ‡¥§ ùë¶ and “ß ùë• to represent the mean values of the ùë• and ùë¶ coordinates, that is all the ùë• and ùë¶ values in our data:\n<!-- formula-not-decoded -->\n- -Then we can simply re-write the equation for ùëè :\n<!-- formula-not-decoded -->\n- -Now, let's find the value of ùëö :\n“ß\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n“ß\n“ß\n“ß\n<!-- formula-not-decoded -->\n“ß\n“ß\n“ß\n“ß\n“ß\n“ß\n“ß\n“ß\n“ß\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0052", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0054"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0054", "chunk_index": 54, "page_numbers": [68], "text": "<!-- formula-not-decoded -->\n“ß\n“ß\n“ß", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0053", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0055"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0055", "chunk_index": 55, "page_numbers": [69], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n“ß\n“ß", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0054", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0056"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0056", "chunk_index": 56, "page_numbers": [70], "text": "- -Goal: choose the weights ùë§ and bias ùëè to best fit the true values of ùë¶ observed in the data\n- -When our inputs consist of ùëë features, we express our prediction ùë¶ ÃÇ as:\n<!-- formula-not-decoded -->\n- -Collecting all features into a vector ùê± and all weights into a vector ùê∞ , our model can be expressed compactly using a dot product:\n<!-- formula-not-decoded -->\n- -ùê± : feature vector for a single data point\n- -ùêó : a collection of data points\n- -ùê≤ ÃÇ: vector of estimated predictions\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0055", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0057"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0057", "chunk_index": 57, "page_numbers": [71], "text": "ùê≤ ÃÇ = ùêóùê∞ + ùëè\nModel:\nHypothesis Function  Set\nÔÅå\n2\n1\n,\nf\nf\nParameters: elements of ùê∞ and ùëè\n- Q1. What is the model? (function hypothesis set)\n- Q2. What does a 'good' function mean?\n- Q3. How do we find the 'best' function?", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0056", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0058"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0058", "chunk_index": 58, "page_numbers": [72], "text": "- -Measure of fitness, quantifies the distance between ùê≤ and ùê≤ ÃÇ.\n- -Sum of squared errors:\nÃÇ\n- -To measure quality over the entire training set, we average over all K training examples:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nResidual = truth - predicted", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0057", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0059"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0059", "chunk_index": 59, "page_numbers": [73], "text": "- -Training the model ‚âà search for parameters ( ùê∞ ‚àó , ùëè ‚àó ) that minimize the total loss across all training samples:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0058", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0060"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0060", "chunk_index": 60, "page_numbers": [74], "text": "- -ùê≤ ÃÇ = ùêóùê∞ + ùëè\n- -Append 1 to features and bias to the weights: ùêó -> [ ùêó 1] and ùê∞ ->\n<!-- formula-not-decoded -->\nùê∞\nùëè", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0059", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0061"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0061", "chunk_index": 61, "page_numbers": [75], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nLoss is convex, hence the optimum solution is at: ùëëùê∞ ùêø ( ùêó , ùë¶ , ùê∞ ) = 0 Therefore,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0060", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0062"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0062", "chunk_index": 62, "page_numbers": [76], "text": "https://docs.google.com/spreadsheets/d/1EJ_5Xby0mRhHFmSRSmxlv6 Gws4Qs5T8P5JUiKYKAZcA/edit?usp=sharing\n(Everyone should sign up by: Sept 23rd)\nYou can start a new team or join an existing one. Teams should be 4-6 people!", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0061", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0063"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0063", "chunk_index": 63, "page_numbers": [77], "text": "- -Make sure that you can contribute with meaningful work before joining a team!\n- -Final reports will include a section on who did what‚Ä¶", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0062", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0064"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0064", "chunk_index": 64, "page_numbers": [78], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0063", "next_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0065"}
{"lecture_id": "ALecture2_NLP_Tasks_DL_intro", "source_pdf": "data/anlp/ALecture2_NLP_Tasks_DL_intro.pdf", "chunk_id": "ALecture2_NLP_Tasks_DL_intro__0065", "chunk_index": 65, "page_numbers": [78], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture2_NLP_Tasks_DL_intro__0064", "next_chunk_id": null}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture3_DeepLearning_Background__0001"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0000", "next_chunk_id": "ALecture3_DeepLearning_Background__0002"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Linear regression is a single-layer neural network, consisting of just a single neuron!\n- -Fully connected layer (also called dense layer): every input is connected to every output.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0001", "next_chunk_id": "ALecture3_DeepLearning_Background__0003"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Seen the solution to linear regression last Thursday!\n- -Even when we cannot solve the models analytically, we can still train models effectively by iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function:\n- -Derivative of the true loss (i.e., average of the losses computed over all examples in the training data).\n- -Expressing the updates mathematically:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0002", "next_chunk_id": "ALecture3_DeepLearning_Background__0004"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0004", "chunk_index": 4, "page_numbers": [6], "text": "- -Steps of the algorithm:\n1. initialize the values of the model parameters, typically at random\n2. iteratively update the parameters in the direction of the negative gradient.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0003", "next_chunk_id": "ALecture3_DeepLearning_Background__0005"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0005", "chunk_index": 5, "page_numbers": [6], "text": ": model parameters ( ùê∞ , ùëè\n```\nùõ≥ C ( ùõ≥ ): ùêø ( ùê∞ , ùëè )\n```\n)", "prev_chunk_id": "ALecture3_DeepLearning_Background__0004", "next_chunk_id": "ALecture3_DeepLearning_Background__0006"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0006", "chunk_index": 6, "page_numbers": [8, 9, 10], "text": "- -Assume that Œ∏ has only one variable\n- -Assume that Œ∏ has two variables { Œ∏ 1 , Œ∏ 2 }\n- -Assume that Œ∏ has two variables { Œ∏ 1 , Œ∏ 2 }\n- Randomly start at ùúÉ 0 : [00] T ‰∏â\n- Compute the gradients of ùê∂ ùúÉ at ùúÉ 0 :\n- Update parameters:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- Compute the gradients of ùê∂ ùúÉ at ùúÉ 1 : ¬∑ ‚Ä¶\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0005", "next_chunk_id": "ALecture3_DeepLearning_Background__0007"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0007", "chunk_index": 7, "page_numbers": [12], "text": "<!-- formula-not-decoded -->\nAfter seeing all training samples, the model can be updated ‚Üí slow", "prev_chunk_id": "ALecture3_DeepLearning_Background__0006", "next_chunk_id": "ALecture3_DeepLearning_Background__0008"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0008", "chunk_index": 8, "page_numbers": [13], "text": "- -Gradient Descent\n- -Stochastic Gradient Descent (SGD)\n<!-- formula-not-decoded -->\n- -Pick a training sample x k\n<!-- formula-not-decoded -->\nTraining Data ( ) ( ) ÔÅª ÔÅΩ ÔÅã , ÀÜ , , ÀÜ , 2 2 1 1 y x y x\n- -If all training samples have the same probability to be picked, then\n<!-- formula-not-decoded -->\nThe model can be updated after seeing one training sample ‚Üí faster", "prev_chunk_id": "ALecture3_DeepLearning_Background__0007", "next_chunk_id": "ALecture3_DeepLearning_Background__0009"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0009", "chunk_index": 9, "page_numbers": [14], "text": "- -When running SGD, the model starts Œ∏ 0\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nsee all training samples once\n‚Üí one epoch", "prev_chunk_id": "ALecture3_DeepLearning_Background__0008", "next_chunk_id": "ALecture3_DeepLearning_Background__0010"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0010", "chunk_index": 10, "page_numbers": [15], "text": "- -Batch Gradient Descent\nUse all K samples in each iteration\n- -Stochastic Gradient Descent (SGD)\n<!-- formula-not-decoded -->\n- -Pick a training sample x k\nUse 1 sample in each iteration\n- -Mini-Batch SGD\n<!-- formula-not-decoded -->\n- -Pick a set of B training samples as a batch b\nUse all B samples in each iteration\nB is the 'batch size'\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0009", "next_chunk_id": "ALecture3_DeepLearning_Background__0011"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0011", "chunk_index": 11, "page_numbers": [16], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0010", "next_chunk_id": "ALecture3_DeepLearning_Background__0012"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0012", "chunk_index": 12, "page_numbers": [17], "text": "- -Regression is useful for 'how much?' and 'how many?' questions, where the target can take a real value.\n- -Many NLP problems are about 'which one?'\n- -Sentiment classification: positive, negative(, neutral)\n- -Sentence boundary detection from text: is the punctuation mark defining a sentence boundary or not\n- -What is the part-ofspeech tag of the word 'word' in this sentence? NOUN, VERB, etc.?\n- -We can represent target labels with one-hot encodings as well. For example, for sentiment classification:\nùë¶ ‚àà {(1,0,0),(0,1,0),(0,0,1)}.\nPositive   Negative  Neutral", "prev_chunk_id": "ALecture3_DeepLearning_Background__0011", "next_chunk_id": "ALecture3_DeepLearning_Background__0013"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0013", "chunk_index": 13, "page_numbers": [18], "text": "- -To estimate the conditional probabilities associated with each class, we need a model with multiple outputs, one per class.\n- -As many linear functions as we have outputs.\n- -Example: Assume our input is represented with 4 features, then to compute the logits ùëú 1 , ùëú 2 , ùëú 3 , for each output, we need:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0012", "next_chunk_id": "ALecture3_DeepLearning_Background__0014"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0014", "chunk_index": 14, "page_numbers": [19], "text": "- -Need to interpret model outputs as probabilities and optimize our parameters to produce probabilities that maximize the likelihood of the observed data\n- -To generate predictions, we can set a threshold or choose the argmax of the predicted probabilities\n- -Interpret the logits ùëú directly as our outputs of interest, however:\n- -Nothing constrains these numbers to sum to 1.\n- -Depending on the inputs, they can take negative values.", "prev_chunk_id": "ALecture3_DeepLearning_Background__0013", "next_chunk_id": "ALecture3_DeepLearning_Background__0015"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0015", "chunk_index": 15, "page_numbers": [20], "text": "- -Transforms logits such that they become nonnegative and sum to 1, while requiring that the model remains differentiable.\n<!-- formula-not-decoded -->\n- -ùë¶ 1+ ùë¶ 2+ ùë¶ 3 = 1,\nÃÇ\nÃÇ\nÃÇ\nÃÇ\n- -0‚â§ ùë¶ ùëñ ‚â§1 for all i , and\n- -The ordering of the logits has not changed; hence we can still pick the output using argmax!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0014", "next_chunk_id": "ALecture3_DeepLearning_Background__0016"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0016", "chunk_index": 16, "page_numbers": [21], "text": "ÃÇ\n- -The softmax function gives us a vector ùê≤ ÃÇ, interpreted as estimated conditional probabilities of each class given the input ùë• , e.g., ùë¶ 1 = ùëÉ ( ùë¶ =cat ‚à£ ùê± ).\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -Maximizing ùëÉ ( ùëå ‚à£ ùëã ) (and thus equivalently minimizing -log ùëÉ ( ùëå ‚à£ ùëã )) corresponds to predicting the label well!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0015", "next_chunk_id": "ALecture3_DeepLearning_Background__0017"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0017", "chunk_index": 17, "page_numbers": [22], "text": "q : the number of classes l(y,y) = y; log yj.\n<!-- formula-not-decoded -->\n- -If we add ùëú into the definition of the loss ùëô and use the definition of the softmax we obtain:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -If we compute its derivative with respect to ùëú j , we get:\n<!-- formula-not-decoded -->\nThe gradient is the difference between the probability assigned to the true class by our model. This makes computing gradients very easy in practice!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0016", "next_chunk_id": "ALecture3_DeepLearning_Background__0018"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0018", "chunk_index": 18, "page_numbers": [23], "text": "- -Gradient Descent\n- -Softmax Regression\n- -Multi-layer Perceptron", "prev_chunk_id": "ALecture3_DeepLearning_Background__0017", "next_chunk_id": "ALecture3_DeepLearning_Background__0019"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0019", "chunk_index": 19, "page_numbers": [24], "text": "- -Linear regression inputs directly to our outputs via a single linear transformation.\n- -But what if our data is not linear?", "prev_chunk_id": "ALecture3_DeepLearning_Background__0018", "next_chunk_id": "ALecture3_DeepLearning_Background__0020"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0020", "chunk_index": 20, "page_numbers": [26], "text": "- -Function approximation\n- -Without non-linearity , deep neural networks work the same as linear transform\n<!-- formula-not-decoded -->\n- -With non-linearity , networks with more layers can approximate more complex functions\nFigure from: http://cs224d.stanford.edu/lectures/CS224d-Lecture4.pdf", "prev_chunk_id": "ALecture3_DeepLearning_Background__0019", "next_chunk_id": "ALecture3_DeepLearning_Background__0021"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0021", "chunk_index": 21, "page_numbers": [27], "text": "<!-- formula-not-decoded -->\nA single neuron can only handle binary classification", "prev_chunk_id": "ALecture3_DeepLearning_Background__0020", "next_chunk_id": "ALecture3_DeepLearning_Background__0022"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0022", "chunk_index": 22, "page_numbers": [28], "text": "- -Handwriting digit classification\nA layer of neurons can handle multiple possible output, and the result depends on the max one\n<!-- formula-not-decoded -->\nWhich one is max?", "prev_chunk_id": "ALecture3_DeepLearning_Background__0021", "next_chunk_id": "ALecture3_DeepLearning_Background__0023"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0023", "chunk_index": 23, "page_numbers": [29], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0022", "next_chunk_id": "ALecture3_DeepLearning_Background__0024"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0024", "chunk_index": 24, "page_numbers": [31, 32, 33], "text": "weights between two layers ‚Üí a matrix\nbias of all neurons at each layer ‚Üí a vector\nLayer -l\nN\n‚Ä¶ ‚Ä¶ nodes l N Layer l ‚Ä¶ ‚Ä¶ 1 nodes 1 -l ‚Ä¶ ‚Ä¶ 1 2 j i : input of the activation function for neuron i at layer l 1 activation function input at each layer ‚Üí a vector 2 2 -1 a 1 7 w i1 m 2 2 Z C wi1ai l-1 + W'2a2 1 l-1 ÂçÅ ... +b' 2 N-1 Z M l-1 b w a 2] j=1 b Z -1 Wj a 7 b 2", "prev_chunk_id": "ALecture3_DeepLearning_Background__0023", "next_chunk_id": "ALecture3_DeepLearning_Background__0025"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0025", "chunk_index": 25, "page_numbers": [34], "text": "l\ni a l a l i z l i b l b : output of a neuron : output vector of a layer : input of activation function activation function for a : a weight : a weight matrix : a bias : a bias vector\nl z : input vector of layer", "prev_chunk_id": "ALecture3_DeepLearning_Background__0024", "next_chunk_id": "ALecture3_DeepLearning_Background__0026"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0026", "chunk_index": 26, "page_numbers": [35], "text": "l", "prev_chunk_id": "ALecture3_DeepLearning_Background__0025", "next_chunk_id": "ALecture3_DeepLearning_Background__0027"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0027", "chunk_index": 27, "page_numbers": [37], "text": "l\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0026", "next_chunk_id": "ALecture3_DeepLearning_Background__0028"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0028", "chunk_index": 28, "page_numbers": [39, 40], "text": "- -Fully connected feedforward network\n<!-- formula-not-decoded -->\n- -Fully connected feedforward network\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0027", "next_chunk_id": "ALecture3_DeepLearning_Background__0029"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0029", "chunk_index": 29, "page_numbers": [41], "text": "Back to an earlier slide!", "prev_chunk_id": "ALecture3_DeepLearning_Background__0028", "next_chunk_id": "ALecture3_DeepLearning_Background__0030"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0030", "chunk_index": 30, "page_numbers": [42], "text": "```\nAlgorithm Initialization: start at ùúÉ 0 while( ùúÉ (ùëñ+1) ‚â† ùúÉ ùëñ ) { compute gradient at ùúÉ ùëñ update parameters } ..¬∑ o(W2Œ±(W1 + bl) + b¬≤)... + bL) y=f(x)=o(WL 1,bl,W2,b¬≤,...WL, W1. A w w 12 b Wl W21 W22 Ôºö 0C(0) Cme VC(0) ‰∫å gi+1 Ôºö C(0) Ob, get1 ‚Üë ‰∏Ä nVA C(0)\n```\nTo update weights efficiently, we use backpropagation .", "prev_chunk_id": "ALecture3_DeepLearning_Background__0029", "next_chunk_id": "ALecture3_DeepLearning_Background__0031"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0031", "chunk_index": 31, "page_numbers": [43], "text": "- -forward propagation\n- -from input ùë• to output ùë¶ information flows forward through the network\n- -during training, forward propagation can continue onward until it produces a scalar loss C ( Œ∏ )\n- -back-propagation\n- -allows the information from the cost to then flow backwards through the network, in order to compute the gradient\n- -can be applied to any function", "prev_chunk_id": "ALecture3_DeepLearning_Background__0030", "next_chunk_id": "ALecture3_DeepLearning_Background__0032"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0032", "chunk_index": 32, "page_numbers": [44], "text": "<!-- formula-not-decoded -->\nforward propagation for computing loss\n<!-- formula-not-decoded -->\nback-propagation of gradients for updating weights", "prev_chunk_id": "ALecture3_DeepLearning_Background__0031", "next_chunk_id": "ALecture3_DeepLearning_Background__0033"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0033", "chunk_index": 33, "page_numbers": [45], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n```\nAlgorithm Initialization: start at ùúÉ 0 while( ùúÉ (ùëñ+1) ‚â† ùúÉ ùëñ ) { compute gradient at ùúÉ ùëñ update parameters } Ai+1 0¬≤ - nVC(0¬∞) ‚Üë ‰∏Ä\n```\nTo update weights efficiently, we use backpropagation .", "prev_chunk_id": "ALecture3_DeepLearning_Background__0032", "next_chunk_id": "ALecture3_DeepLearning_Background__0034"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0034", "chunk_index": 34, "page_numbers": [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIdea: computing ùõø ùëô layer by layer (from ùõø ùêø to ùõø 1 ) is more efficient\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùúπ ùë≥\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\n<!-- formula-not-decoded -->\nùúïùê∂Œ§ùúïùë¶ùëñ depends on the loss function\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùúπ ùë≥\n- ÔÇÇ Compute ùõø ùëô based on ùõø ùëô+1\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùúπ ùíç based on ùúπ ùíç+ùüè 1 + l\n1\nŒîz\nŒîz\nl\ni\nl\ni\n1\n+\nl\n2\nŒîz\nl\nk\n‚Ä¶ ‚Ä¶\n‚ñ≥C\n<!-- formula-not-decoded -->\n+\n1\nŒîz\n‚Üí\nŒîa\n- -Rethink the propagation\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -Idea: from L to 1\n- ÔÇÅ Initialization: compute ùõø ùêø\n- ÔÇÇ Compute ùõø ùëô-1 based on ùõø ùëô\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0033", "next_chunk_id": "ALecture3_DeepLearning_Background__0035"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0035", "chunk_index": 35, "page_numbers": [60], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0034", "next_chunk_id": "ALecture3_DeepLearning_Background__0036"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0036", "chunk_index": 36, "page_numbers": [60], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0035", "next_chunk_id": "ALecture3_DeepLearning_Background__0037"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0037", "chunk_index": 37, "page_numbers": [61], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0036", "next_chunk_id": "ALecture3_DeepLearning_Background__0038"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0038", "chunk_index": 38, "page_numbers": [61], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture3_DeepLearning_Background__0037", "next_chunk_id": "ALecture3_DeepLearning_Background__0039"}
{"lecture_id": "ALecture3_DeepLearning_Background", "source_pdf": "data/anlp/ALecture3_DeepLearning_Background.pdf", "chunk_id": "ALecture3_DeepLearning_Background__0039", "chunk_index": 39, "page_numbers": [63], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture3_DeepLearning_Background__0038", "next_chunk_id": null}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0000", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002", "chunk_index": 2, "page_numbers": [3], "text": "- Typically, words are treated as discrete, arbitrary symbols in NLP systems.\n- But words have lots of interesting relationships to each other!\n- A lot of the previous work examined how to represent these:\n- Manually-built resources like WordNet provide one way to define words and their similarities\n- Distributional representations  and word embeddings are another way\n- And they can be  learned automatically from large text collections", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0001", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Characterizes knowledge of a word in terms of 'the company it keeps' (Firth, 1957)\n- -Word categories can be defined by the context in which they appear\n- -Such characterization is the idea behind static word representations estimated from data (Mikolov et al., 2013) and modern large language models.\n- -For a word w, find all the contexts w 1 ww2 in which w appears\n- -Find all words w' that share many frequent contexts\n- -Use a LARGE text collection to get good counts", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0002", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004", "chunk_index": 4, "page_numbers": [5], "text": "- -Representing discrete types (i.e., words) as vectors\n- -Each word = a vector\n- -Similar words are \"nearby in space\"", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0003", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005", "chunk_index": 5, "page_numbers": [6], "text": "- -Called an \"embedding\" because it's embedded into a space\n- -The standard way to represent meaning in NLP\n- -Fine-grained model of meaning for similarity\n- -NLP tasks like sentiment analysis\n- -With words,  requires same word to be in training and test\n- -With embeddings: ok if similar words occur.\n- -Question answering, conversational agents, etc.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0004", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -Term frequency (TF), term frequency, inverse document frequency (TF.IDF)\n- -A common baseline model\n- -Words are represented by a simple function of the counts of nearby words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0005", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007", "chunk_index": 7, "page_numbers": [7], "text": "- -E.g. Word2Vec, Representation is created by training a classifier to distinguish nearby and far-away words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0006", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008", "chunk_index": 8, "page_numbers": [8], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0007", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009", "chunk_index": 9, "page_numbers": [9], "text": "apricot, aardvark = 0. apricot, computer digital = 0. apricot, data = 0. apricot, pinch = 1. apricot, result = 0. apricot, sugar = 1. apricot, ‚Ä¶ = . pineapple, aardvark = 0. pineapple, computer digital = 0. pineapple, data = 0. pineapple, pinch = 1. pineapple, result = 0. pineapple, sugar = 1. pineapple, ‚Ä¶ = . digital computer, aardvark = 0. digital computer, computer digital = 2. digital computer, data = 1. digital computer, pinch = 0. digital computer, result = 1. digital computer, sugar = 0. digital computer, ‚Ä¶ = . information, aardvark = 0. information, computer digital = 1. information, data = 6. information, pinch = 0. information, result = 4. information, sugar = 0. information, ‚Ä¶ = \n9", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0008", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010", "chunk_index": 10, "page_numbers": [10], "text": "- -Count how often 'apple' occurs close to other words in a large text collection (corpus):\n- -Do the same for 'orange':\n- -Interpret counts as coordinates.\neat, 1 = fall. eat, 2 = ripe. eat, 3 = slice. eat, 4 = peel. eat, 5 = tree. eat, 6 = throw. eat, 7 = fruit. eat, 8 = pie. eat, 9 = bite. eat, 10 = crab. 794, 1 = 244. 794, 2 = 47. 794, 3 = 221. 794, 4 = 208. 794, 5 = 160. 794, 6 = 145. 794, 7 = 156. 794, 8 = 109. 794, 9 = 104. 794, 10 = 88\neat, 1 = fall. eat, 2 = ripe. eat, 3 = slice. eat, 4 = peel. eat, 5 = tree. eat, 6 = throw. eat, 7 = fruit. eat, 8 = pie. eat, 9 = bite. eat, 10 = crab. 265, 1 = 22. 265, 2 = 25. 265, 3 = 62. 265, 4 = 220. 265, 5 = 64. 265, 6 = 74. 265, 7 = 111. 265, 8 = 4. 265, 9 = 4. 265, 10 = 8\nEvery context word becomes a dimension.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0009", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011", "chunk_index": 11, "page_numbers": [11], "text": "Similarity between two words as proximity in space", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0010", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012", "chunk_index": 12, "page_numbers": [12], "text": "Use the angle between vectors instead of point distance to get around word frequency issues", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0011", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013", "chunk_index": 13, "page_numbers": [13], "text": "- -Function words co-occur frequently with all words\n- -That makes them less informative\n- -They have much higher co-occurrence counts than content words\n- -They can 'drown out' more informative contexts\n- -Some counts for 'letter' in 'Pride and Prejudice'.\nthe, 1 = to. the, 2 = of. the, 3 = and. the, 4 = a. the, 5 = her. the, 6 = she. the, 7 = his. the, 8 = is. the, 9 = was. the, 10 = in. the, 11 = that. 102, 1 = 75. 102, 2 = 72. 102, 3 = 56. 102, 4 = 52. 102, 5 = 50. 102, 6 = 41. 102, 7 = 36. 102, 8 = 35. 102, 9 = 34. 102, 10 = 34. 102, 11 = 33. had, 1 = i from. had, 2 = i from. had, 3 = you. had, 4 = as. had, 5 = . had, 6 = this. had, 7 = mr. had, 8 = for. had, 9 = on. had, 10 = be. had, 11 = he. 32, 1 = 28 28 25. 32, 2 = 28 28 25. 32, 3 = 28 28 25. 32, 4 = 23 23 with. 32, 5 = 23 23 with. 32, 6 = 22 him. 32, 7 = 22 him. 32, 8 = 21 which. 32, 9 = 20 by when. 32, 10 = 18. 32, 11 = 17 jane. but 17, 1 = but 17. but 17, 2 = elizabeth 17. but 17, 3 = elizabeth 17. but 17, 4 = 16. but 17, 5 = 16. but 17, 6 = 16. but 17, 7 = 16. but 17, 8 = 16. but 17, 9 = 15 14. but 17, 10 = 15 14. but 17, 11 = 12\nAll the most frequent cooccurring words are function words.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0012", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014", "chunk_index": 14, "page_numbers": [14], "text": "- tf: term frequency . frequency count (usually log-transformed):\n- Idf: inverse document frequency:\nWords like \"the\" or \"good\" have very low idf\ntf-idf value for word t in document d:\nAlternative to tf.idf: pointwise mutual information", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0013", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015", "chunk_index": 15, "page_numbers": [15], "text": "- -Whether a context word is particularly informative about the target word.\n- -Degree of association between target and context,  rather than cooccurrence:\n- -High association: high co-occurrence with specific words, lower with everything else\n- -Low association: lots of co-occurrence with all words", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0014", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016", "chunk_index": 16, "page_numbers": [16], "text": "Do events x and y co-occur more than if they were independent?\n<!-- formula-not-decoded -->\nPMI between two words :  (Church & Hanks 1989)\nDo words x and y co-occur more than if they were independent?\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0015", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017", "chunk_index": 17, "page_numbers": [17], "text": "- -long (length |V|= 20,000 to 50,000)\n- -sparse (most elements are zero)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0016", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018", "chunk_index": 18, "page_numbers": [18], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0017", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019", "chunk_index": 19, "page_numbers": [19], "text": "- -AIM: learn vectors to represent the semantics of a word from its contextual uses (just like distributional measure)\n- -Dense vectors\n- -Words that have similar meanings -> close to each other in the vector space different meanings -> far from each other\n- -short (length 50-1000)\n- -dense (most elements are non-zero)\n- -Why dense vectors?\n- -What are they good for?\n- -How are they built?\n- -How are they used?\n- -Why are they an improvement over distributional semantics precursor?", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0018", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020", "chunk_index": 20, "page_numbers": [20], "text": "- -Why dense vectors?\n- -Short vectors may be easier to use as features in machine learning (less weights to tune)\n- -Dense vectors may generalize better than storing explicit counts\n- -They may do better at capturing synonymy:\n- -car and automobile are synonyms; but are distinct dimensions\n- -a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren't\n- -In practice, they work better", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0019", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021", "chunk_index": 21, "page_numbers": [21], "text": "- -Train with just words (i.e., unlabeled data), in a supervised fashion, by constructing auxiliary tasks:\n- -Language modeling: Given a sequence of words, predict the next word:\n<!-- formula-not-decoded -->\n- -Given lexical context (i.e., previous and following words), predict the missing one ( CBOW model ):\nGiven w1 ,‚Ä¶,w n-1 , and w n+1 ,‚Ä¶, wn+k , predict w n\n- -Given a word, predict words that occur within a window (independent of their position) ( skip-gram model ):\nGiven wn , predict w 1 ,‚Ä¶,w n-1 , and w n+1 ,‚Ä¶, wn+k", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0020", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022", "chunk_index": 22, "page_numbers": [22], "text": "- -Word2Vec\n- -Stanford's GLoVe\n- -Facebook's FastText\n- -Typical format of word embeddings:\n6op -1.242 -0.3600.5730.367 0.600 -0.189 91.273\ncat -0.964 4-0.610 0.6740.351 0.413 -0.212 1.380", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0021", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023", "chunk_index": 23, "page_numbers": [23], "text": "- -Idea: predict rather than count\n- -Word2vec (Mikolov et al., 2013)\n- -https://code.google.com/archive/p/word2vec/\n- -Instead of counting how often each word w occurs near \"apricot\"\n- -Train a classifier on a binary prediction task:\n- -Is w likely to show up near \"apricot\"?\n- -Fake task: We don't actually care about this task\n- -But we'll take the learned classifier weights as the word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0022", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024", "chunk_index": 24, "page_numbers": [24], "text": "- -Goal: predict surrounding words within a window of each word\n- -Objective function: maximize the probability of any context word given the current center word\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0023", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025", "chunk_index": 25, "page_numbers": [25], "text": "- -Goal: predict surrounding words within a window of each word", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0024", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026", "chunk_index": 26, "page_numbers": [26], "text": "words 10,000", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0025", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027", "chunk_index": 27, "page_numbers": [27, 28], "text": "- -Hidden layer weight matrix = word vector lookup\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n10,000words\nEach vocabulary entry has two vectors: as a target word and as a context word\nneurons\nEach vocabulary entry has two vectors: as a target word and as a context word", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0026", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028", "chunk_index": 28, "page_numbers": [30], "text": "- Training sentence:\n- ... lemon, a tablespoon of apricot preserves   or pinch ...\n- c1        c2      t c3             c4\n- For each positive example, create k negative examples.\n- Using noise words\n- Any random word that isn't t", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0027", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029", "chunk_index": 29, "page_numbers": [31], "text": "- Training sentence:\n- ... lemon, a tablespoon of apricot preserves or pinch ...\n- c1         c2     t c3        c4", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0028", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030", "chunk_index": 30, "page_numbers": [32], "text": "- -Could pick w according to their unigram frequency P(w)\n- -More common to chose them according to p Œ± (w)\n- -Œ± = ¬æ works well because it gives rare noise words slightly higher probability\n- -To show this, imagine two events p(a)=.99 and p(b) = .01:\nIdea: less frequent words sampled more often.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0029", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031", "chunk_index": 31, "page_numbers": [33], "text": "- -Relations between words, captured by vector operations\n- -Lots of online lectures/tutorial\n- -https://www.slideshare.net/mlprague/tom-mikolov-distributedrepresentations-for-nlp\n- There are multiple degrees of similarity among words:\n- KING is similar to QUEEN as MAN is similar to WOMAN\n- KING is similar to KINGS as MAN is similar to MEN\n- Simple vector operations with the word vectors provide very intuitive results\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0030", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032", "chunk_index": 32, "page_numbers": [34], "text": "vector( 'king' ) - vector( 'man' ) + vector( 'woman' ) ‚âà vector('queen') vector( 'Paris' ) - vector( 'France' ) + vector( 'Italy' ) ‚âà vector('Rome')\nFrom Presentation by Mikolov", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0031", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033", "chunk_index": 33, "page_numbers": [36], "text": "0.6", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0032", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034", "chunk_index": 34, "page_numbers": [37], "text": "Paris - France + Italy bigger - big + cold sushi - Japan + Germany Cu - copper + gold Windows - Microsoft + Google Montreal Canadiens - Montreal + Toronto, Nearest token = Rome colder bratwurst Au Android Toronto Maple Leafs", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0033", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035", "chunk_index": 35, "page_numbers": [38], "text": "Czech + currency Vietnam + capital German + airlines Russian + river French + actress, Nearest tokens = koruna, Czech crown, Polish zloty, CTK Hanoi, Ho Chi Minh City, Viet Nam, Vietnamese airline Lufthansa, carrier Lufthansa, flag carrier Lufthansa Moscow, Volga River, upriver, Russia Juliette Binoche, Vanessa Paradis, Charlotte Gainsbourg", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0034", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036", "chunk_index": 36, "page_numbers": [40], "text": "- -large amounts of monolingual data for source and target language\n- -Small amount of bilingual data\n- -Word Vectors have similar structures in two languages\n- -Need to learn projection from one language to another\n- -Can then translate any word seen in the monolingual data", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0035", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037", "chunk_index": 37, "page_numbers": [41], "text": "Similar idea works for other NLP tasks and transferring models for NLP tasks too. For example, spoken language understanding (Upadhyay et al, IEEE ICASSP, 2018).", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0036", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038", "chunk_index": 38, "page_numbers": [42], "text": "- -Compare to human scores on word similarity-type tasks:\n- WordSim-353 (Finkelstein et al., 2002)\n- -SimLex-999 (Hill et al., 2015)\n- Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012)\n- TOEFL dataset: Levied is closest in meaning to: imposed, believed, requested, correlated", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0037", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039", "chunk_index": 39, "page_numbers": [43], "text": "- -Goal: use word vectors in neural net models built for subsequent tasks\n- -Benefit\n- -Ability to also classify words accurately\n- -Ex. countries cluster together a classifying location words should be possible with word vectors\n- -Incorporate any information into them other tasks\n- -Ex. project sentiment into words to find most positive/negative words in corpus", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0038", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040", "chunk_index": 40, "page_numbers": [44], "text": "- -Global Vectors for Word Representation (GloVe)\n- -Methods like skipgram do good on the analogy task, but‚Ä¶\n- -They poorly utilize the statistics of the corpus, since they train on separate local context windows instead of on global co-occurrence counts.\n- -A specific weighted least squares model that trains on global word-word cooccurrence counts and thus makes efficient use of statistics.\n- -GloVe learns based on a co-occurrence matrix and trains word vectors so their differences predict co-occurrence ratios.\n- -Source code for the model as well as trained word vectors at https://nlp.stanford.edu/projects/glove/\n- -Pennington et al., 2014\n- Newer Version: Carlson et al., 2025", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0039", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041", "chunk_index": 41, "page_numbers": [45, 46], "text": "- -Idea: ratio of co-occurrence probability can encode meaning\n- -Co-occurrence Probability, Pij is the probability that word wj appears in\n<!-- formula-not-decoded -->\n- -Relationship between the words wi and wj\nP ( x | ice) P ( x | ice), x = solid x = solid = 1.9√ó10 -4 large. P ( x | ice) P ( x | ice), x = gas x = gas = 6.6√ó10 -5 small. P ( x | ice) P ( x | ice), x = water x = water = 3.0√ó10 -3 large. P ( x | ice) P ( x | ice), x = fashion x = random = 1.7√ó10 -5 small. P ( x | steam) P ( x | steam), x = solid x = solid = 2.2√ó10 -5 small. P ( x | steam) P ( x | steam), x = gas x = gas = 7.8√ó10 -4 large. P ( x | steam) P ( x | steam), x = water x = water = 2.2√ó10 -3 large. P ( x | steam) P ( x | steam), x = fashion x = random = 1.8√ó10 -5 small. P x | ice P x | steam P x | ice P x | steam, x = solid x = solid = 8.9 large. P x | ice P x | steam P x | ice P x | steam, x = gas x = gas = 8.5√ó10 -2 small. P x | ice P x | steam P x | ice P x | steam, x = water x = water = 1.36 ~ 1. P x | ice P x | steam P x | ice P x | steam, x = fashion x = random = 0.96 ~ 1\nThe relationship of wi and wj can be examined by studying the ratio of their co-occurrence probabilities with various probe words\n- -The model enforces this property by learning embeddings ùë§ùëñ, ‡∑• ùë§ùëó such that:\n- -New, weighted least squares regression:\n<!-- formula-not-decoded -->\nWeighting function\n- -f(x) is a weighting function that down-weights very rare and very frequent co-occurrences (so common words like ' the ' don ' t dominate).\n<!-- formula-not-decoded -->\nCmax", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0040", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042", "chunk_index": 42, "page_numbers": [47], "text": "- -Word2Vec uses local context (a sliding window over a corpus), whereas GloVe uses global co-occurrence statistics.\n- -GloVe needs big co-occurrence matrix upfront", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0041", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043", "chunk_index": 43, "page_numbers": [48], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0042", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044", "chunk_index": 44, "page_numbers": [49], "text": "- Goal: estimate the probability of a word sequence\n<!-- formula-not-decoded -->\n- Example task: determine whether a sequence is grammatical or makes more sense\nIf P(recognize speech)\n- > P(wreck a nice beach)\nOutput =  'recognize speech'\nrecognize speech or wreck a nice beach", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0043", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045", "chunk_index": 45, "page_numbers": [50], "text": "- Goal: estimate the probability of a word sequence\n<!-- formula-not-decoded -->\n- N-gram language model\n- Probability is conditioned on a window of ( n1) previous words\n<!-- formula-not-decoded -->\n- Estimate the probability based on the training data\n<!-- formula-not-decoded -->\nIssue: some sequences may not appear in the training data", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0044", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046", "chunk_index": 46, "page_numbers": [51], "text": "- Training data:\n- The dog ran ‚Ä¶‚Ä¶\n- The cat jumped ‚Ä¶‚Ä¶\n- The probability is not accurate.\n- The phenomenon happens because we cannot collect all the possible text in the world as training data.\n```\nP( jumped | dog ) = 0 P( ran | cat ) = 0 0.0001 0.0001\n```\ngive some small probability ‚Üí smoothing", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0045", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047", "chunk_index": 47, "page_numbers": [52], "text": "<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0046", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048", "chunk_index": 48, "page_numbers": [53], "text": "- -Corpus of 1K tokens, |V| = 20, C(the) = 100 C(orange) = 14, C(apple) = 0\n- -Before smoothing:\n<!-- formula-not-decoded -->\nP(orange) = 14/1000 = 0.014\nP(apple) = 0/1000 = 0\n- -After smoothing:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0047", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049", "chunk_index": 49, "page_numbers": [55], "text": "- -Idea: estimate not from counts, but based on the NN prediction Wi-(n-1); Wi-1) 7\nP( ' wreck a nice beach ' ) = P(wreck|START)P(a|wreck)P(nice|a)P(beach|nice)", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0048", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050", "chunk_index": 50, "page_numbers": [56, 57], "text": "<!-- formula-not-decoded -->\n- -The input layer (or hidden layer) of the related words are close\n- -If P(jump|dog) is large, P(jump|cat) increases accordingly (even there is no '‚Ä¶ cat jump ‚Ä¶' in the data)\nSmoothing is automatically done\nIssue: fixed context window for conditioning", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0049", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051", "chunk_index": 51, "page_numbers": [58], "text": "- -Distributional Similarity\n- -Sparse Word Representations\n- -Word Embeddings and Word2Vec\n- -Language Modeling\n- -Unexpected things we learn with word embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0050", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052", "chunk_index": 52, "page_numbers": [59], "text": "Semantics derived automatically from language corpora contain human-like biases, Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan, Science 2017.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0051", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053", "chunk_index": 53, "page_numbers": [60], "text": "Automatically generate he : x :: she : y analogies.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0052", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054", "chunk_index": 54, "page_numbers": [61], "text": "Automatically generate he : x :: she : y analogies.", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0053", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055", "chunk_index": 55, "page_numbers": [62], "text": "- ùë£ùëöùëéùëõ - ùë£ùë§ùëúùëöùëéùëõ +ùë£ùë¢ùëõùëêùëôùëí ‚àº ùë£ùëéùë¢ùëõùë°\nhe: ________, 1 = she:_______. brother, 1 = sister. barbershop, 1 = salon. carpentry, 1 = sewing. bartender, 1 = hostess. physician, 1 = registered_nurse. professor, 1 = associate professor\nGoogle w2v embedding trained from the news", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0054", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056", "chunk_index": 56, "page_numbers": [63], "text": "Typical Caucasian names, 1 = Typical African American names. Molly, 1 = Aisha. Amy, 1 = Keisha. Jake, 1 = Leroy. Luke, 1 = Jermaine. singer, 1 = rapper. lobster, 1 = shrimp. geeks, 1 = dudes. hockey, 1 = basketball. urban, 1 = slums", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0055", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057", "chunk_index": 57, "page_numbers": [64], "text": "19% of the top 150 analogies were rated as gender stereotypical by a majority of crowdworkers\nmidwife:doctor, Before executing debiasing.Appropriate = 1. midwife:doctor, Before executing debiasing.Biased = 10. sewing:carpentry, Before executing debiasing.Appropriate = 2. sewing:carpentry, Before executing debiasing.Biased = 9. pediatrician:orthopedic surgeon, Before executing debiasing.Appropriate = 0. pediatrician:orthopedic surgeon, Before executing debiasing.Biased = 9. registered nurse:physician, Before executing debiasing.Appropriate = 1. registered nurse:physician, Before executing debiasing.Biased = 9. housewife:shopkeeper, Before executing debiasing.Appropriate = 1. housewife:shopkeeper, Before executing debiasing.Biased = 9", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0056", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058", "chunk_index": 58, "page_numbers": [65], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0057", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059", "chunk_index": 59, "page_numbers": [65], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0058", "next_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0060"}
{"lecture_id": "ALecture4_LanguageModels_WordEmbeddings", "source_pdf": "data/anlp/ALecture4_LanguageModels_WordEmbeddings.pdf", "chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0060", "chunk_index": 60, "page_numbers": [66], "text": "- -Previous lectures are not recorded, but previous slides are under Modules in Canvas\n- -The homework timeline is on the main Canvas page for the course\n- -Immediate thing to act on:\n- -Form a final project team!", "prev_chunk_id": "ALecture4_LanguageModels_WordEmbeddings__0059", "next_chunk_id": null}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture5_SequenceModeling__0001"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0000", "next_chunk_id": "ALecture5_SequenceModeling__0002"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -We have discussed models that deal with paired data: input words or utterances and output categories.\n- -Example: One-hot representations or word embeddings to represent input words\n- -Sometimes data exhibits rich structure, such as images, natural language.\n- -Structure-less networks, i.e., MLPs, can fall short.\n- -CNNs: a type of NNs well-suited to detecting spatial substructure.", "prev_chunk_id": "ALecture5_SequenceModeling__0001", "next_chunk_id": "ALecture5_SequenceModeling__0003"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0003", "chunk_index": 3, "page_numbers": [4, 5], "text": "- -Some patterns are much smaller than the whole image\nA neuron does not have to see the whole image to discover the pattern.\nConnecting to small region with fewer parameters\n- -The same pattern can appear in different regions.", "prev_chunk_id": "ALecture5_SequenceModeling__0002", "next_chunk_id": "ALecture5_SequenceModeling__0004"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0004", "chunk_index": 4, "page_numbers": [6], "text": "- -Recognizing original nationalities from last names O' Neill O' Shaughnessy Anton opoulos Kost opoulos Giann opoulos\n- -Sentiment classification A delicious breakfast was served to us at Pillerago that morning.\nThe brunch at Margoli, especially the scones, were delicious .", "prev_chunk_id": "ALecture5_SequenceModeling__0003", "next_chunk_id": "ALecture5_SequenceModeling__0005"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0005", "chunk_index": 5, "page_numbers": [13], "text": "- -CNNs: a type of NNs well-suited to detecting spatial substructure.", "prev_chunk_id": "ALecture5_SequenceModeling__0004", "next_chunk_id": "ALecture5_SequenceModeling__0006"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0006", "chunk_index": 6, "page_numbers": [14], "text": "- -Dimension of the Convolution Operation", "prev_chunk_id": "ALecture5_SequenceModeling__0005", "next_chunk_id": "ALecture5_SequenceModeling__0007"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0007", "chunk_index": 7, "page_numbers": [15, 16], "text": "- -Channels\n- -input_channels=2, output_channels=1, kernel_size=2, stride=1, padding=0\n- Channels\n- input_channels=1, output_channels=2, kernel_size=2, stride=1, padding=0\nInput tensor with 1 channel and two convolutional kernels\nThe kernels being applied to the input tensor\nThe output tensor", "prev_chunk_id": "ALecture5_SequenceModeling__0006", "next_chunk_id": "ALecture5_SequenceModeling__0008"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0008", "chunk_index": 8, "page_numbers": [17], "text": "An input matrix and a convolutional kernel\nThe kernel being applied to the input matrix\nThe output matrix", "prev_chunk_id": "ALecture5_SequenceModeling__0007", "next_chunk_id": "ALecture5_SequenceModeling__0009"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0009", "chunk_index": 9, "page_numbers": [18], "text": "Stride = 1\nStride = 2", "prev_chunk_id": "ALecture5_SequenceModeling__0008", "next_chunk_id": "ALecture5_SequenceModeling__0010"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0010", "chunk_index": 10, "page_numbers": [20], "text": "http://cs231n.github.io/convolutional-networks/", "prev_chunk_id": "ALecture5_SequenceModeling__0009", "next_chunk_id": "ALecture5_SequenceModeling__0011"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0011", "chunk_index": 11, "page_numbers": [24], "text": "- The convolution between two functions, say ùëì , ùëî : ‚Ñù ùëë ‚Üí ùëÖ is defined as:\n<!-- formula-not-decoded -->\n- Whenever we have discrete objects, the integral turns into a sum.\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture5_SequenceModeling__0010", "next_chunk_id": "ALecture5_SequenceModeling__0012"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0012", "chunk_index": 12, "page_numbers": [29], "text": "- -Fully-connected layer and softmax layer\n- -need fixed-size input", "prev_chunk_id": "ALecture5_SequenceModeling__0011", "next_chunk_id": "ALecture5_SequenceModeling__0013"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0013", "chunk_index": 13, "page_numbers": [30], "text": "- -choose the k-max values\n- -preserve the order of input values\n- -variable-size input, fixed-size output", "prev_chunk_id": "ALecture5_SequenceModeling__0012", "next_chunk_id": "ALecture5_SequenceModeling__0014"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0014", "chunk_index": 14, "page_numbers": [32], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0013", "next_chunk_id": "ALecture5_SequenceModeling__0015"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0015", "chunk_index": 15, "page_numbers": [33], "text": "- -Idea: condition the neural network on all previous words and tie the weights at each time step\n- -Assumption: temporal information matters", "prev_chunk_id": "ALecture5_SequenceModeling__0014", "next_chunk_id": "ALecture5_SequenceModeling__0016"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0016", "chunk_index": 16, "page_numbers": [34], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture5_SequenceModeling__0015", "next_chunk_id": "ALecture5_SequenceModeling__0017"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0017", "chunk_index": 17, "page_numbers": [35], "text": "Idea: pass the information from the previous hidden layer to leverage all contexts", "prev_chunk_id": "ALecture5_SequenceModeling__0016", "next_chunk_id": "ALecture5_SequenceModeling__0018"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0018", "chunk_index": 18, "page_numbers": [42], "text": "- Input: init, x\n1 , x 2 , ‚Ä¶, x t\n- Output: o\nt\n- Target: y\nt", "prev_chunk_id": "ALecture5_SequenceModeling__0017", "next_chunk_id": "ALecture5_SequenceModeling__0019"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0019", "chunk_index": 19, "page_numbers": [44], "text": "- Input: init, x 1 , x 2 , ‚Ä¶, x t\n- Output: o t\n- Target: y\nt", "prev_chunk_id": "ALecture5_SequenceModeling__0018", "next_chunk_id": "ALecture5_SequenceModeling__0020"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0020", "chunk_index": 20, "page_numbers": [46], "text": "- -The gradient is a product of Jacobian matrices, each associated with a step in the forward computation\n- -Multiply the same matrix at each time step during backprop\n<!-- formula-not-decoded -->\nThe gradient becomes very small or very large quickly ‚Üí vanishing or exploding gradient", "prev_chunk_id": "ALecture5_SequenceModeling__0019", "next_chunk_id": "ALecture5_SequenceModeling__0021"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0021", "chunk_index": 21, "page_numbers": [47], "text": "Idea: control the gradient value to avoid exploding\n<!-- formula-not-decoded -->\nParameter setting: values from half to ten times the average can still yield convergence", "prev_chunk_id": "ALecture5_SequenceModeling__0020", "next_chunk_id": "ALecture5_SequenceModeling__0022"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0022", "chunk_index": 22, "page_numbers": [48], "text": "- -RNN models temporal sequence information\n- -can handle 'long -term dependencies' in theory\nIssue: RNN cannot handle such 'long -term dependencies' i n practice due to vanishing gradient ‚Üí apply the gating mechanism to directly encode the long-distance information", "prev_chunk_id": "ALecture5_SequenceModeling__0021", "next_chunk_id": "ALecture5_SequenceModeling__0023"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0023", "chunk_index": 23, "page_numbers": [49], "text": "‚Ñé = ‚Ñé; ‚Ñé represents (summarizes) the past and future around a single token", "prev_chunk_id": "ALecture5_SequenceModeling__0022", "next_chunk_id": "ALecture5_SequenceModeling__0024"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0024", "chunk_index": 24, "page_numbers": [50], "text": "Each memory layer passes an intermediate representation to the next", "prev_chunk_id": "ALecture5_SequenceModeling__0023", "next_chunk_id": "ALecture5_SequenceModeling__0025"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0025", "chunk_index": 25, "page_numbers": [51], "text": "- -Convolutional Neural Networks\n- -Recurrent Neural Networks\n- -Example Applications", "prev_chunk_id": "ALecture5_SequenceModeling__0024", "next_chunk_id": "ALecture5_SequenceModeling__0026"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0026", "chunk_index": 26, "page_numbers": [52], "text": "- -Example depiction from: http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnnperform-text-classification-with-word-embeddings/", "prev_chunk_id": "ALecture5_SequenceModeling__0025", "next_chunk_id": "ALecture5_SequenceModeling__0027"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0027", "chunk_index": 27, "page_numbers": [53], "text": "- -Example depiction from: http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnnperform-text-classification-with-word-embeddings/", "prev_chunk_id": "ALecture5_SequenceModeling__0026", "next_chunk_id": "ALecture5_SequenceModeling__0028"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0028", "chunk_index": 28, "page_numbers": [54], "text": "- -(Lee & Dernoncourt, NAACL, 2016)\n- -Dialogue Act Classification", "prev_chunk_id": "ALecture5_SequenceModeling__0027", "next_chunk_id": "ALecture5_SequenceModeling__0029"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0029", "chunk_index": 29, "page_numbers": [55], "text": "- -(Ravuri and Stolcke, Interspeech, 2015)\n- -Addressee Detection", "prev_chunk_id": "ALecture5_SequenceModeling__0028", "next_chunk_id": "ALecture5_SequenceModeling__0030"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0030", "chunk_index": 30, "page_numbers": [56], "text": "- -(Mesnil et al., IEEE TASLP, 2015)\n- -Slot tagging", "prev_chunk_id": "ALecture5_SequenceModeling__0029", "next_chunk_id": "ALecture5_SequenceModeling__0031"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0031", "chunk_index": 31, "page_numbers": [57], "text": "- -(HakkaniT√ºr et al., Interspeech, 2016 )\n- -Slot filling (or tagging) and intent prediction in the same output sequence", "prev_chunk_id": "ALecture5_SequenceModeling__0030", "next_chunk_id": "ALecture5_SequenceModeling__0032"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0032", "chunk_index": 32, "page_numbers": [58], "text": "- -(Tafforeau et al., Interspeech, 2016)\n- -Goal: exploit data from domains/tasks with a lot of data to improve ones with less data\n- -Lower layers are shared across domains/tasks\n- -Output layer is specific to task", "prev_chunk_id": "ALecture5_SequenceModeling__0031", "next_chunk_id": "ALecture5_SequenceModeling__0033"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0033", "chunk_index": 33, "page_numbers": [59], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture5_SequenceModeling__0032", "next_chunk_id": "ALecture5_SequenceModeling__0034"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0034", "chunk_index": 34, "page_numbers": [60], "text": "- -September 30 th\n- -In class\n- -True/False and multiple-choice questions from content we discussed in class.", "prev_chunk_id": "ALecture5_SequenceModeling__0033", "next_chunk_id": "ALecture5_SequenceModeling__0035"}
{"lecture_id": "ALecture5_SequenceModeling", "source_pdf": "data/anlp/ALecture5_SequenceModeling.pdf", "chunk_id": "ALecture5_SequenceModeling__0035", "chunk_index": 35, "page_numbers": [61], "text": "- -Final Project Proposal team sign up deadline: Sept 23 rd\n- -Spreadsheet to sign up project teams: https://docs.google.com/spreadsheets/d/1EJ_5Xby0mRhHFmSRSmxl v6Gws4Qs5T8P5JUiKYKAZcA/edit?usp=sharing\n- -Reach out to me or TAs soon if you need help with project ideas and teaming.", "prev_chunk_id": "ALecture5_SequenceModeling__0034", "next_chunk_id": null}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0001"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0000", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0002"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0002", "chunk_index": 2, "page_numbers": [4], "text": "- -RNN models temporal sequence information\n- -can handle 'long -term dependencies' in theory\nIssue: RNN cannot handle such 'long -term dependencies' i n practice due to vanishing gradient ‚Üí apply the gating mechanism to directly encode the long-distance information\nSlide from previous lecture", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0001", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0003"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0003", "chunk_index": 3, "page_numbers": [5], "text": "- -What comes after the following context window?\n'purred' or 'barked' ?\n'‚Ä¶. the cat'\n'The dog that was chasing the cat'\n'purred'\n'barked'", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0002", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0004"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0004", "chunk_index": 4, "page_numbers": [6], "text": "- -What are the intent categories of the following utterances? ‚óè What are the intent categories of the following utterances?\n- -I would like to set an alarm ‚óè I would like to set an alarm\n- -I would like to send an email ‚óè I would like to send an email\n- -I need to book a table ‚óè I need to book a table\n- -I need to set an alarm ‚óè I need to set an alarm\n- ‚ûî Set_alarm\n- ‚ûî Send_email\n- ‚ûî Reserve_restaurant\n- ‚ûî Set_alarm", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0003", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0005"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0005", "chunk_index": 5, "page_numbers": [7], "text": "A: Have you had a chance to finish homework 1? A: Have you had a chance to finish homework 1?\nB: I am almost done. I heard they found a new case of coronavirus in the bay area today. B: I am almost done. I heard they found a new case of coronavirus in the bay area today.\nA: Yeah, I saw that too. We need to be careful to not spread it further. A: Yeah, I saw that too. We need to be careful to not spread it further.\nB: Yes, absolutely. I tried using Fasttext embeddings they helped a bit. B: Yes, absolutely. I tried using Fasttext embeddings they helped a bit.\nA: Oh, I am using Glove embeddings. A: Oh, I am using Glove embeddings.\nB: Yeah, I tried them as well. B: Yeah, I tried them as well.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0004", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0006"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0006", "chunk_index": 6, "page_numbers": [8], "text": "- -Dedicated mechanisms, for example, when:\n- -a hidden state should be updated,\n- -it should be reset, and\n- -information in hidden state should be used\n- -LSTMs: Gated memory call and input, output, forget gates\n- -GRUs: update and reset gates.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0005", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0007"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0007", "chunk_index": 7, "page_numbers": [9], "text": "- -LSTMs are explicitly designed to enable the long-term dependencies", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0006", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0008"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0008", "chunk_index": 8, "page_numbers": [12, 13, 14, 15, 16], "text": "runs straight down the chain with minor linear interactions ‚Üí easy for information to flow along it unchanged Gates are a way to optionally let information through ‚Üí composed of a sigmoid and a pointwise multiplication operation\nforget gate (a sigmoid layer): decides what information we're going to throw away from the cell state\n<!-- formula-not-decoded -->\n- 1: 'completely keep this'\n- 0: 'completely get rid of this'\nExample: The cell state might include the gender of the present subject, so that the correct pronouns can be used. When seeing a new subject, we want to forget the old subject's gender.\nVanilla RNN\nExample: We want to add the new subject's gender to the cell state for replacing the old one.\ncell state update: forgets the things we decided to forget earlier and adds the new candidate values, scaled by how much we decided to update each state value\n<!-- formula-not-decoded -->\n¬∑ ¬∑ Example: where we actually drop the information about the old subject's gender and add the new information\n- f t : decides what to forget i t : decides what to update\nA\nA\noutput gate (a sigmoid layer): decides what new information we're going to output\n<!-- formula-not-decoded -->\nExample: It might output whether the subject is singular or plural given the context", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0007", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0009"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0009", "chunk_index": 9, "page_numbers": [17], "text": "- -Can a network can learn to extract relevant information conveyed by the size of the time lags?\n- -LSTMs with peephole connections\n- -Are all connections in an LSTM cell necessary?\n- -LSTMs with coupled input and forget gates", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0008", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0010"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0010", "chunk_index": 10, "page_numbers": [19], "text": "Idea: instead of separately deciding what to forget and what we should add new information to, we make those decisions together\n<!-- formula-not-decoded -->\nWe only forget when we're going to input something in its place, and vice versa.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0009", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0011"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0011", "chunk_index": 11, "page_numbers": [20], "text": "Model Architectures and Contextual Embeddings\n- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0010", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0012"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0012", "chunk_index": 12, "page_numbers": [21], "text": "- -Reset gate : how much of the previous state we might still want to remember\n- -Update gate : how much of the new state is just a copy of the old state.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0011", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0013"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0013", "chunk_index": 13, "page_numbers": [22], "text": "- combine the forget and input gates into a single 'update gate';\n- merge the cell state and hidden state\n<!-- formula-not-decoded -->\nr t=0 : ignore previous memory and only store the new word information (conventional feed forward network) r t=1 : conventional RNN", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0012", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0014"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0014", "chunk_index": 14, "page_numbers": [25], "text": "- -Bi-directional RNN with LSTM/GRU cells\n- -Deep bi-directional RNN with LSTM/GRU cells", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0013", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0015"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0015", "chunk_index": 15, "page_numbers": [26], "text": "- -They aim to capture long dependencies\n- -They aim to solve vanishing gradient problem\n- -They present interpretable gating mechanisms.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0014", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0016"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0016", "chunk_index": 16, "page_numbers": [26], "text": "- -They are slow due to sequential processing\n- -They are memory-heavy\n- -Now, largely replaced by Transformers.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0015", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0017"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0017", "chunk_index": 17, "page_numbers": [27], "text": "- -GRUs are simpler i.e., fewer gates/parameters and hence faster training and inference.\n- -They often perform similarly, though LSTMs can be slightly better on tasks that require very long-term dependencies.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0016", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0018"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0018", "chunk_index": 18, "page_numbers": [28], "text": "- -Sequential processing in LSTMs, whereas attention in transformers allows parallelization (though quadratic complexity)\n- -LSTMs provide useful intuition for memory in sequences.\n- -Emerging approach: structured state space models\n- -Can capture long sequences efficiently (lengths of 10k -100k)\n- -E.g., Structured State Space Sequence Model (S4) (Gu et al., ICLR 2022) proposes a state space model (SSM) that:\n- -Can train in parallel like Transformers\n- -Can retain the recurrence structure (good for continuous-time or streaming tasks)", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0017", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0019"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0019", "chunk_index": 19, "page_numbers": [29], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0018", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0020"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0020", "chunk_index": 20, "page_numbers": [30], "text": "- -The goal of the learning algorithm f is to map the input domain X into the output domain Y\n<!-- formula-not-decoded -->\n- -Input domain: word, word sequence, audio signal, click logs\n- -Output domain: single label, sequence tags, tree structure, probability distribution\nNetwork design should leverage input and output domain properties", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0019", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0021"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0021", "chunk_index": 21, "page_numbers": [31], "text": "- -Tag a word at each timestamp\n- -Input: word sequence\n- -Output: corresponding POS tag sequence", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0020", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0022"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0022", "chunk_index": 22, "page_numbers": [32, 33], "text": "- -Ambiguities: The same word can be tagged with different POS tags depending on the context.\n- I can can the can.\n- -Tag a word at each timestamp\n- -Input: word sequence\n- -Output: corresponding POS tag sequence", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0021", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0023"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0023", "chunk_index": 23, "page_numbers": [34], "text": "- -Named entity: anything that can be referred to with a proper name: a person, a location, an organization\n- -Commonly extended to include things that aren't entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices.\nPeople, Tag = PER. People, Sample Categories = people, characters. People, Example sentences = Turing g is a giant of computer science.. Organization, Tag = ORG. Organization, Sample Categories = companies, sports teams. Organization, Example sentences = The IPCC  warned about the cyclone.. Location, Tag = LOC. Location, Sample Categories = regions, mountains, seas. Location, Example sentences = The Mt. Sanitas loop is in Sunshine Canyon.. Geo-Political Entity, Tag = GPE. Geo-Political Entity, Sample Categories = countries, states, provinces. Geo-Political Entity, Example sentences = Palo Alto is raising the fees for parking.. Facility, Tag = FAC. Facility, Sample Categories = bridges, buildings, airports. Facility, Example sentences = Consider the Golden Gate Bridge.. Vehicles, Tag = VEH. Vehicles, Sample Categories = planes, trains, automobiles. Vehicles, Example sentences = It was a classic Ford Falcon.\nFigure 18.1\nA list of generic named entity types with the kinds of entities they refer to.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0022", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0024"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0024", "chunk_index": 24, "page_numbers": [35, 36], "text": "The fourth Wells account moving to another agency is the packaged paperproducts division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\nPERSON LOCATION  ORGANIZATION\n- -Finding spans of text that constitute proper name, and then classifying the type of the entity.\n- -Used to be a difficult task because of ambiguity of boundaries and types.\nWashington Downing St. IRA LouisVuitton, Possible Categories = Person, Location, Political Entity, Organization, Vehicle Location, Organization Person, Organization, Monetary Instrument Person, Organization, Commercial Product\nCommon categorical ambiguities associated with various proper names.\n[PER Washington] was born into slavery on the farm of James Burroughs. [ORG Washington] went up 2 games to 1 in the four-game series. Blair arrived in [Loc Washington] for what may well be his last state visit. In June, IGPE Washington] passed a primary seatbelt law. The [vEH Washington] had proved to be a leaky ship, every passage I made...\nExamples of type ambiguities in the use of the name Washington.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0023", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0025"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0025", "chunk_index": 25, "page_numbers": [37], "text": "- -IOB or IO representation is used for entities in sequence labeling.\n- -IOB representation:\n- -Introduce a tag for the beginning (B) and inside (I) of each entity type, and one for tokens outside (O) any entity.\n- -The number of tags is thus 2n + 1 tags, where n is the number of entity types.\n- -IOB tagging can represent exactly the same information as the bracketed notation", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0024", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0026"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0026", "chunk_index": 26, "page_numbers": [38], "text": "- -[ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim\nWagner] said.\nWords American Airlines Ôºå a unit of AMR Corp. immediately matched the move, IOB Label = B-ORG I-ORG 0 0 0 0 B-ORG I-ORG 0 0 0 0 0. Words American Airlines Ôºå a unit of AMR Corp. immediately matched the move, 10 Label = I-ORG I-ORG 0 0 0 0 I-ORG I-ORG 0 0 0 0 0", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0025", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0027"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0027", "chunk_index": 27, "page_numbers": [40], "text": "- -Long-Short Term Memory (LSTM)\n- -Gated Recurrent Units (GRU)\n- -Example Sequence Classification Tasks\n- -Elmo and Contextual Embeddings", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0026", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0028"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0028", "chunk_index": 28, "page_numbers": [41], "text": "- -Assumption: one representation for a word\n- -Issues:", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0027", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0029"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0029", "chunk_index": 29, "page_numbers": [42], "text": "- -Idea: condition the neural network on all previous words and tie the weights at each time step\nP(next w='wreck') P(next w='a') P(next w='nice') P(next w='beach')\nThe LM produces context-specific word representations at each position", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0028", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0030"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0030", "chunk_index": 30, "page_numbers": [43], "text": "- -Idea: train (contextual) LM on big unannotated data and provide the context-specific embeddings for the target task ‚Üí semi-supervised learning\n- -Named entity recognition (NER) and chunking", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0029", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0031"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0031", "chunk_index": 31, "page_numbers": [44], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0030", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0032"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0032", "chunk_index": 32, "page_numbers": [45], "text": "- -Step 1 can use very large, unlabeled dataset.\n- -Steps 2 and 3 use the training data labeled for the task.\n- CoNLL 2003 NER English data (Sang and De Meulder, 2003)\n- 1B Word Benchmark dataset (Chelba et al, 2014)\n- using 800M for LM training.\nTraining set, Articles = 946. Training set, Sentences = 14,987. Training set, Tokens = 203,621. Training set,  = Training set. Training set, English data LOC = 7140. Training set, MISC = 3438. Training set, ORG = 6321. Training set, PER = 6600. Development set, Articles = 216. Development set, Sentences = 3,466. Development set, Tokens = 51,362. Development set,  = Development set. Development set, English data LOC = 1837. Development set, MISC = 922. Development set, ORG = 1341. Development set, PER = 1842. Test set, Articles = 231. Test set, Sentences = 3,684. Test set, Tokens = 46,435. Test set,  = Test set. Test set, English data LOC = 1668. Test set, MISC = 702. Test set, ORG = 1661. Test set, PER = 1617", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0031", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0033"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0033", "chunk_index": 33, "page_numbers": [46], "text": "- -Find and classify names in text\nThe decision by the independent MP Andrew Wilkie to withdraw his support for the minority Labor government sounded dramatic but it should not further threaten its stability. When, after the 2010 election, Wilkie, Rob Oakeshott, Tony Windsor and the Greens agreed to support Labor, they gave just two guarantees: confidence and supply.\nModel, 1 = Description. Model, 2 = CONLL 2003 F1. Klein+, 2003, 1 = MEMM softmax markov model. Klein+, 2003, 2 = 86.07. Florian+, 2003, 1 = Linear/softmax/TBL/HMM. Florian+, 2003, 2 = 88.76. Finkel+, 2005, 1 = Categorical feature CRF. Finkel+, 2005, 2 = 86.86. Ratinov and Roth, 2009, 1 = CRF+Wiki+Word cls. Ratinov and Roth, 2009, 2 = 90.80. Peters+, 2017, 1 = BLSTM + char CNN + CRF. Peters+, 2017, 2 = 90.87. Ma and Hovy, 2016, 1 = BLSTM + char CNN + CRF. Ma and Hovy, 2016, 2 = 91.21. TagLM (Peters+, 2017), 1 = LSTM BiLM in BLSTM Tagger. TagLM (Peters+, 2017), 2 = 91.93", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0032", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0034"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0034", "chunk_index": 34, "page_numbers": [47], "text": "- -Idea: contextualized word representations\n- -Learn word vectors using long contexts instead of a context window\n- -Learn a deep Bi-LM and use all its layers in prediction\nEach token is assigned a representation that is a function of entire input sentence.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0033", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0035"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0035", "chunk_index": 35, "page_numbers": [49], "text": "<!-- formula-not-decoded -->\n- -Character CNN for initial word embeddings\n- -2 BLSTM layers\n<!-- formula-not-decoded -->\nJoint maximization of log likelihood in backward and forward directions.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0034", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0036"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0036", "chunk_index": 36, "page_numbers": [50], "text": "- -Learn the task-specific linear combination of LM representations\nPeters et al., 'Deep Contextualized Word Representations', in NAACL-HLT , 2018. Elmo embeddings are deep: A task-specific embedding with combination weights learned from a downstream task.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0035", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0037"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0037", "chunk_index": 37, "page_numbers": [51], "text": "- -Get LM embedding for each word\n- -Freeze the LM weights and form ELMo enhanced embeddings\n```\n: concatenate ELMo into the intermediate layer : concatenate ELMo into the input layer task] task]\n```\nThe way of concatenation depends on the task", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0036", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0038"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0038", "chunk_index": 38, "page_numbers": [52], "text": "Backward Language Model", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0037", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0039"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0039", "chunk_index": 39, "page_numbers": [53], "text": "- 1- Concatenate hidden layers\n- 2- Multiply each vector by a weight based on the task\nX\nS2\nX\nS1\nX\nSo\n- 3- Sum the (now weighted) vectors\nForward Language Model\nELMoembeddingof\"stick\"forthistaskinthiscontext", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0038", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0040"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0040", "chunk_index": 40, "page_numbers": [54], "text": "Good transfer learning in NLP (similar to computer vision)", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0039", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0041"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0041", "chunk_index": 41, "page_numbers": [55], "text": "GloVe, Source = play. GloVe, NearestNeighbors = playing, game, games, played,players,plays,player, Play, football, multiplayer. biLM, Source = Chico Ruiz made a spec- tacular play on Alusik's grounder {...}. biLM, NearestNeighbors = Kieffer , the only junior in the group , was commended for his ability tohit in the clutch,aswell ashis all-round excellent play. biLM, Source = Olivia De Havilland signed to do a Broadway play for Garson { ...}. biLM, NearestNeighbors = . they were actors who had been handed fat roles in asuccessful 1playÔºåandhad talentenoughtofill theroles competently,with nice understatement.\nThe biLM is able to disambiguate both the PoS and word sense in the source sentence", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0040", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0042"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0042", "chunk_index": 42, "page_numbers": [57], "text": "- -The two LM layers have differentiated uses/meanings\n- -Lower layer is better for lower-level syntax , etc. Part-of-speech tagging, syntactic dependencies, NER\n- -Higher layer is better for higher-level semantics Sentiment, Semantic role labeling, question answering, SNLI", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0041", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0043"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0043", "chunk_index": 43, "page_numbers": [58], "text": "- -Contextualized embeddings learned from LMs provide informative cues\n- -ELMo -a general approach for learning high-quality deep contextdependent representations from biLMs\n- -Pre-trained ELMo: https://allennlp.org/elmo\n- -ELMo can process the character-level inputs", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0042", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0044"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0044", "chunk_index": 44, "page_numbers": [59], "text": "- -Model Architectures, Decoding, and Attention", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0043", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0045"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0045", "chunk_index": 45, "page_numbers": [59], "text": "- -Transformers", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0044", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0046"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0046", "chunk_index": 46, "page_numbers": [60], "text": "- -Not online: paper and pen\n- -Not open book and no cheat sheets\n- -Multiple choice and True/False questions only.\n- -From the content covered in class.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0045", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0047"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0047", "chunk_index": 47, "page_numbers": [61], "text": "Neural networks provide a guarantee for obtaining the global optimal solution.\n(A) TRUE (B) FALSE", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0046", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0048"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0048", "chunk_index": 48, "page_numbers": [62], "text": "Which of the following aims to help with the vanishing gradients problem of recurrent neural networks?\n- (A) Gradient clipping\n- (B) Gating mechanisms capturing longer term dependencies\n- (C) Weight decay\n- (D) A and B\n- (E) None of the above", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0047", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0049"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0049", "chunk_index": 49, "page_numbers": [63], "text": "1. Contextual Embeddings\n2. Pre-train embeddings using large unlabeled datasets. Two strategies for applying pre-trained language representations to downstream tasks:\n- Feature-based approach , uses task-specific architectures that include the pretrained representations as additional features (i.e., ELMo (Peters et al., 2018)).\n- Fine-tuning approach , introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters, using smaller datasets with task-specific labels (i.e., the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018) or BERT (Devlin et al, 2019)).", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0048", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0050"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0050", "chunk_index": 50, "page_numbers": [64], "text": "- Tag a word at each timestamp\n- Input: word sequence\n- Output: IOB-format slot tag and intent tag\nTemporal orders for input and output are the same", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0049", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0051"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0051", "chunk_index": 51, "page_numbers": [66], "text": "- Requires a domain ontology: early connection to backend\n- Contains core content (intent, a set of slots with fillers)", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0050", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0052"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0052", "chunk_index": 52, "page_numbers": [67], "text": "- Pipelined", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0051", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0053"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0053", "chunk_index": 53, "page_numbers": [68], "text": "- Given a collection of utterances ui with labels c i , D= {( u1 ,c 1 ) ,‚Ä¶, ( un ,c n )} where c i ‚àä C , train a model to estimate labels for new\nutterances uk . find me a cheap taiwanese restaurant in oakland\nMovies Restaurants Music Sports\n‚Ä¶\nDomain find_movie, buy_tickets find_restaurant, find_price, book_table find_lyrics, find_singer\n‚Ä¶\nIntent", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0052", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0054"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0054", "chunk_index": 54, "page_numbers": [69], "text": "- Unsupervised training of weights\n- Fine-tuning by back-propagation\n- Compared to MaxEnt, SVM, and boosting\nhttp://ieeexplore.ieee.org/abstract/document/5947649/", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0053", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0055"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0055", "chunk_index": 55, "page_numbers": [70], "text": "http://ieeexplore.ieee.org/abstract/document/6289054/; http://ieeexplore.ieee.org/abstract/document/6424224/\n- Deep convex networks (DCN)\n- Simple classifiers are stacked to learn complex functions\n- Feature selection of salient n-grams\n- Extension to kernel-DCN", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0054", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0056"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0056", "chunk_index": 56, "page_numbers": [71], "text": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/RNNLM_addressee.pdf\n- RNN and LSTMs for utterance classification\nutt class ho\n<S>\nutt class\nh\nW1\nuttclass\nW„ÄÇ\nhn\n</s>\nIntent decision after reading all words performs better\nW„ÄÇ\nPw\nWt", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0055", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0057"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0057", "chunk_index": 57, "page_numbers": [72, 73], "text": "- -IV (Lee & Dernoncourt, 2016)\n- ÔÇ® RNN and CNNs for dialogue act classification\nEntity Tag Slot Tag\nO, from = O. O, Bosto n = B-city. O, to = O. O, New = B-city. O, York = I-city. O, today = O. O, from = O. O, Bosto n = B-dept. O, to = O. O, New = B-arrival. O, York = I-. O, today = B-date", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0056", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0058"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0058", "chunk_index": 58, "page_numbers": [74], "text": "http://131.107.65.14/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf; http://dl.acm.org/citation.cfm?id=2876380\n- Variations:\n- a. RNNs with LSTM cells\n- b. Input, sliding window of n-grams\n- c. Bi-directional LSTMs\nùë¶0\n‚Ñé0\nùë§0\nùë¶1\n‚Ñé1\nùë¶2\n‚Ñé2\nùë§1\nùë§2\n(a) LSTM\nùë¶ùëõ\n‚Ñéùëõ\nùë§ùëõ\nùë¶0\n‚Ñé0\nùë¶1\n‚Ñé1\nùë¶2\n‚Ñé2\nùë§0\nùë§1\nùë§2\n(b) LSTM-LA\nùë¶ùëõ\n‚Ñéùëõ\nùë§ùëõ", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0057", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0059"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0059", "chunk_index": 59, "page_numbers": [75], "text": "III (Jaech et al., 2016; Tafforeau et al., 2016)\nhttps://arxiv.org/abs/1604.00117; http://www.sensei-conversation.eu/wp-content/uploads/2016/11/favre_is2016b.pdf", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0058", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0060"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0060", "chunk_index": 60, "page_numbers": [75], "text": "- Goal: exploit data from domains/tasks with a lot of data to improve ones with less data\n- Lower layers are shared across domains/tasks\n- Output layer is specific to task", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0059", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0061"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0061", "chunk_index": 61, "page_numbers": [76], "text": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_MultiJoint.pdf; https://arxiv.org/abs/1609.01454", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0060", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0062"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0062", "chunk_index": 62, "page_numbers": [77], "text": "- Domain and Intent Classification -> F-measure, similar to homework 1\n- Slot Tagging: Slot F-measure\n- Both the slot span and category should be estimated correctly.", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0061", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0063"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0063", "chunk_index": 63, "page_numbers": [78], "text": "who was the main actor in the exorcist -> O O O O O O B_movie I_movie how many woody allen movies starred diane keaton -> O O B_director I_director O O B_cast I_cast\n- Test data: who played in the movie captain America -> ?\n- Aim: Training vanilla RNNs, RNNs with LSTM cells and so on‚Ä¶", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0062", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0064"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0064", "chunk_index": 64, "page_numbers": [79], "text": "- 2,312 utterances in the training set (subset of HW1)\n- 981 utterances in the test set (same as HW1)\n- 13 slot types in the training set (27 possible IOB tags): Cast, char, country, director, genre, language, location, movie,  mpaa_rating, person, producer, release_year, subject", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0063", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0065"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0065", "chunk_index": 65, "page_numbers": [80, 81], "text": "- You'll need to change how labels are obtained from the output layer\n- Study the examples in the book and others available on the web\n- Start implementing simpler models (i.e., vanilla RNN)\n- Then move towards more complex ones (LSTM, etc.)\n- Dynamic RNN (unroll for each example) OR\n- Fixed size utterances (chop long utterances and pad short ones), for example, if fixed size is 8:\n- I want to book a table at Cascal for 8 people tomorrow -> I want to book a table at Cascal\n- ‚ñ† Will miss date=tomorrow, #_people =8\n- 2 people -> 2 people PAD PAD PAD PAD PAD PAD\n- ‚ñ† Many unnecessary computations\n- Best system from last year: multiple LSTM layers + a CRF layer on the top to ensure output label consistency!", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0064", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0066"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0066", "chunk_index": 66, "page_numbers": [82], "text": "- Midterm in classroom (today's content not included!)\n- Starts at 5:20pm sharp, until 6:50pm!", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0065", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0067"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0067", "chunk_index": 67, "page_numbers": [83], "text": "- Long Short Term Memory (LSTM)\n- Implementing LSTMs\n- Gated Recurrent Units (GRU)\n- Sequence Classification Tasks and homework 3\n- Reading:\n- Starting Chapter 9 of Dive Into Deep Learning\n- Chapter 7 of NLP with PyTorch", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0066", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0068"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0068", "chunk_index": 68, "page_numbers": [84], "text": "- Code by Varuna Jayasiri, using numpy: https://blog.varunajayasiri.com/numpy_lstm.html\n- Inspired by Karpathy's character LM", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0067", "next_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0069"}
{"lecture_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings", "source_pdf": "data/anlp/ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings.pdf", "chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0069", "chunk_index": 69, "page_numbers": [85, 86], "text": "```\ndef forward(x, h_prev, C_prev, p = parameters): assert x.shape == (x_size, 1) assert h_prev.shape == (H_size, 1) assert C_prev.shape == (H_size, l) Z = np.row_stack((h_prev, x)) f ‰∫å sigmoid(np.dot(p.W_f.v, z) + p.b_f.v) i sigmoid(np.dot(p.W_i.v, z) + p.b_i.v) C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v) C f * C_prev + i * C_bar o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v) h: = o * tanh(C) V = np.dot(p.W_v.v, h) + p.b_v.v Y =1 np.exp(v) / np.sum(np.exp(v)) #softmax return z, f, i, C_bar, C, O, h, v, Y\n```\n```\nft =o(Wf¬∑[ht-1,t] + bf) it =o(W¬∑[ht-1,ct] + bi) Ct =tanh(Wc¬∑[ht-1,ct] + bc) Ct=ft*Ct-1+it*Ct Ot =o(W„ÄÇ[ht-1,‚àût] + b„ÄÇ) ht = Ot *tanh(Ct)\n```\n```\nIn [9]: def backward(target, dh_next, dC_next, C_prev, ZÔºå f, iÔºå C_bar, C, o, h, vÔºå Y p = parameters): assert z.shape == (x_size + H_size, 1) assert v.shape == (X_size, l) assert Y.shape == (x_sizeÔºå l) for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]: assert param.shape == (H_size, l) dv = np.copy(y) dv[target] -= 1 p.W_v.d += np.dot(dv, h.T) p.b_v.d += dv dh = np.dot(p.W_v.v.T, dv) dh n += dh_next do = ( dh * tanh(C) do =Ôºà dsigmoid(oÔºâ * do p.W_o.d += np.dot(do, z.T) p.b_o.d += do\n```", "prev_chunk_id": "ALecture6_LSTM_ModelArchitectures_ContextualEmbeddings__0068", "next_chunk_id": null}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0001"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0000", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0002"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Ch 11 of the Dive into Deep Learning book", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0001", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0003"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Inputs can have variable length!\n- -V = {the, a, chased, cat, dog, is, cute}\n- -the cat is cute\n- -\n- the cat chased a dog [1 1 1 1 1 0 0]\n- -", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0002", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0004"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0004", "chunk_index": 4, "page_numbers": [4], "text": "[1 0 0 1 0 1 1]\n- the dog chased a cat [1 1 1 1 1 0 0]\nBag of words approach represents variable length inputs with vectors of size |V|. Works well for tasks where word ordering is not important.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0003", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0005"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0005", "chunk_index": 5, "page_numbers": [5, 7], "text": "- -CNNs with k-max pooling:\nk-max pooling maps all inputs to a fixed number of vectors.\n- -What if the outputs are a list of tokens of variable length?\n- -Examples:\nTask, 1 = Input. Task, 2 = Output. nasƒ±lsƒ±n, 1 = Nasƒ±lsƒ±n ? (in Turkish). nasƒ±lsƒ±n, 2 = How are you?. Summarization, 1 = 1 or more documents. Summarization, 2 = Summary. Dialogue Systems, 1 = Previous conversation A: Howare you? B: Thanks, I am doing ok. And you?. Dialogue Systems, 2 = Next response A: Thanks, I am fine. Have you seen the Parasite movie? Wanna go see it together tomorrow?", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0004", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0006"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0006", "chunk_index": 6, "page_numbers": [8], "text": "- -Aiming to converting messages typed on a phone keyboard to emojis.\n- -Alignment between the input and the output:\nOther examples: machine translation, question answering, dialogue response generation, ‚Ä¶", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0005", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0007"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0007", "chunk_index": 7, "page_numbers": [9], "text": "- -Composition of two models:\n- -Encoder aims to capture important information for the task\n- -…∏ is usually a vector\n- -Decoder aims to decode the encoded input to target output\n- -Trained jointly", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0006", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0008"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0008", "chunk_index": 8, "page_numbers": [10], "text": "Image from Wikipedia: https://en.wikipedia.org/wiki/Transfer-based_machine_translation\nEncoderdecoder models aim to learn representations from data: 'Representation learning'", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0007", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0009"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0009", "chunk_index": 9, "page_numbers": [10], "text": "- Language independent meaning representation\n- makes explicit the distinctions necessary for successful translation", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0008", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0010"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0010", "chunk_index": 10, "page_numbers": [11], "text": "- -For NL sequences, the encoder and decoder could both be RNNs.\n- -Sequence-to-sequence (S2S) models\n- -(Sutskever et al, 2014)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0009", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0011"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0011", "chunk_index": 11, "page_numbers": [12], "text": "- -Input: ùë• 1 ,‚Ä¶, ùë•ùëá where ùë•ùë° is the ùë° th word\n- -The transformation of the RNN's hidden states, denoted by ùëì : ùê°ùë° = ùëì ( ùê±ùë° , ùê°ùë° -1 )\n- -The encoder captures information and encodes it into the context vector …∏ with a function ùëû : …∏ = ùëû ( ùê° 1 ,‚Ä¶, ùê°ùëá )\nùëû ( ùê° 1 ,‚Ä¶, ùê°ùëá )= ùê°ùëá", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0010", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0012"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0012", "chunk_index": 12, "page_numbers": [13], "text": "- -The context vector, …∏ , encodes the information from the complete input sequence ùë• 1 ,‚Ä¶, ùë•ùëá .\n<!-- formula-not-decoded -->\n- -If given outputs in the training set are ùë¶ 1 ,‚Ä¶, ùë¶ùëá ‚Ä≤ , at each timestep ùë° ‚Ä≤ ,\n- -Another RNN as decoder, with hidden states s t\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0011", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0013"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0013", "chunk_index": 13, "page_numbers": [14, 15], "text": "- -Decoder has a dense layer after the RNN layers, where the hidden size is the vocabulary size.\n- -The dense layer predicts the confidence score for each word.\n- -We can use softmax on the output scores\n- -And use cross entropy as the loss.\n- -Simplest method for predictions, greedy search.\n<!-- formula-not-decoded -->\nGenerate until the '< eos >' symbol is detected, or the output sequence has reached the max length ùëá .", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0012", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0014"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0014", "chunk_index": 14, "page_numbers": [16], "text": "- -The conditional probability for generating an output sequence ‚àè ùëá ‚Ä≤ ùë° ‚Ä≤ =1 ùëÉ ( ùë¶ùë° ‚Ä≤ ‚à£ ùë¶ 1 ,‚Ä¶, ùë¶ùë° ‚Ä≤ -1 , …∏ )\nP('ABC< eos >'|‚Ä¶) = 0.048\nP('ACB< eos >'|‚Ä¶) = 0.054\nNo guarantee that optimal sequence will be obtained with greedy search!", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0013", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0015"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0015", "chunk_index": 15, "page_numbers": [17], "text": "- -Examination of all possible sequences.\n- -Computational overhead:  O(| Y | ùëá ‚Ä≤ )\n- -For example, if | Y |=10000 and ùëá ‚Ä≤ =10, we will need to evaluate 10000 10  = 10 40 sequences\nExhaustive search is too expensive even with small vocabulary sizes!", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0014", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0016"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0016", "chunk_index": 16, "page_numbers": [18], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0015", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0017"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0017", "chunk_index": 17, "page_numbers": [19], "text": "- -Beam-size = ùëò\n- -At time step 1, select top ùëò candidates\n- -For each subsequent timestep:\n- -Determine the ùëò | Y | possible output sequences based on the ùëò candidate output sequences from the previous timestep\n- -select the top ùëò output sequences with the highest conditional probability amongst those.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0016", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0018"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0018", "chunk_index": 18, "page_numbers": [20, 23], "text": "k=2\nTimestep 1 Candidates\nk\n- -The computational overhead of beam search is between greedy search and exhaustive search.\n- -Greedy search can be treated as a beam search with a beam size of 1.\n- -Beam search strikes a balance between computational overhead and search quality using a flexible beam size of ùëò .", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0017", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0019"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0019", "chunk_index": 19, "page_numbers": [24], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0018", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0020"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0020", "chunk_index": 20, "page_numbers": [25], "text": "- -Aim: Learn to generate a response from large conversational datasets.\n- -Inference time: Given the conversation context, generate a natural and appropriate response.\n- -Modeled with E2E architectures.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0019", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0021"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0021", "chunk_index": 21, "page_numbers": [26], "text": "Modeling conversations as request-response pairs.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0020", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0022"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0022", "chunk_index": 22, "page_numbers": [27, 28], "text": "A: Yeah, me too. My favorite movie is Seven.\nOh, yes, that was a great movie!\nI don't like scary movies. I enjoy documentaries about historical events.\nI love to watch horror movies.\nA: Yeah, me too. My favorite movie is Seven.\nOh, yes, that was a great movie!\nI don't like scary movies. I enjoy documentaries about historical events.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0021", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0023"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0023", "chunk_index": 23, "page_numbers": [29, 30, 31], "text": "The complete conversation context can be treated as a long sequence.\nU: Hi, how have you been?\nS: I am doing great, and you?\nU: Good, thanks, I am looking for an interesting movie to see.\nS: Deepwater Horizon is showing at Angelika.\nU: What else is there?\nS: Not sure. Did you know m ore than 200 million gallons of oil spilled into the Gulf of Mexico during that time?\nU: Oh, wow, I didn't know that, I cannot even imagine how much that would be. What are the showtimes?\nS: You can see it today at 3:40pm and 5:45pm.\nU: 'Let's do 3:40'\nThe complete conversation context can be treated as a long sequence.\nIt could also be limited length window, defined by # of words / characters.\nU: Hi, how have you been?\nS: I am doing great, and you?\nU: Good, thanks, I am looking for an interesting movie to see.\nS: Deepwater Horizon is showing at Angelika.\nU: What else is there?\nS: Not sure. Did you know m ore than 200 million gallons of oil spilled into the Gulf of Mexico during that time?\nU: Oh, wow, I didn't know that, I cannot even imagine how much that would be. What are the showtimes?\nS: You can see it today at 3:40pm and 5:45pm.\nU: 'Let's do 3:40'\nU: '\ni d like to purchase tickets to see deepwater\n- S: 'for which theatre'\n- U: ' angelika '\n- S: 'you want them for angelika theatre?'\n- U: 'yes angelika '\nS: 'how many tickets would you like ?'\n- U: '3 tickets for saturday '\n- S: 'What time would you like ?'\n- U: 'Any time on saturday is fine'\nS: 'okay , there is 4:10 pm , 5:40 pm and 9:20 pm'\nU: 'Let's do 5:40'\nhorizon'", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0022", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0024"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0024", "chunk_index": 24, "page_numbers": [33], "text": "- The hierarchical RNN encodes the complete conversation context.\n- The context vector is the input for decoding.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0023", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0025"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0025", "chunk_index": 25, "page_numbers": [34], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0024", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0026"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0026", "chunk_index": 26, "page_numbers": [35, 37], "text": "- -The final embedding of the input should capture all the information!\n- -Encoding of long sequences could be problematic.\n- -Even with LSTM/GRU models\n- -The final embedding of the input should capture all the information!\n- -Encoding of long sequences could be problematic.\n- -Even with LSTM/GRU models\n- -Hierarchical models can also be helpful.\n- -Some tasks (i.e., machine translation) require 'attending' to specific words.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0025", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0027"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0027", "chunk_index": 27, "page_numbers": [38], "text": "- -Maximization based decoding methods, such as beam search, result in generic and bland utterances:\n- 'Absolutely!' 'That is interesting!' 'Ok'\n- -Often times with repetition 'I I I am fine. I am fine.'\n- -How can we improve the diversity of the responses?", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0026", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0028"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0028", "chunk_index": 28, "page_numbers": [39], "text": "- -Encoder: hierarchical recurrent neural networks to represent conversation context.\n- -A latent variable at the decoder, allows to model hierarchicallystructured sequences in a two-step generation process:\n- -Injecting stochastic noise during decoding for response generation.\n- -sampling the latent variable\n- -generating the output sequence -while maintaining long-term context.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0027", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0029"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0029", "chunk_index": 29, "page_numbers": [40], "text": "- -To create diversity, sample, rather than picking the most probable token.\n- -Topùëò sampling:\n- -Keep only the top ùëò tokens, re-normalize, then sample.\n- -Prevents picking extremely unlikely tokens", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0028", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0030"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0030", "chunk_index": 30, "page_numbers": [41], "text": "- ...to provide an overview of the current state-of-the-art in thefield of computer vision and machine learning, and to provide an overview of the current state-of-the-art in the field of computer vision and machine learning, and to provide an overviewofthecurrent state-of-the-art in the field of computer vision and machine learning, and to provide an overview of the current state-of-the-art in the field of computer vision and machine learning, and...\nHuman", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0029", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0031"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0031", "chunk_index": 31, "page_numbers": [41], "text": "- ...which grant increased life span and threeyearswarranty.The Antec HCG series consists of five models with capacities spanning from 400w to 900w. Here we should note that we have already tested the HCG-620 in a previous review and were quite satisfied With its performance. In today's reviewwewillrigorouslytestthe Antec HCG-520, which as its model number implies, has 520W capacity and contrary to Antec's strong beliefs in multi-rail PSUs is equipped...\n- -Given the same context, humans generate words of varying probability (according to model), whereas beam search selects the top words.\n- -Nucleus Sampling\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0030", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0032"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0032", "chunk_index": 32, "page_numbers": [42, 43], "text": "could\n‚ñ†\nIn Nucleus Sampling , the number of candidates considered rises and falls dynamically. In Topùëò Sampling , the number of candidates is fixed to ùëò , which could be sub-optimal across varying contexts. If ùëò is small, generated text will be bland, if ùëò is large, then the list will include inappropriate candidates.\n- -Another common approach, shape the probability distribution through temperature.\n- -Given the logits, u1 :| V | and temperature t , the softmax is reestimated as:\n<!-- formula-not-decoded -->\nt < 1 : Sharper distribution ‚Üí more deterministic\nt > 1 : Flatter distribution ‚Üí more random\nSetting temperature, t, in [0,1) skews the probability distribution towards high probability events. Shown to improve quality at the cost of reducing diversity.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0031", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0033"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0033", "chunk_index": 33, "page_numbers": [44], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0032", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0034"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0034", "chunk_index": 34, "page_numbers": [45, 46], "text": "When the input is a very long sequence or an image\nPay attention on part of the input object each time\nWhen the input is a very long sequence or an image time\nPay attention on part of the input object each\nWhen access to longer term memory is needed\nWe can retrieve contents and use attention mechanisms https://en.wikipedia.org/wiki/Memory", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0033", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0035"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0035", "chunk_index": 35, "page_numbers": [47], "text": "Attention on Sensory Info\nBahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0034", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0036"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0036", "chunk_index": 36, "page_numbers": [48], "text": "- -Sequence-to-sequence learning: both input and output are both sequences with different lengths .\n- -E.g. A B C D ‚Üí deep learning", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0035", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0037"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0037", "chunk_index": 37, "page_numbers": [54], "text": "- -Input sentence: c1Ôºå X ‰∏â\n- -Encoder RNN:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -In S2S models (i.e., Sutskever et al 2014)\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0036", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0038"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0038", "chunk_index": 38, "page_numbers": [55], "text": "- -Target Translation:\n- -Decoder RNN:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0037", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0039"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0039", "chunk_index": 39, "page_numbers": [56], "text": "- -Encoder is similar.\n- -Decoder RNN:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -The context vector c i depends on hj\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0038", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0040"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0040", "chunk_index": 40, "page_numbers": [57], "text": "- -Attention probability matrix can be useful for visualization.\n- -Pixels show the weight Œ± ij of the annotation of the j th source word for the i th target word\n- -Plot is grayscale (0: black, 1: white)\nTranslation", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0039", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0041"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0041", "chunk_index": 41, "page_numbers": [59], "text": "- -Encoder-Decoder Architecture\n- -Beam Search\n- -Case Study: Response generation in E2E Dialogue Systems\n- -Limitations of S2S models & Solutions\n- -Attention\n- -Attention to Implement Copy Mechanisms (and Pointer Generator Networks)", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0040", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0042"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0042", "chunk_index": 42, "page_numbers": [60], "text": "- -The output will always be limited to the vocabulary of the training data.\n- -However, knowledge and conversation context can include new words.\n- -How to generate new words in conversations?", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0041", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0043"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0043", "chunk_index": 43, "page_numbers": [63], "text": "The overall distribution is obtained by interpolating the LLM output probabilities with input tokens, weighted according to attention.\nOut-of-vocabulary (OOV) words can be decoded if they were seen in knowledge.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0042", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0044"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0044", "chunk_index": 44, "page_numbers": [64], "text": "- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0043", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0045"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0045", "chunk_index": 45, "page_numbers": [65], "text": "- Input: image\n- Output: word sequence", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0044", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0046"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0046", "chunk_index": 46, "page_numbers": [69], "text": "littl\ngroun of neo nie sittinc ona hoat\nA ston sian road With a\n‚ñ≥ airaffe standing ina forest With", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0045", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0047"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0047", "chunk_index": 47, "page_numbers": [70], "text": "Alarge white bird standing Lina forest\npersonisstanding ona abeach\nA woman holding cIock in Lher hand\nA woman IS ssitting atatable\nLhat and\nmanistalking on his cellphone", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0046", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0048"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0048", "chunk_index": 48, "page_numbers": [71, 72], "text": "Ref: A man and a woman ride a motorcycle A man and a woman are talking on the road\nRef:\nA woman is frying food\nSomeone is frying a fish ina pot", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0047", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0049"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0049", "chunk_index": 49, "page_numbers": [73], "text": "- Attention\n- Application of Attention in Tagging\n- Analysis of attention\n‚óè\nAttention and Memory Networks\n- Attention to Implement Copy Mechanisms (and Pointer Generator Networks)\n- Readings:\n- Continuing Chapter 8 of NLP with PyTorch\n- Chapter 10 of Dive Into Deep Learning (only 10.1 and 10.2)\n- Bahdanu et al., Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0048", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0050"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0050", "chunk_index": 50, "page_numbers": [74], "text": "- Tag a word at each timestamp\n- Input: word sequence\n- Output: IOB-format slot tag and intent tag\nTemporal orders for input and output are the same", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0049", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0051"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0051", "chunk_index": 51, "page_numbers": [75], "text": "http://131.107.65.14/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf; http://dl.acm.org/citation.cfm?id=2876380\n- Variations:\n- a. RNNs with LSTM cells\n- b. Input, sliding window of n-grams\n- c. Bi-directional LSTMs\nùë¶0\n‚Ñé0\nùë§0\nùë¶1\n‚Ñé1\nùë¶2\n‚Ñé2\nùë§1\nùë§2\n(a) LSTM\nùë¶ùëõ\n‚Ñéùëõ\nùë§ùëõ\nùë¶0\n‚Ñé0\nùë¶1\n‚Ñé1\nùë¶2\n‚Ñé2\nùë§0\nùë§1\nùë§2\n(b) LSTM-LA\nùë¶ùëõ\n‚Ñéùëõ\nùë§ùëõ", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0050", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0052"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0052", "chunk_index": 52, "page_numbers": [76], "text": "II (Kurata et al., 2016; Simonnet et al., 2015)\nhttp://www.aclweb.org/anthology/D16-1223; https://hal.archives-ouvertes.fr/hal-01433202\n- Encoder-decoder networks\n- Leverages sentence level information\n- Attention-based encoderdecoder\n- Use of attention (as in MT) in the encoder-decoder network\n- Attention is estimated using a feed-forward network with input: ht and s t at time t", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0051", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0053"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0053", "chunk_index": 53, "page_numbers": [77], "text": "- Input sentence is encoded with a biLSTM.\n- Decoder estimates tags based on corresponding encoder hidden states as well as attention.\n- Comparable performance is achieved due to the attention mechanism.\n- The encoder-decoder approach enables multi-task learning with other tasks.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0052", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0054"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0054", "chunk_index": 54, "page_numbers": [78], "text": "- Attention\n- Application of Attention in Tagging\n- Analysis of attention\n- Attention and Memory Networks\n- Attention to Implement Copy Mechanisms (and Pointer Generator Networks)\n- Readings:\n- Continuing Chapter 8 of NLP with PyTorch\n- Chapter 10 of Dive Into Deep Learning (only 10.1 and 10.2)\n- Bahdanu et al., Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0053", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0055"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0055", "chunk_index": 55, "page_numbers": [79], "text": "- S tanford Qu estion A nswering D ataset (SQuAD)\nThefirstrecordedtravelsbyEuropeanstoChina andbackdatefromthistime. ThemostfamoustraveleroftheperiodwastheVenetianMarcoPolo,whose accountof histripto\"Cambaluc,thecapital of the GreatKhan,and of life there astoundedthepeopleofEurope.Theaccountofhistravels,llmilioneÔºàor,The Million,knowninEnglishastheTravelsofMarcoPolo),appearedabouttheyear 1299.Some argueoverthe accuracyofMarcoPolo's accountsduetothelackof mentioningtheGreatWall ofChina,teahouses,whichwould havebeena prominentsightsinceEuropeanshadyettoadoptateaculture,aswell the practice of footbindingbythewomen incapital of theGreatKhan.Some suggestthatMarcoPoloacquiredmuchofhisknowledge throughcontactwith Persian traders sincemanyof theplaceshenamedwereinPersian.\nHow did some suspect that Polo learned about China instead of by actually visiting it?\nAnswer: through contact with Persian traders\nWe can use attention to focus on parts of long documents!", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0054", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0056"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0056", "chunk_index": 56, "page_numbers": [83], "text": "- Multi-hop performance analysis on babI task:\nDaniel went to the bathroom.,  = . Daniel went to the bathroom., Hop 1 = 0.00. Daniel went to the bathroom., Hop 2 = 0.00. Daniel went to the bathroom., Hop 3 = 0.03. Mary travelled to the hallway.,  = . Mary travelled to the hallway., Hop 1 = 0.00. Mary travelled to the hallway., Hop 2 = 0.00. Mary travelled to the hallway., Hop 3 = 0.00. John went to the bedroom.,  = . John went to the bedroom., Hop 1 = 0.37. John went to the bedroom., Hop 2 = 0.02. John went to the bedroom., Hop 3 = 0.00. John travelled to the bathroom.,  = yes. John travelled to the bathroom., Hop 1 = 0.60. John travelled to the bathroom., Hop 2 = 0.98. John travelled to the bathroom., Hop 3 = 0.96. Mary went to the office.,  = . Mary went to the office., Hop 1 = 0.01. Mary went to the office., Hop 2 = 0.00. Mary went to the office., Hop 3 = 0.00. Where is John? Answer: bathroom,  = Prediction: bathroom. Where is John? Answer: bathroom, Hop 1 = Prediction: bathroom. Where is John? Answer: bathroom, Hop 2 = Prediction: bathroom. Where is John? Answer: bathroom, Hop 3 = Prediction: bathroom. Story (16: basic induction),  = . Story (16: basic induction), Hop 1 = Hop 1. Story (16: basic induction), Hop 2 = Hop 2. Story (16: basic induction), Hop 3 = Hop 3. Brian is a frog.,  = yes. Brian is a frog., Hop 1 = 0.00. Brian is a frog., Hop 2 = 0.98. Brian is a frog., Hop 3 = 0.00. Lily is gray.,  = . Lily is gray., Hop 1 = 0.07. Lily is gray., Hop 2 = 0.00. Lily is gray., Hop 3 = 0.00. Brian is yellow.,  = yes. Brian is yellow., Hop 1 = 0.07. Brian is yellow., Hop 2 = 0.00. Brian is yellow., Hop 3 = 1.00. Julius is green.,  = . Julius is green., Hop 1 = 0.06. Julius is green., Hop 2 = 0.00. Julius is green., Hop 3 = 0.00. Greg is a frog.,  = yes. Greg is a frog., Hop 1 = 0.76. Greg is a frog., Hop 2 = 0.02. Greg is a frog., Hop 3 = 0.00. What color is Greg? Answer: yellow,  = Prediction: yellow. What color is Greg? Answer: yellow, Hop 1 = Prediction: yellow. What color is Greg? Answer: yellow, Hop 2 = Prediction: yellow. What color is Greg? Answer: yellow, Hop 3 = Prediction: yellow\nhttps://www.facebook.com/Engineering/videos/10153098860532200/", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0055", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0057"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0057", "chunk_index": 57, "page_numbers": [84], "text": "- Natural language understanding in conversational systems mainly consider the last user utterance.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0056", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0058"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0058", "chunk_index": 58, "page_numbers": [85], "text": "- ‚óª User utterances are highly ambiguous in isolation", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0057", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0059"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0059", "chunk_index": 59, "page_numbers": [86], "text": "- ‚óª Seq2Seq model (Hori et al, 2015)\n- ‚¨ú Words are input one at a time, tags are output at the end of each\nutterance\n- ‚óª Extension: LSTM with speaker role dependent layers", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0058", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0060"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0060", "chunk_index": 60, "page_numbers": [87], "text": "History utterances, u 1,...,t-1 , s 1,...,t\n- Encoding current and previous conversation turns\n- Combining history embeddings\n- Using them during CLU for the current conversation turn", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0059", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0061"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0061", "chunk_index": 61, "page_numbers": [88], "text": "U: ' i d like to purchase tickets to see deepwater horizon'\n- S: 'for which theatre'\n- U: ' angelika '\n- S: 'you want them for angelika theatre?'\n- U: 'yes angelika '\n- S: 'how many tickets would you like ?'\n- U: '3 tickets for saturday '\n- S: 'What time would you like ?'\n- U: 'Any time on saturday is fine'\n- S: 'okay , there is 4:10 pm , 5:40 pm and 9:20 pm'\n- U: 'Let's do 5:40'", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0060", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0062"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0062", "chunk_index": 62, "page_numbers": [89], "text": "Chen et al., End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding, Interspeech 2016.\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_ContextualSLU.pdf\nIdea: additionally incorporating contextual knowledge during slot tagging", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0061", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0063"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0063", "chunk_index": 63, "page_numbers": [90], "text": "```\nU: ' i d like to purchase tickets to see deepwater horizon' S: 'for which theatre' U: ' angelika ' S: 'you want them for angelika theatre?' U: 'yes angelika ' S: 'how many tickets would you like ?' U: '3 tickets for saturday ' S: 'What time would you like ?' U: 'Any time on saturday is fine' S: 'okay , there is 4:10 pm , 5:40 pm and 9:20 pm' U: 'Let's do 5:40' 0.69 0.13 0.16\n```", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0062", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0064"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0064", "chunk_index": 64, "page_numbers": [91, 92], "text": "Bapna et al, Sequential Dialogue Context Modeling for Spoken Language Understanding, SigDial, 2017.\n- E2E MemNN for Contextual CLU relies on cosine similarity for attention.\n- Can we use a machine learning model instead?\nBapna et al, Sequential Dialogue Context Modeling for Spoken Language Understanding, SigDial, 2017.\n- E2E MemNN for Contextual CLU relies on cosine similarity for attention.\n- Can we use a machine learning model instead?", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0063", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0065"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0065", "chunk_index": 65, "page_numbers": [93], "text": "- Attention\n- Application of Attention in Tagging\n- Analysis of attention\n- Attention and Memory Networks\n- Attention to Implement Copy Mechanisms (and Pointer Generator Networks)\n- Readings:\n- Continuing Chapter 8 of NLP with PyTorch\n- Chapter 10 of Dive Into Deep Learning (only 10.1 and 10.2)\n- Bahdanu et al., Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0064", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0066"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0066", "chunk_index": 66, "page_numbers": [94], "text": "- Sequence-to-sequence models typically generate generic responses.\n- Information previously unseen in conversations may be available elsewhere\n- Unstructured knowledge: news articles, Wikipedia, reviews, etc.\n- Structured knowledge: knowledge graphs\nUser: Can you find me an Italian restaurant in Santa Clara?\nSystem: Il Fornaio is an Italian restaurant in Santa Clara. Their gnocchi gorgonzola and spaghetti carbonara are quite popular according to reviews.\n'Our favorites: caesar salad, gamberini in padella, risotto, spaghetti carbonara, and the arrabiata pasta.' in 11 reviews\n'That all being said, the Gnocchi Gorgonzola was divine, and much lighter than I expected.' in 6 reviews", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0065", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0067"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0067", "chunk_index": 67, "page_numbers": [95], "text": "- A simple approach is to have a network for knowledge selection .\n- Given a conversation context, c t , and a set of knowledge sentences k1 ,‚Ä¶, k|K|\n- k t = Argmax j=1,‚Ä¶,|K| f(c t , k j )\n- f() could be similarity measure or a network that could be trained.\n- Then, feed encodings of both c t and kt as input to the decoder.\nGopalakrishnan et al., Interspeech 2019. Topical-Chat: Towards Knowledge-Grounded Open-Domain Convers\n- Knowledge selection could be trained jointly with response generation.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0066", "next_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0068"}
{"lecture_id": "ALecture7_ModelArchitectures_Decoding_Attention", "source_pdf": "data/anlp/ALecture7_ModelArchitectures_Decoding_Attention.pdf", "chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0068", "chunk_index": 68, "page_numbers": [96], "text": "- All knowledge snippets are encoded in a representation using conversation context.", "prev_chunk_id": "ALecture7_ModelArchitectures_Decoding_Attention__0067", "next_chunk_id": null}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture8_Attention_and_Transformers__0001"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0000", "next_chunk_id": "ALecture8_Attention_and_Transformers__0002"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Vaswani et al, NIPS 2017. Attention is all you need.\n- -Continuing Ch 11 of the Dive into Deep Learning book\n- -Blogs:\n- -Jay Alammar, The illustrated transformer\n- -PyTorch explanation by Sasha Rush: http://nlp.seas.harvard.edu/2018/04/03/attention.html", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0001", "next_chunk_id": "ALecture8_Attention_and_Transformers__0003"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Easy to parallelize at each layer.\n- -Exploit local dependencies\n- -Long-distance dependencies require many layers", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0002", "next_chunk_id": "ALecture8_Attention_and_Transformers__0004"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0004", "chunk_index": 4, "page_numbers": [5], "text": "- Allow for modeling of long- and short-range dependencies (though not explicitly)\n- Sequential computation is slow, and parallelization is not straightforward.\n- Context window is fixed size and may not be able to store all the information => attention", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0003", "next_chunk_id": "ALecture8_Attention_and_Transformers__0005"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0005", "chunk_index": 5, "page_numbers": [6], "text": "- -Encoder-decoder approach has been successful in NMT and other sequence-to-sequence problems.\n- -RNNs' attention mechanism is useful to handle long dependencies\n- -Attention allows us to access any state\nCan we use attention to replace recurrent architectures?", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0004", "next_chunk_id": "ALecture8_Attention_and_Transformers__0006"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0005", "next_chunk_id": "ALecture8_Attention_and_Transformers__0007"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0007", "chunk_index": 7, "page_numbers": [8], "text": "- -Self-Attention Mechanism : Lets the model directly relate each element of a sequence to every other element, regardless of distance.\n- -Parallelization : Unlike RNNs, all tokens can be processed at once.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0006", "next_chunk_id": "ALecture8_Attention_and_Transformers__0008"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0008", "chunk_index": 8, "page_numbers": [10], "text": "- -Stack of encoder and decoder layers.\n- -Each have the same architecture.\n- -They do not share weights.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0007", "next_chunk_id": "ALecture8_Attention_and_Transformers__0009"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0009", "chunk_index": 9, "page_numbers": [12], "text": "1\nThe number of layers to stack is a hyperparameter.\nThe original MT paper had 6 layers.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0008", "next_chunk_id": "ALecture8_Attention_and_Transformers__0010"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0010", "chunk_index": 10, "page_numbers": [13], "text": "Transformers\n- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0009", "next_chunk_id": "ALecture8_Attention_and_Transformers__0011"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0011", "chunk_index": 11, "page_numbers": [14], "text": "- -Introduced by the 'Attention is all you need' paper.\n- -Instead of attending to input while decoding, self attention attends to each token in the input while encoding them.\n- -The aim is to capture dependencies between input tokens.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0010", "next_chunk_id": "ALecture8_Attention_and_Transformers__0012"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0012", "chunk_index": 12, "page_numbers": [15, 16], "text": "- -Input: a query ùëû and a set of key-value ( ùëò -ùë£ )\nInner product of query and corresponding key\n<!-- formula-not-decoded -->\n- -Query ùëû is a ùëëùëò -dim vector\n- -Key ùëò is a ùëëùëò -dim vector\n- -Value ùë£ is a ùëëùë£ -dim vector\n- -Output: weighted sum of values\nW Q , W K , and W V are learned during training.\n- -Linear projections to obtain q, k, v for each token:\nqi = W Q x i ki = W K xi\nvi = W V xi\n- -They typically map from the embedding dimension to a smaller one.\n- -The separation aims to enable the model to learn different projections for 'asking,' 'indexing,' and 'carrying content.'", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0011", "next_chunk_id": "ALecture8_Attention_and_Transformers__0013"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0013", "chunk_index": 13, "page_numbers": [17], "text": "- -Problem: when ùëëùëò gets large, the variance of ùëû ùëá ùëò increases\n- ‚Üí some values inside softmax get large\n- ‚Üí the softmax gets very peaked\n- ‚Üí hence its gradient gets smaller\n- -Solution: scale by the length of query/key vectors\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0012", "next_chunk_id": "ALecture8_Attention_and_Transformers__0014"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0014", "chunk_index": 14, "page_numbers": [18], "text": "Input\nEmbedding\nQueries\nKeys\nValues\nScore\nDivide by 8 (‚àödk)\nSoftmax\nSoftmax\nX\nValue\nSum\n<!-- formula-not-decoded -->\n- Scaled dot product attention.\n- dk is the size of the q, k, v vectors (64 in this case).", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0013", "next_chunk_id": "ALecture8_Attention_and_Transformers__0015"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0015", "chunk_index": 15, "page_numbers": [19], "text": "- -Input: multiple queries ùëû and a set of key-value ( ùëò -ùë£ ) pairs\n- -Output: a set of weighted sum of values\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0014", "next_chunk_id": "ALecture8_Attention_and_Transformers__0016"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0016", "chunk_index": 16, "page_numbers": [20], "text": "- -Constant 'path length' between two positions (just 1 attention hop)\n- -Every token can attend to every other in a single operation\n- -Easy to parallelize", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0015", "next_chunk_id": "ALecture8_Attention_and_Transformers__0017"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0017", "chunk_index": 17, "page_numbers": [24], "text": "Transformers\n- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0016", "next_chunk_id": "ALecture8_Attention_and_Transformers__0018"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0018", "chunk_index": 18, "page_numbers": [30], "text": "- -Convolution: different linear transformations by relative positions\n- -Attention: a weighted average\n- -Multi-Head Attention: parallel attention layers with different linear transformations on input/output", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0017", "next_chunk_id": "ALecture8_Attention_and_Transformers__0019"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0019", "chunk_index": 19, "page_numbers": [31], "text": "- -Idea: allow words to interact with one another\n- -Model\n- -Map V, K, Q to lower dimensional spaces\n- -Apply attention, concatenate outputs\n- -Linear transformation\nMultiHead(Q, K, V)\n- Concat(head1,... , headn)WO ‰∫å\nhead; = Attention(QW, KWK, VWY)", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0018", "next_chunk_id": "ALecture8_Attention_and_Transformers__0020"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0020", "chunk_index": 20, "page_numbers": [33], "text": "- 1) Concatenatealltheattentionheads\n2. 3)TheresultwouldbetheZmatrixthatcapturesinformation from all the attention heads.We can send this forward to the FFNN\n3. 2)Multiplywithaweight matrixW? thatwas trained jointly with the model\nX\nwo", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0019", "next_chunk_id": "ALecture8_Attention_and_Transformers__0021"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0021", "chunk_index": 21, "page_numbers": [35], "text": "- 1.Multi-headAttention\n- Query=Key=Value 2.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0020", "next_chunk_id": "ALecture8_Attention_and_Transformers__0022"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0022", "chunk_index": 22, "page_numbers": [35], "text": "- 1.Masked Multi-head Attention\n- Query=Key=Value 2.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0021", "next_chunk_id": "ALecture8_Attention_and_Transformers__0023"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0023", "chunk_index": 23, "page_numbers": [35], "text": "- 1.Multi-headAttention\n- 3.DecoderSelfattention= Query\n- 2.Encoder Self attention=Key=Va alue", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0022", "next_chunk_id": "ALecture8_Attention_and_Transformers__0024"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0024", "chunk_index": 24, "page_numbers": [36], "text": "Transformers\n- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0023", "next_chunk_id": "ALecture8_Attention_and_Transformers__0025"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0025", "chunk_index": 25, "page_numbers": [37], "text": "- -Problem: temporal information is missing\n- -Solution: positional encoding allows words at different locations to have different embeddings with fixed dimensions\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0024", "next_chunk_id": "ALecture8_Attention_and_Transformers__0026"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0026", "chunk_index": 26, "page_numbers": [38, 39], "text": "0.8\n0.4\n0.0\n-0.4\n-0.8\n- -Positional embeddings are concatenated to embeddings of each token.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0025", "next_chunk_id": "ALecture8_Attention_and_Transformers__0027"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0027", "chunk_index": 27, "page_numbers": [40], "text": "- -Every self-attention and feedforward sublayer has:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0026", "next_chunk_id": "ALecture8_Attention_and_Transformers__0028"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0028", "chunk_index": 28, "page_numbers": [41], "text": "- -As networks get deeper, gradients often vanish or explode.\n- -Even if they are okay, deep stacks of nonlinear layers are simply hard to optimize -they might fail to converge or converge very slowly.\n- -A residual block adds the input ùë• back to the transformed output ùêπ ùë• :\n<!-- formula-not-decoded -->\n- -With residuals, the layer only needs to learn the difference from the identity mapping, and not the entire transformation.\n- -Useful for easier optimization, better gradient flow, stabilization and preserving information.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0027", "next_chunk_id": "ALecture8_Attention_and_Transformers__0029"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0029", "chunk_index": 29, "page_numbers": [42], "text": "- Deep networks can suffer from internal covariate shift : the distribution of activations changes layer by layer, making training unstable.\n- Normalization techniques stabilize this by keeping activations in a consistent range.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0028", "next_chunk_id": "ALecture8_Attention_and_Transformers__0030"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0030", "chunk_index": 30, "page_numbers": [43], "text": "- -Each block has\n- -multi-head attention\n- -2-layer feed-forward NN (w/ ReLU)\n- -Both parts contain\n- -Residual connection & layer normalization (LayerNorm)\n- -Change input to have 0 mean and 1 variance per layer & per training point\n- -LayerNorm(x + sublayer(x))\nCompute mean, variance and then normalize\n‰∏≠ÊñáÁâà\n-ef2ddf8597a4", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0029", "next_chunk_id": "ALecture8_Attention_and_Transformers__0031"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0031", "chunk_index": 31, "page_numbers": [44], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0030", "next_chunk_id": "ALecture8_Attention_and_Transformers__0032"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0032", "chunk_index": 32, "page_numbers": [44], "text": "<!-- formula-not-decoded -->\nSame for all featuredimensions\nUnlike BatchNorm , which normalizes across a batch of examples, LayerNorm normalizes across the features of a single example -perfect for sequences and variable batch sizes.\n-", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0031", "next_chunk_id": "ALecture8_Attention_and_Transformers__0033"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0033", "chunk_index": 33, "page_numbers": [45], "text": "Transformers\n- -The Transformer Model Architecture\n- -Self Attention\n- -Multi-head attention\n- -The encoder block\n- -The decoder block", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0032", "next_chunk_id": "ALecture8_Attention_and_Transformers__0034"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0034", "chunk_index": 34, "page_numbers": [46], "text": "- -The decoder block is a stack of decoders of the same number.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0033", "next_chunk_id": "ALecture8_Attention_and_Transformers__0035"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0035", "chunk_index": 35, "page_numbers": [49], "text": "- -Non-recurrent encoder-decoder for MT\n- -http://nlp.seas.harvard.edu/2018/04/03/attention. html\n- -PyTorch explanation by Sasha Rush\nVaswani et al.,\n'\nAttention Is All You Need\n'\n, in\nNIPS\n, 2017.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0034", "next_chunk_id": "ALecture8_Attention_and_Transformers__0036"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0036", "chunk_index": 36, "page_numbers": [50], "text": "ByteNet [18], BLEU.EN-DE = 23.75. ByteNet [18], BLEU.EN-FR = . ByteNet [18], Training Cost (FLOPs).EN-DE = . ByteNet [18], Training Cost (FLOPs).EN-FR = . Deep-Att + PosUnk [39], BLEU.EN-DE = . Deep-Att + PosUnk [39], BLEU.EN-FR = 39.2. Deep-Att + PosUnk [39], Training Cost (FLOPs).EN-DE = . Deep-Att + PosUnk [39], Training Cost (FLOPs).EN-FR = 1.0¬∑ 1020. GNMT + RL [38], BLEU.EN-DE = 24.6. GNMT + RL [38], BLEU.EN-FR = 39.92. GNMT + RL [38], Training Cost (FLOPs).EN-DE = 2.3 ¬∑ 1019. GNMT + RL [38], Training Cost (FLOPs).EN-FR = 1.4 ¬∑ 1020. ConvS2S [9], BLEU.EN-DE = 25.16. ConvS2S [9], BLEU.EN-FR = 40.46. ConvS2S [9], Training Cost (FLOPs).EN-DE = 9.6 ¬∑ 1018. ConvS2S [9], Training Cost (FLOPs).EN-FR = 1.5 ¬∑ 1020. MoE [32], BLEU.EN-DE = 26.03. MoE [32], BLEU.EN-FR = 40.56. MoE [32], Training Cost (FLOPs).EN-DE = 2.0¬∑1019. MoE [32], Training Cost (FLOPs).EN-FR = 1.2¬∑ 1020. Deep-Att + PosUnk Ensemble [39], BLEU.EN-DE = . Deep-Att + PosUnk Ensemble [39], BLEU.EN-FR = 40.4. Deep-Att + PosUnk Ensemble [39], Training Cost (FLOPs).EN-DE = . Deep-Att + PosUnk Ensemble [39], Training Cost (FLOPs).EN-FR = 8.0 ¬∑ 1020. GNMT + RL Ensemble [38], BLEU.EN-DE = 26.30. GNMT + RL Ensemble [38], BLEU.EN-FR = 41.16. GNMT + RL Ensemble [38], Training Cost (FLOPs).EN-DE = 1.8¬∑1020. GNMT + RL Ensemble [38], Training Cost (FLOPs).EN-FR = 1.1¬∑1021. ConvS2S Ensemble [9], BLEU.EN-DE = 26.36. ConvS2S Ensemble [9], BLEU.EN-FR = 41.29. ConvS2S Ensemble [9], Training Cost (FLOPs).EN-DE = 7.7 ¬∑ 1019. ConvS2S Ensemble [9], Training Cost (FLOPs).EN-FR = 1.2 ¬∑ 1021. Transformer (base model), BLEU.EN-DE = 27.3. Transformer (base model), BLEU.EN-FR = 38.1. Transformer (base model), Training Cost (FLOPs).EN-DE = 3.3¬∑ 1018. Transformer (base model), Training Cost (FLOPs).EN-FR = 3.3¬∑ 1018. Transformer (big), BLEU.EN-DE = 28.4. Transformer (big), BLEU.EN-FR = 41.8. Transformer (big), Training Cost (FLOPs).EN-DE = 2.3 ¬∑1019. Transformer (big), Training Cost (FLOPs).EN-FR = 2.3 ¬∑1019", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0035", "next_chunk_id": "ALecture8_Attention_and_Transformers__0037"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0037", "chunk_index": 37, "page_numbers": [51], "text": "Vinyals & Kaiser el al. (2014) [37] Petrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8], Training = WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative. Vinyals & Kaiser el al. (2014) [37] Petrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8], WSJ 23 F1 = 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0036", "next_chunk_id": "ALecture8_Attention_and_Transformers__0038"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0038", "chunk_index": 38, "page_numbers": [52], "text": "- -Byte-pair encodings (details next week)\n- -ADAM optimizer with learning rate changes\n- -Dropout during training at every layer just before adding residual\n- -Auto-regressive decoding\n- -Use previous time step output as input\n- -Beam search and length penalties (beam size = 4, ‚ç∫ =0.65)\n- -Short utterances are favored in MT, so scores (log-probabilities) are divided by length ‚ç∫\n- -Label smoothing", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0037", "next_chunk_id": "ALecture8_Attention_and_Transformers__0039"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0039", "chunk_index": 39, "page_numbers": [53], "text": "- -Regularization technique that aims to deal with the problem of over-confidence on outputs.\n- -Replaces one-hot encoded target label vector yhot with a mixture of yhot and the uniform distribution:\n<!-- formula-not-decoded -->\nK: the number of label classes\nŒ± : hyperparameter that determines the amount of smoothing.\nIf Œ± = 0 ‚Üí the original one-hot encoded yhot .\nIf Œ± = 1 ‚Üí uniform distribution.", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0038", "next_chunk_id": "ALecture8_Attention_and_Transformers__0040"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0040", "chunk_index": 40, "page_numbers": [54], "text": "- -Directly correlated with a set of hyper-parameters.\n- -Number of:\n- -Layers\n- -Attention heads\n- -Embeddings", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0039", "next_chunk_id": "ALecture8_Attention_and_Transformers__0041"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0041", "chunk_index": 41, "page_numbers": [55], "text": "- -Non-recurrence model is easy to parallelize\n- -Multi-head attention captures different aspects by interacting between words\n- -Positional encodings capture location information\n- -Each transformer block can be applied to diverse tasks", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0040", "next_chunk_id": "ALecture8_Attention_and_Transformers__0042"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0042", "chunk_index": 42, "page_numbers": [56], "text": "- -Modern LLMs are using transformers, more recently the focus is on autoregressive/decoder-based models.\n- -Variations on positional embeddings, such as, relative position embeddings in TransformerXL (Dai et al., 2019), KERPLE, kernelized relative position embeddings (Chi et al, 2022).\n- -Several improvements since the original architecture, examples:\n- -Variations on attention structure, such as, sparse attention to relax the quadratic computation complexity for long sequences (Child et al., 2019), flash attention (Dao et al., 2022) and extensions, gated attention/ mixture of experts (Lepikhin et al., 2020) and switch transformers (Fedus et al., 2022).\n- -Changes in ordering of layer normalization, for example, original version: Post-LN versus Pre-LN (Xiong et al, 2020).\n- -Parameter efficiency during training, such as, adapter layers (Houlsby et al., 2019) and LowRank Adaptation (Hu et al., 2021).", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0041", "next_chunk_id": "ALecture8_Attention_and_Transformers__0043"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0043", "chunk_index": 43, "page_numbers": [57], "text": "- -Pre-training and Fine-tuning\nThursday\n- -Prompting & in-context learning", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0042", "next_chunk_id": "ALecture8_Attention_and_Transformers__0044"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0044", "chunk_index": 44, "page_numbers": [58, 59], "text": "Due: Tuesday, October 14th, 2025\nGoals: explore how pretrained language models can be adapted for NLP tasks\n1. Establish a baseline\n2. Fine-tune a model\n3. Evaluate\n4. Build good practices\nDue: Tuesday, October 14th, 2025\nGoals: explore how pretrained language models can be adapted for NLP tasks", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0043", "next_chunk_id": "ALecture8_Attention_and_Transformers__0045"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0045", "chunk_index": 45, "page_numbers": [59], "text": "1. The difference between frozen and fine-tuned representations\n2. How to set up training and evaluation loops\n3. How to critically assess model performance beyond accuracy", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0044", "next_chunk_id": "ALecture8_Attention_and_Transformers__0046"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0046", "chunk_index": 46, "page_numbers": [60, 61], "text": "- -NLP Task: Sentiment Classification on the IMDB movie reviews dataset\n- -Metrics such as recall, precision, and F1-scores for evaluation\nDue: Tuesday, October 14th, 2025", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0045", "next_chunk_id": "ALecture8_Attention_and_Transformers__0047"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0047", "chunk_index": 47, "page_numbers": [61], "text": "1. Fill in the TODO ' s in the .ipynb notebook\n2. Once you ' re satisfied with your answers, submit your notebook to Canvas with the following name: ' hw1_<YOUR_NET_ID>.ipynb '\n- a. Example: ' hw1_sagnikm3.ipynb '", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0046", "next_chunk_id": "ALecture8_Attention_and_Transformers__0048"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0048", "chunk_index": 48, "page_numbers": [62], "text": "Due: Tuesday, October 14th, 2025", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0047", "next_chunk_id": "ALecture8_Attention_and_Transformers__0049"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0049", "chunk_index": 49, "page_numbers": [62], "text": "1. Visible test cases in the HW ' s notebook\n2. A few hidden test cases", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0048", "next_chunk_id": "ALecture8_Attention_and_Transformers__0050"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0050", "chunk_index": 50, "page_numbers": [63], "text": "Due: Tuesday, October 14th, 2025", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0049", "next_chunk_id": "ALecture8_Attention_and_Transformers__0051"}
{"lecture_id": "ALecture8_Attention_and_Transformers", "source_pdf": "data/anlp/ALecture8_Attention_and_Transformers.pdf", "chunk_id": "ALecture8_Attention_and_Transformers__0051", "chunk_index": 51, "page_numbers": [63], "text": "1. Upload your model to HuggingFace (instructions here: https://mediaspace.illinois.edu/media/t/1_cr5kp3vd)\n2. Enter your model name and your email id on the google sheet here: https://docs.google.com/spreadsheets/d/1hC5i2Q6JcLvHAz2TOvAM mommita5XlpiypR4wMqMCYI/edit?usp=sharing", "prev_chunk_id": "ALecture8_Attention_and_Transformers__0050", "next_chunk_id": null}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture9_Pretrain_Finetune__0001"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Radford et al., 'Improving Language Understanding by Generative Pre -Training', 2018.\n- -Devlin et al., 'BERT: Pre -training of Deep Bidirectional Transformers for Language Understanding', NAACL-HLT, 2019.\n- -Lewis et al., ' BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension', arXiv preprint: 1910.13461, 2019.\n- -Raffel et al., 'Exploring the Limits of Transfer Learning with a Unified Text -toText Transformer', JMLR, 2020.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0000", "next_chunk_id": "ALecture9_Pretrain_Finetune__0002"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Tokenization and Byte Pair Encoding (BPE)\n- -Decoder-Only Models: OpenAI GPT\n- -Encoder-Decoder Models: BART, T5\n- -Parameter-Efficient Fine-Tuning (PEFT)\n- Encoder-Only Models: BERT\nAdapters, Prefix Tuning, LoRA", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0001", "next_chunk_id": "ALecture9_Pretrain_Finetune__0003"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -The input to all the models: tokens.\n- -How to define the token vocabulary?\n- -Inference usually encounters previously unseen words.\n- -Characters as the input unit, |V| = #characters in a language\n- -Could be too short to capture semantics, though we have seen recent success.\n- -Could word pieces be better tokens?\n- -Interaction, interdependency, international: inter-\n- -Training, chaining: -ing", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0002", "next_chunk_id": "ALecture9_Pretrain_Finetune__0004"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0004", "chunk_index": 4, "page_numbers": [5], "text": "- -A simple form of compression\n- -A table of replacements is required to rebuild the data.\n- -The most common byte pair is replaced with a byte not appearing in the data.\n- -Wikipedia example:\n```\naaabdaaabac ‚Üí ZabdZabac, Z=aa ‚Üí ZYdZYac, Y=ab, Z=aa ‚Üí XdXac, X=ZY, Y=ab, Z=aa aaab d aaab a c              V={aaab, a, b, c, d}\n```\nThe vocabulary is automatically determined.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0003", "next_chunk_id": "ALecture9_Pretrain_Finetune__0005"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0005", "chunk_index": 5, "page_numbers": [6], "text": "```\n[ ] from transformers import OpenAIGPTTokenizer text_example = \"Simplest t internationalization examples\" tokenizer.tokenize(text_example) C> ['simplest</w>'Ôºå'intern'Ôºå'ation'Ôºå'alization</w>'Ôºå'examples</w>'] [ ] from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") tokenizer.tokenize(text_example) C> ['simplest'Ôºå 'international'Ôºå '##ization'Ôºå 'examples'ÔºΩ\n```", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0004", "next_chunk_id": "ALecture9_Pretrain_Finetune__0006"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -Tokenization and Byte Pair Encoding (BPE)\n- -Decoder-Only Models: OpenAI GPT\n- -Encoder-Decoder Models: BART, T5\n- -Parameter-Efficient Fine-Tuning (PEFT)\n- Encoder-Only Models: BERT\nAdapters, Prefix Tuning, LoRA", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0005", "next_chunk_id": "ALecture9_Pretrain_Finetune__0007"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0007", "chunk_index": 7, "page_numbers": [8], "text": "- -Built from transformer decoder blocks\n- -Goals:\n- -leverage linguistic information from unlabeled data\n- -Learn a universal representation that transfer with little adaptation to a wide range of tasks\n- -Two-stage training procedure:\n- -Generative, unsupervised pre-training , LM objective\n- -Discriminative, supervised fine-tuning , using a taskspecific objective", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0006", "next_chunk_id": "ALecture9_Pretrain_Finetune__0008"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0008", "chunk_index": 8, "page_numbers": [9], "text": "- -Given a corpus of tokens: n un} ¬∑¬∑Ôºå\n- -Uses the standard LM objective to maximize likelihood:\n<!-- formula-not-decoded -->\n- -the context vector of tokens: (u-k,...,U-1) =\n- -the number of layers: n\n- -the token/position embedding matrix: We / Wp", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0007", "next_chunk_id": "ALecture9_Pretrain_Finetune__0009"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0009", "chunk_index": 9, "page_numbers": [10], "text": "-Labeled dataset, C: sequence of input tokens x 1 ,‚Ä¶, x m , and label y\n- -Classification Objective:\n<!-- formula-not-decoded -->\n- -Additional LM objective:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0008", "next_chunk_id": "ALecture9_Pretrain_Finetune__0010"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0010", "chunk_index": 10, "page_numbers": [11], "text": "- -Stanford Sentiment Treebank, binary sentiment\n- -The Corpus of Linguistic Acceptability (CoLA): whether a sentence is grammatical or not", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0009", "next_chunk_id": "ALecture9_Pretrain_Finetune__0011"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0011", "chunk_index": 11, "page_numbers": [12], "text": "- -Input: pair of sentences (premise and hypothesis),\n- -Output: their relationship: entailment, contradiction, neutral", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0010", "next_chunk_id": "ALecture9_Pretrain_Finetune__0012"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0012", "chunk_index": 12, "page_numbers": [13], "text": "- -Paraphrase Detection\n- -Two sentences, Text1 and Text2, are semantically equivalent or not.\n- -No ordering (unlike entailment)", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0011", "next_chunk_id": "ALecture9_Pretrain_Finetune__0013"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0013", "chunk_index": 13, "page_numbers": [14], "text": "- -RACE: English passages with associated questions from middle/high school exams\n- -Story Cloze Test: selecting the correct ending to multi-sentence stories, from two options.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0012", "next_chunk_id": "ALecture9_Pretrain_Finetune__0014"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0014", "chunk_index": 14, "page_numbers": [15], "text": "- -Pre-training: Books corpus (7K unique, unpublished books, ~1B words)\n- -# of layers transferred.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0013", "next_chunk_id": "ALecture9_Pretrain_Finetune__0015"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0015", "chunk_index": 15, "page_numbers": [16], "text": "- -Pre-training: Books corpus (7K unique, unpublished books, ~1B words)\n- -Ablation Studies:\nTransformer w/ aux LM (full), Avg. Score = 74.7. Transformer w/ aux LM (full), CoLA (mc) = 45.4. Transformer w/ aux LM (full), SST2 (acc) = 91.3. Transformer w/ aux LM (full), MRPC (FI) = 82.3. Transformer w/ aux LM (full), STSB (pc) = 82.0. Transformer w/ aux LM (full), QQP (FI) = 70.3. Transformer w/ aux LM (full), MNLI (acc) = 81.8. Transformer w/ aux LM (full), QNLI (acc) = 88.1. Transformer w/ aux LM (full), RTE (acc) = 56.0. Transformer w/o pre-training, Avg. Score = 59.9. Transformer w/o pre-training, CoLA (mc) = 18.9. Transformer w/o pre-training, SST2 (acc) = 84.0. Transformer w/o pre-training, MRPC (FI) = 79.4. Transformer w/o pre-training, STSB (pc) = 30.9. Transformer w/o pre-training, QQP (FI) = 65.5. Transformer w/o pre-training, MNLI (acc) = 75.7. Transformer w/o pre-training, QNLI (acc) = 71.2. Transformer w/o pre-training, RTE (acc) = 53.8. Transformer w/o aux LM, Avg. Score = 75.0. Transformer w/o aux LM, CoLA (mc) = 47.9. Transformer w/o aux LM, SST2 (acc) = 92.0. Transformer w/o aux LM, MRPC (FI) = 84.9. Transformer w/o aux LM, STSB (pc) = 83.2. Transformer w/o aux LM, QQP (FI) = 69.8. Transformer w/o aux LM, MNLI (acc) = 81.1. Transformer w/o aux LM, QNLI (acc) = 86.9. Transformer w/o aux LM, RTE (acc) = 54.4. LSTM w/ aux LM, Avg. Score = 69.1. LSTM w/ aux LM, CoLA (mc) = 30.3. LSTM w/ aux LM, SST2 (acc) = 90.5. LSTM w/ aux LM, MRPC (FI) = 83.2. LSTM w/ aux LM, STSB (pc) = 71.8. LSTM w/ aux LM, QQP (FI) = 68.1. LSTM w/ aux LM, MNLI (acc) = 73.7. LSTM w/ aux LM, QNLI (acc) = 81.1. LSTM w/ aux LM, RTE (acc) = 54.6", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0014", "next_chunk_id": "ALecture9_Pretrain_Finetune__0016"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0016", "chunk_index": 16, "page_numbers": [17], "text": "- -NeurIPS 2018, ConvAI 2 challenge\n- -Fine-tune pre-trained GPT for response generation in conversational systems.\n- -Fine-tuning is done by optimizing a combination of two loss functions:\n- -a language modeling loss, and\n- -a next-utterance classification loss\nWolf et al., TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents. 2018.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0015", "next_chunk_id": "ALecture9_Pretrain_Finetune__0017"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0017", "chunk_index": 17, "page_numbers": [18], "text": "- -Tokenization and Byte Pair Encoding (BPE)\n- -Encoder-Only Models: BERT\n- -Encoder-Decoder Models: BART, T5\n- -Parameter-Efficient Fine-Tuning (PEFT)\n- Decoder-Only Models: OpenAI GPT\nAdapters, Prefix Tuning, LoRA", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0016", "next_chunk_id": "ALecture9_Pretrain_Finetune__0018"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0018", "chunk_index": 18, "page_numbers": [19], "text": "- -Built from transformer encoder blocks only.\n- -Idea: contextualized word representations\n- -Learn word vectors using long contexts using Transformer instead of LSTM\n- -multi-layer bidirectional Transformer encoder.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0017", "next_chunk_id": "ALecture9_Pretrain_Finetune__0019"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0019", "chunk_index": 19, "page_numbers": [22], "text": "- -Idea: modeling relationship between sentences\n- -Many NLP tasks (i.e., QA) are based on understanding intersentence relationship\n```\nInput = [CLS] the man went to [MASK] ]  store [SEP] he bought a gallon [MASK] ] milk [SEP] Label IsNext Input [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label NotNext\n```", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0018", "next_chunk_id": "ALecture9_Pretrain_Finetune__0020"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0020", "chunk_index": 20, "page_numbers": [23], "text": "- -Idea: modeling relationship between sentences", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0019", "next_chunk_id": "ALecture9_Pretrain_Finetune__0021"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0021", "chunk_index": 21, "page_numbers": [24], "text": "- -Special tokens:\n- -[CLS]: added to the beginning of the text.\n- -[SEP]: added to the end. Also used as a separator.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0020", "next_chunk_id": "ALecture9_Pretrain_Finetune__0022"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0022", "chunk_index": 22, "page_numbers": [25], "text": "- -Input embeddings contain\n- -Word-level token embeddings\n- -Sentence-level segment embeddings\n- -Position embeddings", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0021", "next_chunk_id": "ALecture9_Pretrain_Finetune__0023"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0023", "chunk_index": 23, "page_numbers": [26], "text": "- -Training data: Wikipedia + BookCorpus\n- -2 BERT models\n- -BERT-Base: 12-layer, 768-hidden, 12-head\n- -BERT-Large: 24-layer, 1024-hidden, 16-head\nHyperparameters that determine the model size: number of layers, number of attention heads and hidden layer size.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0022", "next_chunk_id": "ALecture9_Pretrain_Finetune__0024"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0024", "chunk_index": 24, "page_numbers": [27], "text": "- -Idea: simply learn a classifier/tagger built on the top layer for each target task\n- -Mainly natural language understanding tasks", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0023", "next_chunk_id": "ALecture9_Pretrain_Finetune__0025"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0025", "chunk_index": 25, "page_numbers": [29], "text": "# layers, BERT base = 12. # layers, BERT large = 24. hidden layer size, BERT base = 768. hidden layer size, BERT large = 1024. # attention heads, BERT base = 12. # attention heads, BERT large = 16. Total # parameters, BERT base = 110M. Total # parameters, BERT large = 340M", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0024", "next_chunk_id": "ALecture9_Pretrain_Finetune__0026"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0026", "chunk_index": 26, "page_numbers": [30], "text": "- -'Probing LMs': representing language in a satisfying way?\n- -(Peters et al., 2018b): lower layers of a language model encode more local syntax while higher layers capture more complex semantics.\n- -Common components of a traditional NLP pipeline, such as part-ofspeech (POS), constituents (Consts.), dependencies (Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic protoroles (SPR; Reisinger et al., 2015), and relation classification (SemEval).", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0025", "next_chunk_id": "ALecture9_Pretrain_Finetune__0027"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0027", "chunk_index": 27, "page_numbers": [31, 32, 33], "text": "- -Edge Probing: how well information about linguistic structure can be extracted from a pre-trained encoder\n- -Given input tokens T = [t 0, t 1 , ‚Ä¶ , t n ], the encoder produces a set of layer activations, H (0) ,H (1) , ‚Ä¶, H (L) , where\n- -Classifier receives spans s 1 and (optionally) s 2 and must predict the relation, given only the per-token contextual vectors within the target spans.\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -Scalar mixing weights tell us which layers, in combination, are most relevant when a probing classifier, as in ELMo work.\n<!-- formula-not-decoded -->\n- -Higher weights seen as evidence that the corresponding layers have more information for a given task. L\n<!-- formula-not-decoded -->\n- -Center of Gravity\n- -Cumulative Scoring\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n: a set of classifiers that use scalar mixing to attend to layer l as well as all previous layers. : uses only the first l layers.\n- -Expected layer\n<!-- formula-not-decoded -->\nP(e)\n- -Center of Gravity\n- -Cumulative Scoring\n<!-- formula-not-decoded -->\n- -Expected layer\n<!-- formula-not-decoded -->\n: a set of classifiers that use scalar mixing to attend to layer l as well as all previous layers. : uses only the first l layers.\n<!-- formula-not-decoded -->\nT\nP(e)", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0026", "next_chunk_id": "ALecture9_Pretrain_Finetune__0028"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0028", "chunk_index": 28, "page_numbers": [34], "text": "- -Analyzing the attention mechanisms of pre-trained models.\n- -How much a particular word will be weighted when computing the next representation for the current word?\n- -BERT base: 12 layers, 12 attention heads.\n- -Analysis over 1000 random Wikipedia segments, at most 128 tokens each. Input:\n- -[CLS] <paragraph-1> [SEP] <paragraph-2> [SEP]", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0027", "next_chunk_id": "ALecture9_Pretrain_Finetune__0029"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0029", "chunk_index": 29, "page_numbers": [35], "text": "Head <layer>-<head-number>", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0028", "next_chunk_id": "ALecture9_Pretrain_Finetune__0030"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0030", "chunk_index": 30, "page_numbers": [35], "text": "Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the strength of the attention weight (some attention weights are so low they are invisible).", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0029", "next_chunk_id": "ALecture9_Pretrain_Finetune__0031"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0031", "chunk_index": 31, "page_numbers": [36, 37, 38, 39, 40], "text": "- -Relative position: how often BERTs heads attend to\n- -current token?\n- -little attention\n- -previous or next token?\n- -heads that specialize to attending to previous or next tokens, especially in the lower layers\n- -Attention to separator tokens:\n- -Lower layer heads attend to [CLS], middle to [SEP], higher layer heads attend to periods and commas (often more than 50%)\n- -Heads attend to [SEP] tokens even more when the current token is [SEP] itself.\nLayer\n12\n- -Focused vs. broad attention\n- -whether attention heads focus on a few words or attend broadly over many words.\n- -average entropy of each head's attention distribution\n- -some attention heads, especially in lower layers, have very broad attention.\n- -Probing individual attention heads\n- -Dependency syntax: evaluated both directions of prediction for each attention head:\n- -the head word attending to the dependent and vice versa\n- -For a given attention head and word, whichever other word receives the most attention weight is the model's prediction (excluding [SEP] and [CLS]).\n- Probing individual attention heads\n- Dependency syntax , the best performing attentions heads of BERT on WSJ dependency parsing by dependency type.\n- Fixed offset baseline: the word n positions to the left of the dependent is always considered to be the head\nAll, Head = 7-6. All, Accuracy = 34.5. All, Baseline = 26.3 (1). prep, Head = 7-4. prep, Accuracy = 66.7. prep, Baseline = 61.8 (-1). rqod, Head = 9-6. rqod, Accuracy = 76.3. rqod, Baseline = 34.6 (-2). det, Head = 8-11. det, Accuracy = 94.3. det, Baseline = 51.7 (1). nn, Head = 4-10. nn, Accuracy = 70.4. nn, Baseline = 70.2 (1). nsubj, Head = 8-2. nsubj, Accuracy = 58.5. nsubj, Baseline = 45.5 (1). amod, Head = 4-10. amod, Accuracy = 75.6. amod, Baseline = 68.3 (1). dobj, Head = 8-10. dobj, Accuracy = 86.8. dobj, Baseline = 40.0 (-2). advmod, Head = 7-6. advmod, Accuracy = 48.8. advmod, Baseline = 40.2 (1). aux, Head = 4-10. aux, Accuracy = 81.1. aux, Baseline = 71.5 (1). poss, Head = 7-6. poss, Accuracy = 80.5. poss, Baseline = 47.7 (1). auxpass, Head = 4-10. auxpass, Accuracy = 82.5. auxpass, Baseline = 40.5 (1). duooo, Head = 8-1. duooo, Accuracy = 48.8. duooo, Baseline = 12.4 (-2). mark, Head = 8-2. mark, Accuracy = 50.7. mark, Baseline = 14.5 (2). prt, Head = 6-7. prt, Accuracy = 99.1. prt, Baseline = 91.4 (-1)\nThere is no single attention head that does well at syntax 'overall'. However, certain attention heads specialize to specific dependency relations, sometimes achieving high accuracy and substantially outperforming the fixed-offset baseline.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0030", "next_chunk_id": "ALecture9_Pretrain_Finetune__0032"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0032", "chunk_index": 32, "page_numbers": [41], "text": "- -Tokenization and Byte Pair Encoding (BPE)\n- -Decoder-Only Models: OpenAI GPT\n- -Encoder-Decoder Models: BART, T5\n- -Parameter-Efficient Fine-Tuning (PEFT)\n- Encoder-Only Models: BERT\nAdapters, Prefix Tuning, LoRA", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0031", "next_chunk_id": "ALecture9_Pretrain_Finetune__0033"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0033", "chunk_index": 33, "page_numbers": [42], "text": "- -Pre-trains a model by combining bi-directional and auto-regressive transformers.\nBERT\nB\nBidirectional\nEncoder\nGPT\nC\nAB\nD\nE\nAutoregressive\nDecoder\n<S>A B C\n- -The architecture is similar to BERT, with the following differences:\n- -each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder\n- -BERT uses an additional feed-forward network before word prediction, BART does not.\nA key advantage: noising flexibility, arbitrary transformations can be applied to the original text, including changing its length.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0032", "next_chunk_id": "ALecture9_Pretrain_Finetune__0034"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0034", "chunk_index": 34, "page_numbers": [44], "text": "- -Sequence classification\n- -Token classification\nLabel sequence: s a b c d e\n- -Sequence generation\n- -Encoder: input sequence\n- -Decoder: generates output autoregressively.\n- -Machine translation", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0033", "next_chunk_id": "ALecture9_Pretrain_Finetune__0035"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0035", "chunk_index": 35, "page_numbers": [45], "text": "- -Recent techniques for transfer learning in NLP pre-train models on large, unlabeled data.\n- -Common Crawl project produces about 20TB of text data extracted from web pages each month.\n- -Explores transfer learning techniques through converting all textbased language problems into a text-to-text format.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0034", "next_chunk_id": "ALecture9_Pretrain_Finetune__0036"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0036", "chunk_index": 36, "page_numbers": [46], "text": "- -Randomly sample and drop 15% of tokens in each sentence.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0035", "next_chunk_id": "ALecture9_Pretrain_Finetune__0037"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0037", "chunk_index": 37, "page_numbers": [48], "text": "- -Tokenization and Byte Pair Encoding (BPE)\n- -Decoder-Only Models: OpenAI GPT\n- -Encoder-Decoder Models: BART, T5\n- -Parameter-Efficient Fine-Tuning (PEFT)\n- Encoder-Only Models: BERT\nAdapters, Prefix Tuning, LoRA", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0036", "next_chunk_id": "ALecture9_Pretrain_Finetune__0038"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0038", "chunk_index": 38, "page_numbers": [50], "text": "- -Notable gains due to large models, trained on large datasets on multiple NLP benchmarks.\n- -Training these models require large resources", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0037", "next_chunk_id": "ALecture9_Pretrain_Finetune__0039"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0039", "chunk_index": 39, "page_numbers": [50], "text": "Wow:Google's \"Meena\"chatbot was trained on a full TPUv3 pod (2048 TPU cores)for **30 full days** -That's more than $1,400,000 of compute time to train this chatbot model. (! 1o0+ petaflops of sustained compute\n- -and energy consumption!", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0038", "next_chunk_id": "ALecture9_Pretrain_Finetune__0040"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0040", "chunk_index": 40, "page_numbers": [51], "text": "- -Some of this energy may come from renewable or carbon credit-offset resources, however:\n1. This is still not the case for many locations\n2. Even when renewable energy is available, it might better be allocated for other things.\n- -We must cut carbon emissions by half over the next decade to deter escalating rates of natural disasters.\nAir travel, 1 passenger, NY<‚ÜíSF, CO2e (lbs) = 1984. Human life, avg, 1 year, CO2e (lbs) = 11,023. American life, avg, 1 year, CO2e (lbs) = 36,156. Car, avg incl. fuel, 1 lifetime, CO2e (lbs) = 126,000. Training one model (GPU), CO2e (lbs) = . NLP pipeline (parsing, SRL), CO2e (lbs) = 39. w/ tuning & experimentation, CO2e (lbs) = 78,468. Transformer (big), CO2e (lbs) = 192. w/ neural architecture search, CO2e (lbs) = 626,155\nTable 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.1", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0039", "next_chunk_id": "ALecture9_Pretrain_Finetune__0041"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0041", "chunk_index": 41, "page_numbers": [52], "text": "- Analysis of 4 popular models\n- Re-trained for a maximum of a day with default settings\n- Sampled GPU and SPU power consumption\nT2Tbase, Hardware = P100x8. T2Tbase, Power (W) = 1415.78. T2Tbase, Hours = 12. T2Tbase, kWh.PUE = 27. T2Tbase, CO2e = 26. T2Tbase, Cloudcomputecost = $41-$140. T2Tbig, Hardware = P100x8. T2Tbig, Power (W) = 1515.43. T2Tbig, Hours = 84. T2Tbig, kWh.PUE = 201. T2Tbig, CO2e = 192. T2Tbig, Cloudcomputecost = $289-$981. ELMo, Hardware = P100x3. ELMo, Power (W) = 517.66. ELMo, Hours = 336. ELMo, kWh.PUE = 275. ELMo, CO2e = 262. ELMo, Cloudcomputecost = $433-S1472. BERTbase, Hardware = V100x64. BERTbase, Power (W) = 12,041.51. BERTbase, Hours = 79. BERTbase, kWh.PUE = 1507. BERTbase, CO2e = 1438. BERTbase, Cloudcomputecost = $3751-$12,571. BERTbase, Hardware = TPUv2x16. BERTbase, Power (W) = . BERTbase, Hours = 96. BERTbase, kWh.PUE = . BERTbase, CO2e = . BERTbase, Cloudcomputecost = $2074-$6912. NAS, Hardware = P100x8. NAS, Power (W) = 1515.43. NAS, Hours = 274,120. NAS, kWh.PUE = 656,347. NAS, CO2e = 626,155. NAS, Cloudcomputecost = $942,973-$3.201,722. NAS, Hardware = TPUv2x1. NAS, Power (W) = . NAS, Hours = 32,623. NAS, kWh.PUE = . NAS, CO2e = . NAS, Cloudcomputecost = $44.055-$146.848. GPT-2, Hardware = TPUv3x32. GPT-2, Power (W) = . GPT-2, Hours = 168. GPT-2, kWh.PUE = . GPT-2, CO2e = . GPT-2, Cloudcomputecost = $12,902-$43,008\nTable 3: Estimated cost of training a model in terms of CO2 emissions (Ibs) and cloud compute cost (USD).7 Power and carbon footprint are omitted for TPUs due to lack of public information on power draw for this hardware.\nIt is important to prioritize innovation of computationally efficient hardware and algorithms!", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0040", "next_chunk_id": "ALecture9_Pretrain_Finetune__0042"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0042", "chunk_index": 42, "page_numbers": [53], "text": "- -Feature-based transfer learning\n- -Fine-tuning\n- -Often performs better for NLP tasks!\n- -But, do we have to change all the model parameters during finetuning?", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0041", "next_chunk_id": "ALecture9_Pretrain_Finetune__0043"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0043", "chunk_index": 43, "page_numbers": [54], "text": "- -Houlsby et al, ICML 2019.\n- -New layers added between layers of the original neural network.\n- -An LM with parameters w : …∏ w ( x )\n- -Pretraining involves updating w .\n- -Feature-based transfer composes …∏ w with a new function ùõø v , resulting in ùõø v ( …∏ w ( x )). Only the new parameters, v , are trained.\n- -For adapter tuning, a new function Œ† w,v ( x ) is defined (i.e., adding new layers to the network)\n- -Parameters w are copied from the original model\n- -Parameters v are tuned during training.\n- -| v | << | w |", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0042", "next_chunk_id": "ALecture9_Pretrain_Finetune__0044"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0044", "chunk_index": 44, "page_numbers": [55], "text": "- Weights of the original network are frozen, and are shared by many tasks.\n- Weights of the new layers are initialized at random, and mimic the original behavior at the beginning (near identity).\n- Only the weights of the new layers are updated during training.\n- Bottleneck layers mapping: d dimensions to m then back to d (2 dm + d + m parameters to be trained), d >> m .", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0043", "next_chunk_id": "ALecture9_Pretrain_Finetune__0045"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0045", "chunk_index": 45, "page_numbers": [56], "text": "BERTLARGE, Total num params = 9.0x. BERTLARGE, Trained params / task = 100%. BERTLARGE, CoLA = 60.5. BERTLARGE, SST = 94.9. BERTLARGE, MRPC = 89.3. BERTLARGE, STS-B = 87.6. BERTLARGE, QQP = 72.1. BERTLARGE, MNLIm = 86.7. BERTLARGE, MNLImm = 85.9. BERTLARGE, QNLI = 91.1. BERTLARGE, RTE = 70.1. BERTLARGE, Total = 80.4. Adapters (8-256), Total num params = 1.3x. Adapters (8-256), Trained params / task = 3.6%. Adapters (8-256), CoLA = 59.5. Adapters (8-256), SST = 94.0. Adapters (8-256), MRPC = 89.5. Adapters (8-256), STS-B = 86.9. Adapters (8-256), QQP = 71.8. Adapters (8-256), MNLIm = 84.9. Adapters (8-256), MNLImm = 85.1. Adapters (8-256), QNLI = 90.7. Adapters (8-256), RTE = 71.5. Adapters (8-256), Total = 80.0. Adapters (64), Total num params = 1.2x. Adapters (64), Trained params / task = 2.1%. Adapters (64), CoLA = 56.9. Adapters (64), SST = 94.2. Adapters (64), MRPC = 89.6. Adapters (64), STS-B = 87.3. Adapters (64), QQP = 71.8. Adapters (64), MNLIm = 85.3. Adapters (64), MNLImm = 84.6. Adapters (64), QNLI = 91.4. Adapters (64), RTE = 68.8. Adapters (64), Total = 79.6\n9x parameters are obtained with fine-tuning, as there are 9 tasks. When optimum bottleneck size per task is chosen, GLUE score is 80.0 When bottleneck size is limited to 64 for all tasks, GLUE score is 79.6", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0044", "next_chunk_id": "ALecture9_Pretrain_Finetune__0046"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0046", "chunk_index": 46, "page_numbers": [58], "text": "- -Hu & Shen et al., ICLR 2022.\n- -Adapter layers introduce inference latency.\n- -Prefix tuning can be difficult to optimize.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0045", "next_chunk_id": "ALecture9_Pretrain_Finetune__0047"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0047", "chunk_index": 47, "page_numbers": [59, 60], "text": "- -Autoregressive LM P …∏ ( y | x ) parameterized by …∏ .\n- -Downstream tasks represented by a training dataset of context, target pairs:\n<!-- formula-not-decoded -->\n- -During full fine-tuning, the model is initialized to pre-trained weights …∏ 0 and updated to …∏ 0 + Œî…∏ by repeatedly following the gradient to maximize the conditional language modeling objective:\n<!-- formula-not-decoded -->\n- -For a pre-trained weight matrix W0 ‚àà R d x k , change the training update with a low rank decomposition:\n<!-- formula-not-decoded -->\nwhere A ‚àà R d x r and B ‚àà R r x k and the rank r << min( d, k ).\n- -For h = W0  x , the modified forward pass yields:\n<!-- formula-not-decoded -->\n- -During training, W0 is frozen, only A and B include trainable parameters and are updated.\n- -Random Gaussian initialization for A and zero for B .\nFigure 1: Our reparametriza tion. We only y train A and B.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0046", "next_chunk_id": "ALecture9_Pretrain_Finetune__0048"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0048", "chunk_index": 48, "page_numbers": [61], "text": "- -(Li and Liang, 2021) https://arxiv.org/pdf/2101.00190.pdf\n- -Parameter efficient fine-tuning method, like low-rank adapters (LoRA).\n- -Original model weights are frozen during fine-tuning.\n- -Training optimizes a small vector called the prefix for each new task.\n- -The prefix vectors are prepended to the keys and values of the multi-head attention at every transformer layer.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0047", "next_chunk_id": "ALecture9_Pretrain_Finetune__0049"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0049", "chunk_index": 49, "page_numbers": [62], "text": "- -Depiction of different parameter-efficient finetuning methods from (He et al., ICLR, 2022)\nhttps://arxiv.org/pdf/2110.04366.pdf\n- -All require supervision to finetune the newly added parameters.", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0048", "next_chunk_id": "ALecture9_Pretrain_Finetune__0050"}
{"lecture_id": "ALecture9_Pretrain_Finetune", "source_pdf": "data/anlp/ALecture9_Pretrain_Finetune.pdf", "chunk_id": "ALecture9_Pretrain_Finetune__0050", "chunk_index": 50, "page_numbers": [63], "text": "- -Major Paradigms in NLP\n- -Prompting\n- -In-context Learning\n- -Prompting Examples", "prev_chunk_id": "ALecture9_Pretrain_Finetune__0049", "next_chunk_id": null}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture10_Prompting__0001"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Adapters\n- -LoRA\n- -Prefix Tuning", "prev_chunk_id": "ALecture10_Prompting__0000", "next_chunk_id": "ALecture10_Prompting__0002"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Major Paradigms in NLP\n- -Prompting\n- -In-context Learning\n- -Prompting Examples", "prev_chunk_id": "ALecture10_Prompting__0001", "next_chunk_id": "ALecture10_Prompting__0003"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Liu et al., Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing, ACM Computing Surveys, 2023. *** Many examples, images and tables from this paper are included in my\n- slides.\n- -Schulhoff et al., The Prompt Report: A Systematic Survey of Prompt Engineering Techniques, Preprint, 2024.\n- -Dong et al., A survey on In-context Learning. EMNLP, 2024.\n- -Bertsch et al., In-Context Learning with Long-Context Models: An In-Depth Exploration, NAACL, 2025.", "prev_chunk_id": "ALecture10_Prompting__0002", "next_chunk_id": "ALecture10_Prompting__0004"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0004", "chunk_index": 4, "page_numbers": [5], "text": "Rule-based Systems\n- -The classical approach\n- -Relies on manually created rules and lexical resources to process language.\n- -A human specifies the system's behavior explicitly with rules that directly encode the logic.\n- -Pros: Transparent, interpretable, and precise for well-defined domains.\n- -Cons: Brittle, difficult to scale, and poor generalization to opendomain tasks.", "prev_chunk_id": "ALecture10_Prompting__0003", "next_chunk_id": "ALecture10_Prompting__0005"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0005", "chunk_index": 5, "page_numbers": [6, 7, 8, 9], "text": "Rule-based Systems\nML & Feature Engineering\n- -Fully supervised learning with statistical or classical machine learning models (non-neural networks, models such as support vector machines and conditional random fields)\n- -A human manually designs and selects the features (POS tags, lemmas, TF-IDF, etc.) -or inputs -to the model.\n- -Goal: represent raw text data in a numerical format that is effective for the task.\n- -Pros: Control over what the model sees and works well with limited data.\n- -Cons: Requires domain expertise and features can fail to capture deep domain semantics.\nRule-based Systems\nArchitecture Engineering\nML & Feature Engineering\n- -The approach of designing and training specific neural network architectures (such as, recurrent neural networks, gated attention, etc.) for particular NLP tasks, often without pre-training and fully supervised learning.\n- -The model's architecture itself is engineered to learn features automatically (rather than manual feature engineering).\n- -Pros: Architecture can embed domain knowledge (inductive biases) and the design can be interpretable.\n- -Cons: Manual effort and domain expertise is needed, lots of trial and error is required, models can be fragile.\nRule-based Systems\nArchitecture Engineering\nObjective Engineering\nML & Feature Engineering\n- -The design of training objectives / loss functions that guide training of a model.\n- -Instead of focusing on inputs (feature engineering) or model structure (architecture engineering), objective engineering focuses on what the model is optimized to do.\n- -Examples: contrastive objectives; next sentence or masked word prediction for language models.\n- -Pros: Flexible (you can adapt models to new goals), can scale (i.e., web data)\n- -Cons: Can be complex and require expertise (to match the end task).\nRule-based Systems\nArchitecture Engineering\nObjective Engineering\nML & Feature Engineering\nPre-train & Fine-tuning\n- -Game changer due to performance improvements for many tasks.\n- -Unsupervised pretraining of massive models on large text corpora, then fine-tuning for tasks.\n- -Pros: Few-shot and zero-shot abilities; broad generalization; reduced need for task-specific datasets.\n- -Cons: High compute costs, opaqueness, hallucinations, ethical concerns.", "prev_chunk_id": "ALecture10_Prompting__0004", "next_chunk_id": "ALecture10_Prompting__0006"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0006", "chunk_index": 6, "page_numbers": [10], "text": "- -Major Paradigms in NLP\n- -Prompting\n- -In-context Learning\n- -Prompting Examples", "prev_chunk_id": "ALecture10_Prompting__0005", "next_chunk_id": "ALecture10_Prompting__0007"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0007", "chunk_index": 7, "page_numbers": [11], "text": "Rule-based Systems\nArchitecture Engineering\nObjective Engineering\nPrompt Engineering\nML & Feature Engineering\nPre-train & Fine-tuning\n- -Instead of adapting pre-trained LMs to downstream tasks via adding additional parameters and fine-tuning, downstream tasks are reformulated to look more like those solved during the original LM training.\n- -Example: recognizing the emotion of a tweet 'I missed the bus today.'\n- -Continue it with a prompt\n- 'I felt so '\n- -Manipulate the model behavior so that the pre-trained LM itself can be used to predict the desired output\n- -sometimes even without any additional task-specific training.\nGiven a set of prompts, an LLM trained in a self-supervised fashion can be used to solve many tasks!", "prev_chunk_id": "ALecture10_Prompting__0006", "next_chunk_id": "ALecture10_Prompting__0008"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0008", "chunk_index": 8, "page_numbers": [12], "text": "- -Finding the most appropriate prompt to solve a given task using a given LLM with a high accuracy.\n- -Prompt-based Learning\n- -Steps for Prompting\n- -Design Considerations for Prompting\n- -Choosing the Pre-trained LM (PLM)\n- -Prompt Engineering\n- -Answer Engineering\n- -Multi-Prompt Learning\nImage from the prompting paper of Liu et al, 2021", "prev_chunk_id": "ALecture10_Prompting__0007", "next_chunk_id": "ALecture10_Prompting__0009"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0009", "chunk_index": 9, "page_numbers": [13], "text": "Input Output, Notation = y. Input Output, Example = I love this movie. ++ (very positive). Input Output, Description = One or multiple texts Output label or text. Prompting Function, Notation = fprompt(Œ±). Prompting Function, Example = . Prompting Function, Description = A function that converts the input into a specific form by inserting the input c and adding a slot [Z] where answer z may be filled later.. , Notation = . , Example = [X]  Overall, it was a Z] movie.. , Description = . Prompt, Notation = . Prompt, Example = I love this movie. Overall, it was a [Z] movie.. Prompt, Description = A text where [X] is instantiated by input c but answer slot [z] is not.. Filled Prompt, Notation = fal(Œ±', z). Filled Prompt, Example = I love this movie. Overall, it was a bad movie.. Filled Prompt, Description = A prompt where slot [Z] is filled with any answer.. Answered Prompt, Notation = ffll(Œ±c',‚âà*). Answered Prompt, Example = I love this movie. Overall, it was a good movie.. Answered Prompt, Description = A prompt where slot [Z] is filled with a true answer.. Answer, Notation = . Answer, Example = ‚Äúgood\", ‚Äúfantastic\", ‚Äúboring\". Answer, Description = A token, phrase, or sentence that fills [ZÔºΩ", "prev_chunk_id": "ALecture10_Prompting__0008", "next_chunk_id": "ALecture10_Prompting__0010"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0010", "chunk_index": 10, "page_numbers": [14], "text": "- -Formulating the prompt templates\n- -Formulating the set of answers\n- -Predicting the answers\n- -Mapping answers back to target labels", "prev_chunk_id": "ALecture10_Prompting__0009", "next_chunk_id": "ALecture10_Prompting__0011"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0011", "chunk_index": 11, "page_numbers": [15], "text": "- -Example task: sentiment classification for tweets.\n- -Input: x = 'I enjoyed this movie a lot.'\n- -Output: y ‚àà {positive, negative}\n- -Create a prompt, x, using x.\n- -Template with two slots [x] and[z]:\n- -x = '[x] It was a [z] movie.'\n- -x is the input, z is the intermediate, generated answer that will later be mapped to y.", "prev_chunk_id": "ALecture10_Prompting__0010", "next_chunk_id": "ALecture10_Prompting__0012"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0012", "chunk_index": 12, "page_numbers": [15], "text": "- -'I enjoyed this movie a lot. It was a [z] movie.'", "prev_chunk_id": "ALecture10_Prompting__0011", "next_chunk_id": "ALecture10_Prompting__0013"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0013", "chunk_index": 13, "page_numbers": [16], "text": "Text CLS, Task = Sentiment. Text CLS, Input ([x]) = I love this movie.. Text CLS, Template = [X] The movie is [Z].. Text CLS, Answer([z]Ôºâ = great fantastic. Text CLS, Task = Topics. Text CLS, Input ([x]) = He prompted the LM.. Text CLS, Template = [X] The text is about [Z].. Text CLS, Answer([z]Ôºâ = sports science .\".. Text CLS, Task = Intention. Text CLS, Input ([x]) = What is taxi fare to Denver?. Text CLS, Template = [X]The question is about [Z].. Text CLS, Answer([z]Ôºâ = quantity city .‚Ä¶.. Text-span CLS, Task = Aspect Sentiment. Text-span CLS, Input ([x]) = Poor service but good food.. Text-span CLS, Template = [X]What about service?[Z].. Text-span CLS, Answer([z]Ôºâ = Bad Terrible .\".. Text-pair CLS, Task = NLI. Text-pair CLS, Input ([x]) = [X1]: An old man with ... [X2]: A man walks ... Text-pair CLS, Template = [X1]?[Z],[X2]. Text-pair CLS, Answer([z]Ôºâ = Yes No ..... Tagging, Task = NER. Tagging, Input ([x]) = [X1]: Mike went to Paris. [X2]:Paris. Tagging, Template = [X1][X2] is a [Z] entity.. Tagging, Answer([z]Ôºâ = organization location .... Text Generation, Task = Summarization. Text Generation, Input ([x]) = Las Vegas police .... Text Generation, Template = [X] TL;DR:[Z]. Text Generation, Answer([z]Ôºâ = The victim ... A woman ... .... Text Generation, Task = Translation. Text Generation, Input ([x]) = Je vous aime.. Text Generation, Template = French:[X] English:[Z]. Text Generation, Answer([z]Ôºâ = I love you. I fancy you.", "prev_chunk_id": "ALecture10_Prompting__0012", "next_chunk_id": "ALecture10_Prompting__0014"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0014", "chunk_index": 14, "page_numbers": [17], "text": "- -The empty slot to fill, z, can be in the middle (akin to the cloze task) or at the end.\n- -The template tokens don't need to be natural language.\n- -The number of x and z slots in the template may change depending on the task.\n- -The template may include answer options, e.g., for classification tasks [x] Out of possible topics, such as agriculture, politics, finance, science and technology, this document is about [z]", "prev_chunk_id": "ALecture10_Prompting__0013", "next_chunk_id": "ALecture10_Prompting__0015"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0015", "chunk_index": 15, "page_numbers": [18], "text": "- -Determining the terms in       . It could be equivalent to the complete vocabulary or a small set of terms.\n- -A mapping between labels and answers\n- -Positive => great\n- -Negative => boring", "prev_chunk_id": "ALecture10_Prompting__0014", "next_chunk_id": "ALecture10_Prompting__0016"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0016", "chunk_index": 16, "page_numbers": [19], "text": "- -Given the prompt, predict [z], that maximizes the LM probability. 'I enjoyed this movie a lot. It was a great movie.'\n<!-- formula-not-decoded -->\n- -Constrained generation to set 2\n- -Choosing the right language model, i.e., pre-trained with masked language modeling objective for our template.", "prev_chunk_id": "ALecture10_Prompting__0015", "next_chunk_id": "ALecture10_Prompting__0017"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0017", "chunk_index": 17, "page_numbers": [20], "text": "- -Map the given answer (i.e., great for [z]), into the class label.\n- -Great => positive", "prev_chunk_id": "ALecture10_Prompting__0016", "next_chunk_id": "ALecture10_Prompting__0018"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0018", "chunk_index": 18, "page_numbers": [21], "text": "- -No need for expensive parameter fine-tuning.\n- -No need for creating training datasets (for zero-shot prompting).", "prev_chunk_id": "ALecture10_Prompting__0017", "next_chunk_id": "ALecture10_Prompting__0019"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0019", "chunk_index": 19, "page_numbers": [21], "text": "- -There are many ways to write prompts for each task and performance is dependent on prompt quality.\n- -The same prompt may not work similarly with different models.", "prev_chunk_id": "ALecture10_Prompting__0018", "next_chunk_id": "ALecture10_Prompting__0020"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0020", "chunk_index": 20, "page_numbers": [22], "text": "- -Gonen et al, Findings of EMNLP, 2023.\n- -Perplexity is a strong predictor of the success of a prompt.\n- -Lowest perplexity prompts are consistently effective.\nWhat is this piece of news regarding?, Accuracy = 40.9. What is this article about?, Accuracy = 52.4. What is the best way to describe this article?, Accuracy = 68.2. What is the most accurate label for this news article?, Accuracy = 71.2\nTable 1: Example prompts for the task AG News (news classification) that vary considerably in accuracy.", "prev_chunk_id": "ALecture10_Prompting__0019", "next_chunk_id": "ALecture10_Prompting__0021"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0021", "chunk_index": 21, "page_numbers": [23], "text": "- -How surprised an LM is when seeing the next word.\n- -Measured as 2 H when dealing with log base 2.\n- -Formally, it is the inverse probability of the test set normalized by the number of words:\n<!-- formula-not-decoded -->\nAverage cross entropy", "prev_chunk_id": "ALecture10_Prompting__0020", "next_chunk_id": "ALecture10_Prompting__0022"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0022", "chunk_index": 22, "page_numbers": [24], "text": "- -Sclar et al, ICLR, 2024.\n- -Comparing models with respect to fixed prompts may not make sense.\n- -Studies evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats.\nFigure 1: Slight modifications in prompt format templating may lead to significantly different model performance for a given task. Each <text> represents a different variable-length placeholder to be replaced with actual data samples. Example shown corresponds to 1-shot LLaMA-2-7B performances for task280 from SuperNaturalInstructions (Wang et al., 2022). This StereoSet-inspired task (Nadeem et al., 2021) requires the model to, given a short passage, classify it into one of four types of stereotype or anti-stereotype (gender, profession, race, and religion).", "prev_chunk_id": "ALecture10_Prompting__0021", "next_chunk_id": "ALecture10_Prompting__0023"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0023", "chunk_index": 23, "page_numbers": [25], "text": "- -Given a task/prompt, which LLM may be more appropriate?\n- -E.g., Masked LMs versus Autoregressive models\n- -Or vice versa, what prompt shape would be more appropriate given the task and model?\n- -Cloze/Masking Tokens 'I enjoyed this movie a lot. Overall, it was a [z] movie.'\n- -Prefix\n- 'I enjoyed this movie a lot. Overall, this movie was [z]'", "prev_chunk_id": "ALecture10_Prompting__0022", "next_chunk_id": "ALecture10_Prompting__0024"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0024", "chunk_index": 24, "page_numbers": [26], "text": "- -Given a task/prompt, how to define a good mapping function between the target labels and answers?\n- -Answer shape (tokens, spans, sentences, etc.)\n- -Manual design versus search (discrete versus continuous)", "prev_chunk_id": "ALecture10_Prompting__0023", "next_chunk_id": "ALecture10_Prompting__0025"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0025", "chunk_index": 25, "page_numbers": [27], "text": "- -Prompt mining: given a set of training inputs, x, and targets, y, mine patterns from large corpora (i.e., Wikipedia) that include x and y (Jiang et al., ACL 2020; Xue et al., 2024)\n- -Prompt Scoring: Hand-craft potential templates, fill them with examples to form prompts, score them with an LM, and select the most probable ones. (Davidson et al., EMNLP 2019)", "prev_chunk_id": "ALecture10_Prompting__0024", "next_chunk_id": "ALecture10_Prompting__0026"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0026", "chunk_index": 26, "page_numbers": [28, 29], "text": "- -Prompt paraphrasing: given an existing prompt, derive paraphrases, and use the one that maximizes accuracy on task data. Paraphrasing can be done in many ways, such as\n- -translating to another language and back (Jiang et al, ACL 2020),\n- -using linguistic resources (Yuan et al, NeurIPS 2021),\n- -using LLMs to rewrite the prompts (Haviv et al., ACL 2021),\n- -monotonic paraphrasing (Liu et al., EMNLP 2024)\n- -finding the prompt with the lowest perplexity is challenging\n- -MP is an ensemble based decoding method that combines the token probabilities from the paraphrase model and the target model in each decoding step\nreport\n- -Gradient-based Search: search over tokens to find short sequences that can trigger an LLM to generate the desired target prediction (Wallace et al., EMNLP 2019; Pryzant et al., EMNLP 2023)\n- -Jailbreaking LLMs\n- -Prompt Generation: Treat finding of prompts as text generation task, and training LLMs to generate templates (Gao et al., ACL, 2021)", "prev_chunk_id": "ALecture10_Prompting__0025", "next_chunk_id": "ALecture10_Prompting__0027"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0027", "chunk_index": 27, "page_numbers": [30], "text": "- -Also called soft prompts\n- -Relax the constraint that the embeddings of template words be the embeddings of natural language (e.g., English) words.\n- -Remove the restriction that the template is parameterized by the pretrained LM's parameters.\n- -Templates have their own parameters that can be tuned using training data.\n- -Prefix Tuning (Li and Liang, 2021)", "prev_chunk_id": "ALecture10_Prompting__0026", "next_chunk_id": "ALecture10_Prompting__0028"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0028", "chunk_index": 28, "page_numbers": [31], "text": "- -Focuses on learning multiple prompts for an input, instead of single one.\n- -Prompt Ensembling\n- (a) Prompt Ensembling.\n- -The final answer can then be determined by any ensembling method, such as:\n- -Averaging probabilities\n- -Majority voting", "prev_chunk_id": "ALecture10_Prompting__0027", "next_chunk_id": "ALecture10_Prompting__0029"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0029", "chunk_index": 29, "page_numbers": [32], "text": "- -Major Paradigms in NLP\n- -Prompting\n- -In-context Learning\n- -Prompting Examples", "prev_chunk_id": "ALecture10_Prompting__0028", "next_chunk_id": "ALecture10_Prompting__0030"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0030", "chunk_index": 30, "page_numbers": [33], "text": "- -Key idea: Learning from analogy\n- -A few examples are included in the context of the prompt to provide demonstrations to the LLM.\n- -Example for user intent classification:", "prev_chunk_id": "ALecture10_Prompting__0029", "next_chunk_id": "ALecture10_Prompting__0031"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0031", "chunk_index": 31, "page_numbers": [34], "text": "- -No training required, but existing training examples can be useful.\n- -It could be easy to hand-craft a few examples as demonstrations for many tasks.", "prev_chunk_id": "ALecture10_Prompting__0030", "next_chunk_id": "ALecture10_Prompting__0032"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0032", "chunk_index": 32, "page_numbers": [34], "text": "- With the addition of the examples, the prompt may get too long. Limiting factor for ICL: the context length of LLM", "prev_chunk_id": "ALecture10_Prompting__0031", "next_chunk_id": "ALecture10_Prompting__0033"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0033", "chunk_index": 33, "page_numbers": [35], "text": "- -Adding examples/demonstrations in the context consistently improves performance:\n175B\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarksWhile zero-shot performance improvessteadilywithmodel size,few-shotperformanceincreasesmorerapidly,demonstrating that largermodels are more proficient at in-context learning.SeeFigure3.8for a more detailed analysis onSuperGLUE,a standard NLP benchmarksuite.\nFigure from Brown et al., 2020", "prev_chunk_id": "ALecture10_Prompting__0032", "next_chunk_id": "ALecture10_Prompting__0034"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0034", "chunk_index": 34, "page_numbers": [36], "text": "- How many examples to put in context?\n- Which examples to put in context?\n- What should be the order of examples?\n- Is it better to use ICL or fine-tuning?\n- ‚Üí Number of Examples\n- ‚Üí Example Selection\n- ‚Üí Example Ordering\n- ‚Üí Methodology Decision", "prev_chunk_id": "ALecture10_Prompting__0033", "next_chunk_id": "ALecture10_Prompting__0035"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0035", "chunk_index": 35, "page_numbers": [37], "text": "- -Earlier work, the gains can diminish after a few examples (Liang et al, TMLR, 2023)\nFigure 32: Number of in-context examples. For each model, we set the maximum number of in-context We plot performance as a function of the average number of in-context examples actually used.", "prev_chunk_id": "ALecture10_Prompting__0034", "next_chunk_id": "ALecture10_Prompting__0036"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0036", "chunk_index": 36, "page_numbers": [38], "text": "- -More recent work, with LLMs that have longer context sizes: Many-shot ICL consistently outperforms few-shot ICL (Agarwal et al., NeurIPS, 2024).\nFigure 1| Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH50O test set, and sentiment analysis results are reported with semantically-unrelated labels. See $2, S3, and $4 for more details.", "prev_chunk_id": "ALecture10_Prompting__0035", "next_chunk_id": "ALecture10_Prompting__0037"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0037", "chunk_index": 37, "page_numbers": [39], "text": "- -Bertsch et al., NAACL, 2025\n- -Random\n- -BM24 or BERTscore to select similar examples to put in context\nFigure 2: Comparing three selection methods- random selection, BM25, and BERTScore-Recall (BSR) on two representative datasets. At smaller numbers of demonstrations in-context, BM25 and BSR have differing performance, and the best retriever is dataset-specific; at larger demonstration counts, the twobecome indistinguishable.Both generallyoutperformrandomselection.\nLlama-2 with 4k (Touvron et al., 2023), 32k (TogetherAI, 2023), and 80k (Fu et al., 2024) context windows, Mistral-7bv0.2 (Jiang et al., 2023), Qwen 2.5-7B (Team, 2024).\nRetrieving similar examples is better than randomly putting examples in context (especially when the context size is limited).", "prev_chunk_id": "ALecture10_Prompting__0036", "next_chunk_id": "ALecture10_Prompting__0038"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0038", "chunk_index": 38, "page_numbers": [40], "text": "- -Zhao et al., PMLR, 2021\n- -LLMs can be very sensitive to the ordering of the examples in context.\n- -Recency bias\nFigure 4.Majority label and recencybiases cause GPT-3 tobecome biased towards certain answers and help to explain the highvariance across differentexamples and orderings.Above,we use 4-shotSST-2with prompts thathave differentclassbalances andpermutations, e.g., [P P N N] indicates two positive training examples and then two negative.We plot how often GPT-3 2.7B predicts Positive on the balancedvalidation set.When the prompt is unbalanced,the predictions areunbalanced (majority label bias).In addition,balanced prompts that have one class repeated near the end, e.g., end with two Negative examples, will have a bias towards that class (recency bias).", "prev_chunk_id": "ALecture10_Prompting__0037", "next_chunk_id": "ALecture10_Prompting__0039"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0039", "chunk_index": 39, "page_numbers": [41], "text": "Figure 5: By contrast, sorting examples by label has an increasingly negative impact on performance inlonger context regimes.Results onLlama2-32kwith Clinic150.", "prev_chunk_id": "ALecture10_Prompting__0038", "next_chunk_id": "ALecture10_Prompting__0040"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0040", "chunk_index": 40, "page_numbers": [42], "text": "- -Ordering examples based on their similarity to the input, where the most similar one comes right before the input (Liu et al., ACL DeeLIO, 2022).\n- -Ordering examples according to a curriculum, ranking examples from simple to more complex (Liu et al., Preprint, 2024)\n- -Algorithmic selections, such as DEmO, where ordering is made to achieve label fairness and influential prediction for each test instance (Guo et al., ACL Findings, 2024)", "prev_chunk_id": "ALecture10_Prompting__0039", "next_chunk_id": "ALecture10_Prompting__0041"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0041", "chunk_index": 41, "page_numbers": [43], "text": "Figure 3: Comparing BM25 retrieval ICL, random selection ICL, and two types of finetuning on two representative datasets. Finetuning sometimes, but not always, exceeds ICL at high numbers of demonstrations. Note that, while retrieval ICL uses the listed number of examples in context, it assumes access to the larger test set to draw examples from (Perez et al., 2021). See Appendix C for results on other datasets.\nWhen training data is scarce, ICL is a better choice than fine-tuning.", "prev_chunk_id": "ALecture10_Prompting__0040", "next_chunk_id": "ALecture10_Prompting__0042"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0042", "chunk_index": 42, "page_numbers": [44], "text": "- -Major Paradigms in NLP\n- -Prompting\n- -In-context Learning\n- -Prompting Examples", "prev_chunk_id": "ALecture10_Prompting__0041", "next_chunk_id": "ALecture10_Prompting__0043"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0043", "chunk_index": 43, "page_numbers": [45], "text": "- -Thinking fast and slow (D. Kahneman, 2011):\n- o Human brain has two systems\n- o System 1 is fast, instinctive, and emotional\n- Driving a car on an empty road\n- Reading billboard texts\n- o System 2 is slow and calculating\n- Solving complex problems\n- -LLMs were shown to successfully perform system 1 tasks, but not system 2.\n- -Can LMs generate a coherent chain of thought before arriving at the answer, in a similar way human to reasoning?", "prev_chunk_id": "ALecture10_Prompting__0042", "next_chunk_id": "ALecture10_Prompting__0044"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0044", "chunk_index": 44, "page_numbers": [47], "text": "Models: LaMDA with 422M, 2B, 8B, 68B, 137B parameters.\nTable 1. Examples of correct and incorrect chains of thought produced by the 137B model on the GSM8K dataset. Even more examples are given in Table 5-Table 8 in the Appendix.\nQUEsTIoN: Ducks need to eat 3.5 pounds of insects each week to survive. If there is a fock of ten ducks, how many pounds of insects do they need per day?\nMoDEL ANSwER (CORRECT): Ducks need 3.5 pounds of insects each week.If there is a flock of 10 ducks,then they need 3.5 x 10 = 35 pounds of insects each week. If they need 35 pounds of insects each week, then they need 35 / 7 = 5 pounds of insects each day. The answer is 5. v\nQUEsTIoN: It takes Matthew 3 minutes to dig a small hole for shrubs and 10 minutes to dig a large hole for trees. How many hours will it take him to dig 30 small holes and 15 large holes?\nMoDEL ANSWER (CORRECT): It takes Matthew 3 minutes to dig a small hole and 10 minutes to dig a large hole. So it takes Matthew 3 x 30 = 90 minutes to dig 30 small holes. It takes Matthew 10 x 15 = 150 minutes to dig 15 large holes. So it takes Matthew 90 + 150 = 240 minutes to dig 30 small holes and 15 large holes. 240 minutes is 4 hours. The answer is 4 hours.v\nQUEsTIoN: I have 10 liters of orange drink that are two-thirds water andIwishtoadd it to15liters of pineapple drink that is three-fifths water. But as I pour it, I spill one liter of the orange drink. How much water is in the remaining 24 liters?\nMoDEL ANSWER (INCORRECT): Ihave 10 liters of orange drink that are two-thirds water. I have 15 liters of pineapple drink that is three-fifths water. So I have 10 + 15 = 25 liters of drink. I spill one liter of orange drink. So I have 24 liters of drink. 25 - 24 is 1. The answer is 1. x", "prev_chunk_id": "ALecture10_Prompting__0043", "next_chunk_id": "ALecture10_Prompting__0045"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0045", "chunk_index": 45, "page_numbers": [48], "text": "- -Last Letter Concatenation\n- o Elon Musk ‚Üí nk\n- -Reverse List\n- o \"a\", \"b\" ‚Üí \"b\", \"a\"\n- -Coin Flip\n- o 'A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?' ‚Üí 'no'", "prev_chunk_id": "ALecture10_Prompting__0044", "next_chunk_id": "ALecture10_Prompting__0046"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0046", "chunk_index": 46, "page_numbers": [49], "text": "- Q: Sammy wanted to go to where the people were.Where might he go?\n(a) race track\nOptions:\n(d) apartment\n(b) populated areas\n(c) desert\n(e) roadblock\n- A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b).", "prev_chunk_id": "ALecture10_Prompting__0045", "next_chunk_id": "ALecture10_Prompting__0047"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0047", "chunk_index": 47, "page_numbers": [49], "text": "- Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n- A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months. So a llama could not give birth twice during the War in Vietnam. So the answer is no.", "prev_chunk_id": "ALecture10_Prompting__0046", "next_chunk_id": "ALecture10_Prompting__0048"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0048", "chunk_index": 48, "page_numbers": [49], "text": "- Q:The concert was scheduled tobe on 06/01/1943,but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\n- A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943.", "prev_chunk_id": "ALecture10_Prompting__0047", "next_chunk_id": "ALecture10_Prompting__0049"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0049", "chunk_index": 49, "page_numbers": [49], "text": "- Q: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\n- A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the answer is no.", "prev_chunk_id": "ALecture10_Prompting__0048", "next_chunk_id": "ALecture10_Prompting__0050"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0050", "chunk_index": 50, "page_numbers": [50], "text": "- -Strong instruction following and reasoning ability in large language models.\n- -Reasoning improves model accuracy significantly in almost all cases tested.\n- -More on CoT and reasoning in a few weeks!", "prev_chunk_id": "ALecture10_Prompting__0049", "next_chunk_id": "ALecture10_Prompting__0051"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0051", "chunk_index": 51, "page_numbers": [51], "text": "- -Madaan et al., NeurIPS 2023.\n- -An LLM generates an initial output.\n- -Then, the same LLM provides feedback for its output.\n- -Then, the same LLM uses the feedback to refine itself, iteratively.\nsame model M togetfeedback(‚ë†).Thefeedbackis passed backtoM,which refines thepreviously generated output (?). Steps (‚ë†) and (‚ë°) iterate until a stopping condition is met. SELF-REFINE is instantiatedwithalanguagemodel such as GPT-3.5anddoesnotinvolvehuman assistance.", "prev_chunk_id": "ALecture10_Prompting__0050", "next_chunk_id": "ALecture10_Prompting__0052"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0052", "chunk_index": 52, "page_numbers": [52], "text": "- -Midterm Exam 1", "prev_chunk_id": "ALecture10_Prompting__0051", "next_chunk_id": "ALecture10_Prompting__0053"}
{"lecture_id": "ALecture10_Prompting", "source_pdf": "data/anlp/ALecture10_Prompting.pdf", "chunk_id": "ALecture10_Prompting__0053", "chunk_index": 53, "page_numbers": [52], "text": "- -Instruction Tuning\n- -Overview\n- -Instruction Tuning Datasets\n- -Evaluation\n- -Other Recent Instruction-Tuning Related Work", "prev_chunk_id": "ALecture10_Prompting__0052", "next_chunk_id": null}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture11_InstructionTuning__0001"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0001", "chunk_index": 1, "page_numbers": [2], "text": "Highest grade: 102/100 (2 points bonus)\nMean: 88.6\nMedian: 90\nMidterm 1 is 15% of the overall grade.", "prev_chunk_id": "ALecture11_InstructionTuning__0000", "next_chunk_id": "ALecture11_InstructionTuning__0002"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Overview\n- -Instruction Tuning Datasets\n- -Evaluation\n- -Other Recent Instruction-Tuning Related Work", "prev_chunk_id": "ALecture11_InstructionTuning__0001", "next_chunk_id": "ALecture11_InstructionTuning__0003"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Zhang et al, Instruction Tuning for Large Language Models: A Survey, 2023 (updated till recently).\n- -https://github.com/RenzeLou/awesome-instruction-learning", "prev_chunk_id": "ALecture11_InstructionTuning__0002", "next_chunk_id": "ALecture11_InstructionTuning__0004"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0004", "chunk_index": 4, "page_numbers": [5], "text": "- -Mismatch between the training objective and the user's objective: Language modeling pre-training objective of predicting the next token does not align well with actual LLM use cases.\n- o People are not only using LLMs for text completion.\n- o They want the models to follow their instructions accurately and safely.\nLanguage Modeling Objective\nPredicting the next token in a document\nHelpful LMs\nFollow the user's instructions helpfully and safely", "prev_chunk_id": "ALecture11_InstructionTuning__0003", "next_chunk_id": "ALecture11_InstructionTuning__0005"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0005", "chunk_index": 5, "page_numbers": [6, 7], "text": "- -Before instruction tuning\n- -Example output of GPT-3, from https://openai.com/index/instructionfollowing/\n- -Also called supervised fine-tuning or SFT\n- -Aims to steer models towards accurately (and safely) responding to real user requests.\n- -Further training LLMs using instruction and output pairs:\n- o Instruction: human instruction for the models\n- o Output: desired output that follows the instruction", "prev_chunk_id": "ALecture11_InstructionTuning__0004", "next_chunk_id": "ALecture11_InstructionTuning__0006"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0006", "chunk_index": 6, "page_numbers": [8], "text": "Write questions about the highlights of a story.", "prev_chunk_id": "ALecture11_InstructionTuning__0005", "next_chunk_id": "ALecture11_InstructionTuning__0007"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0007", "chunk_index": 7, "page_numbers": [8], "text": "2. Write questions about the highlights\n1. Read the highlights", "prev_chunk_id": "ALecture11_InstructionTuning__0006", "next_chunk_id": "ALecture11_InstructionTuning__0008"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0008", "chunk_index": 8, "page_numbers": [8], "text": "- ¬∑Sarah Palin from Alaskameetswith McCain\n- ¬∑Fareed Zakaria says John McCain did not put country first with his choice\n- ¬∑Zakaria: This is\"hell of a time\" for Palin to start thinking aboutnational,global issues", "prev_chunk_id": "ALecture11_InstructionTuning__0007", "next_chunk_id": "ALecture11_InstructionTuning__0009"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0009", "chunk_index": 9, "page_numbers": [8], "text": "The questions can refer directly to the highlights, for example:\n- ¬∑Where isPalin from?\n- ¬∑What did Fareed say about John McCain's choice?\n- ¬∑Who is thinking about global issues?\nQuestions must always be related to the highlights but their answers don't have to be in the highlights. You can assume thatthehighlightssummarizea documentwhichcananswer other questionsfor example:\n- ¬∑What was the meeting about?\n- ¬∑What was McCain's choice?\n- ¬∑What issues is Palin thinking about?\n- -Can language models understand instructions?\n- -Introduced an instruction understanding task (IUT).\n- -A model is provided with an input I x that describes in natural language a desired output o .\n- -x is highlighted in green in the example figure.\n- -I x is comprised of a template I called instruction, which is instantiated with a resource x to form I x .", "prev_chunk_id": "ALecture11_InstructionTuning__0008", "next_chunk_id": "ALecture11_InstructionTuning__0010"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0010", "chunk_index": 10, "page_numbers": [9], "text": "- Do not re-use the same or very similar questions.\n- Questions should be written to have short answers.\n- s ssb a m, o mu, m jou oo answers are not short. \"How far/long/many/much\" are okay.", "prev_chunk_id": "ALecture11_InstructionTuning__0009", "next_chunk_id": "ALecture11_InstructionTuning__0011"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0011", "chunk_index": 11, "page_numbers": [9], "text": "- Math geeks and others celebrate Pi Day every March 14\n- Pi, or roughly 3.14, is the ratio of circumference to diameter of a circle\n- The Pi Day holiday idea started at the Exploratorium museum in San Francisco\n- ¬∑Albert Einstein was also born on March 14", "prev_chunk_id": "ALecture11_InstructionTuning__0010", "next_chunk_id": "ALecture11_InstructionTuning__0012"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0012", "chunk_index": 12, "page_numbers": [9], "text": "1. When is Pi Day celebrated?\n2. What is the value of Pi up to the second decimal digit? Another thing is that Pi is important in mathematics.\n3. -Can language models understand instructions ?\n4. -Introduced an instruction understanding task (IUT).\n5. -A model is provided with an input I x that describes in natural language a desired output o .\n6. -x is highlighted in green in the example figure.\n7. -I x is comprised of a template I called instruction, which is instantiated with a resource x to form I x .", "prev_chunk_id": "ALecture11_InstructionTuning__0011", "next_chunk_id": "ALecture11_InstructionTuning__0013"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0013", "chunk_index": 13, "page_numbers": [10], "text": "- -Making language models bigger does not inherently make them better at following a user's intention.\n- -Intentions:\n- -Explicit: following user instructions\n- -Implicit: staying truthful, and not being biased, toxic, or otherwise harmful\n- -3H-objective:\n- -Helpful: they should help user solve their task\n- -Honest: they shouldn't fabricate information or mislead the user\n- -Harmless: they should not cause physical, psychological, or social harm to people or the environment\n- -Method: fine-tuning pre-trained LMs with human feedback\n- Goal: align language models with user's intentions", "prev_chunk_id": "ALecture11_InstructionTuning__0012", "next_chunk_id": "ALecture11_InstructionTuning__0014"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0014", "chunk_index": 14, "page_numbers": [11], "text": "Step 3\nOptimize a policy against the reward model using\nreinforcementlearning-\nA new prompt is sampled from\nthe dataset.\nThe policy generates\nan output.\nThe reward model calculates a\nreward for the output.\nThe reward is used to update the policy using PPO.", "prev_chunk_id": "ALecture11_InstructionTuning__0013", "next_chunk_id": "ALecture11_InstructionTuning__0015"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0015", "chunk_index": 15, "page_numbers": [12], "text": "- -Fine-tune GPT-3 using supervised learning\n- -Data collection:\n- -Collecting Prompts:\n- -Contractor labelers (40 trained experts)\n- -Prompts submitted to the InstructGPT API by customers\n- -Asking labelers to write high-quality responses\ntrain, SFT Data.source = labeler. train, SFT Data.size = 11,295. train, SFT Data.source = customer. train, SFT Data.size = 1,430. valid, SFT Data.source = labeler. valid, SFT Data.size = 1,550. valid, SFT Data.source = customer. valid, SFT Data.size = 103", "prev_chunk_id": "ALecture11_InstructionTuning__0014", "next_chunk_id": "ALecture11_InstructionTuning__0016"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0016", "chunk_index": 16, "page_numbers": [13], "text": "- -Generation: Write a short story where a brown bear to the beach, makes friends with a seal, and then return home.\n- -Brainstorming: List five ideas for how to regain enthusiasm for my career\n- -Classification:  {java code} What language is the code above written in?\n- -Extract:  Extract all place names from the article below: {news article}\n- -Open QA: Who built the statue of liberty?\nGeneration, (%) = 45.6%. Open QA, (%) = 12.4%. Brainstorming, (%) = 11.2%. Chat, (%) = 8.4%. Rewrite, (%) = 6.6%. Summarization, (%) = 4.2%. Classification, (%) = 3.5%. Other, (%) = 3.5%. Closed QA, (%) = 2.6%. Extract, (%) = 1.9%", "prev_chunk_id": "ALecture11_InstructionTuning__0015", "next_chunk_id": "ALecture11_InstructionTuning__0017"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0017", "chunk_index": 17, "page_numbers": [14], "text": "- -RM (GPT-6B with an additional layer):\n- -Output: a scalar reward r( x,y )\n- -Input: a prompt x and a response y\n- -Loss function:\n- -yw, y l : two responses of the prompt x , where yw is the preferred one\n- -K : the total number of responses for the prompt\n<!-- formula-not-decoded -->\ntrain, RM Data.source = labeler. train, RM Data.size = 6,623. train, RM Data.source = customer. train, RM Data.size = 26,584. valid, RM Data.source = labeler. valid, RM Data.size = 3,488. valid, RM Data.source = customer. valid, RM Data.size = 14,399\nStep 2\nCollect comparison data, and trainareward model.\nK ranges from 4 to 9", "prev_chunk_id": "ALecture11_InstructionTuning__0016", "next_chunk_id": "ALecture11_InstructionTuning__0018"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0018", "chunk_index": 18, "page_numbers": [15], "text": "- -Fine-tune the SFT model using reinforcement learning\n- -Objective function:\n- -Reward for y obtained by the RM\n- -KL penalty to make sure the generated output is not too far away from the output of SFT\nMore on alignment and reinforcement learning by Ishika in a few weeks!\ntrain, PPO Data.source = customer. train, PPO Data.size = 31,144. valid, PPO Data.source = customer. valid, PPO Data.size = 16,185", "prev_chunk_id": "ALecture11_InstructionTuning__0017", "next_chunk_id": "ALecture11_InstructionTuning__0019"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0019", "chunk_index": 19, "page_numbers": [16], "text": "- -Evaluations on API distribution:\n- -Dataset:\n- -prompts submitted to InstructGPT API that haven't been included in training\n- -Prompts submitted to GPT-3 API\n- -Metrics:\n- -human preferred rate (winrate) compared with 175B SFT\n- -1-7 Likert score of the overall quality of the outputs, as well as their binary metadata\n- -Evaluations on public NLP datasets:\n- -Measuring LM safety:\n- -TruthfulQA, RealToxicityPrompts , ‚Ä¶\n- -Measuring zero-shot performance on traditional NLP tasks, such as question answering, reading comprehension, and summarization\n- -Hellaswag , SquadV2, ‚Ä¶", "prev_chunk_id": "ALecture11_InstructionTuning__0018", "next_chunk_id": "ALecture11_InstructionTuning__0020"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0020", "chunk_index": 20, "page_numbers": [17], "text": "- -Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.", "prev_chunk_id": "ALecture11_InstructionTuning__0019", "next_chunk_id": "ALecture11_InstructionTuning__0021"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0021", "chunk_index": 21, "page_numbers": [18], "text": "What is the purpose of thelist C in the code below?\n```\ndef binomial_coefficient(n,r): C=[o for i in range(r + 1)]; C[0]=1; for i in range(1Ôºån + 1): j=minÔºàiÔºår); whilej>0: C[j] +=C[j-1]Ôºõ j -= 1; return C[r]\n```", "prev_chunk_id": "ALecture11_InstructionTuning__0020", "next_chunk_id": "ALecture11_InstructionTuning__0022"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0022", "chunk_index": 22, "page_numbers": [18], "text": "- A.to store the value of C[0]\n- B.to store the value of C[1]\n- C.to store the value of C[i]\n- D. to store the value of C[i - 1]", "prev_chunk_id": "ALecture11_InstructionTuning__0021", "next_chunk_id": "ALecture11_InstructionTuning__0023"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0023", "chunk_index": 23, "page_numbers": [18], "text": "Thelist Cin this code is used tostore thevalues of the binomial coefficient as the function iterates through thevalues of n and r.It isused to calculate the value of thebinomial coefficient for a given value of n and r,and stores the result in the final return value of the function.", "prev_chunk_id": "ALecture11_InstructionTuning__0022", "next_chunk_id": "ALecture11_InstructionTuning__0024"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0024", "chunk_index": 24, "page_numbers": [19], "text": "- -It is expensive and labor-intensive to collect such data\n- -Often times you need expert responses or detailed research to form the answer, e.g.,\n' I have taken 80 credits from CS and 12 credits from Linguistics. Can I do a minor in Linguistics at UIUC? '\n- -Tasks that require creative generation do not have one right answer, e.g.,\n' Write a poem to celebrate the birthday of my sister. She likes elephants. '", "prev_chunk_id": "ALecture11_InstructionTuning__0023", "next_chunk_id": "ALecture11_InstructionTuning__0025"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0025", "chunk_index": 25, "page_numbers": [20], "text": "Instruction Tuning\n- -Overview\n- -Instruction Tuning Datasets\n- -Evaluation\n- -Other Recent Instruction-Tuning Related Work", "prev_chunk_id": "ALecture11_InstructionTuning__0024", "next_chunk_id": "ALecture11_InstructionTuning__0026"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0026", "chunk_index": 26, "page_numbers": [21], "text": "- -Manually created datasets\n- -Synthesizing data using strong LLMs\n- -Synthesizing data using the same model", "prev_chunk_id": "ALecture11_InstructionTuning__0025", "next_chunk_id": "ALecture11_InstructionTuning__0027"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0027", "chunk_index": 27, "page_numbers": [22], "text": "- -(Mishra et al, ACL, 2022)\n- -Cross-task generalization benchmark and dataset.\n- -61 distinct NLP tasks, 193 K instances\n- -Instruction tuning models with existing datasets can help with generalization towards unseen tasks.", "prev_chunk_id": "ALecture11_InstructionTuning__0026", "next_chunk_id": "ALecture11_InstructionTuning__0028"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0028", "chunk_index": 28, "page_numbers": [24], "text": "- -(Wang et al, EMNLP 2022)\n- -Extensive cross-task generalization.\n- o Multi-lingual instruction collection (55 languages)\n- o Work of a much larger consortium, following Natural Instructions.\n- o 1,616 diverse NLP tasks, 5M task instances, 76 distinct task types", "prev_chunk_id": "ALecture11_InstructionTuning__0027", "next_chunk_id": "ALecture11_InstructionTuning__0029"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0029", "chunk_index": 29, "page_numbers": [26], "text": "Figure 3: Human evaluation vs. ROUGE-L for several methods (s6.2). The trends of these two metrics are highly correlated with a Pearson coefficient of 0.998.\n- -Instruction-tuning enables stronger generalization to unseen tasks\n- -There is also sizable gap for improvement in comparison to supervised training.", "prev_chunk_id": "ALecture11_InstructionTuning__0028", "next_chunk_id": "ALecture11_InstructionTuning__0030"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0030", "chunk_index": 30, "page_numbers": [27], "text": "- -(Gupta et al., EMNLP 2022)\n- -Specialized to dialogue related tasks that require conversational data.\n- -Instruction tuning for 48 dialogue tasks, 59 datasets\n- -Introduces novel meta-tasks (e.g. select an instruction that matches with an input-output pair) to encourage models to adhere to the instructions.", "prev_chunk_id": "ALecture11_InstructionTuning__0029", "next_chunk_id": "ALecture11_InstructionTuning__0031"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0031", "chunk_index": 31, "page_numbers": [28], "text": "Green represents classification and orange represents generation tasks.", "prev_chunk_id": "ALecture11_InstructionTuning__0030", "next_chunk_id": "ALecture11_InstructionTuning__0032"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0032", "chunk_index": 32, "page_numbers": [30], "text": "- Training on more seen tasks improves generalization on unseen tasks .", "prev_chunk_id": "ALecture11_InstructionTuning__0031", "next_chunk_id": "ALecture11_InstructionTuning__0033"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0033", "chunk_index": 33, "page_numbers": [31], "text": "- -(Zhou et al., Neurips, 2023)\n- -Measure the relative importance of pre-training and instruction finetuning.\n- -LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling.\n- -Even a small amount of carefully selected high-quality instruction data can significantly improve model performance through instruction tuning.", "prev_chunk_id": "ALecture11_InstructionTuning__0032", "next_chunk_id": "ALecture11_InstructionTuning__0034"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0034", "chunk_index": 34, "page_numbers": [32], "text": "- -Annotation quality can be high.", "prev_chunk_id": "ALecture11_InstructionTuning__0033", "next_chunk_id": "ALecture11_InstructionTuning__0035"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0035", "chunk_index": 35, "page_numbers": [32], "text": "- -Diversity of the examples can be limited.\n- -Tasks may be more focused to NLP than real user instructions.", "prev_chunk_id": "ALecture11_InstructionTuning__0034", "next_chunk_id": "ALecture11_InstructionTuning__0036"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0036", "chunk_index": 36, "page_numbers": [33], "text": "- -Manually created datasets\n- -Synthesizing data using strong LLMs\n- -Synthesizing data using the same model", "prev_chunk_id": "ALecture11_InstructionTuning__0035", "next_chunk_id": "ALecture11_InstructionTuning__0037"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0037", "chunk_index": 37, "page_numbers": [34], "text": "- -(Honovich et al., ACL, 2023)\n- -Collecting data with instructions and accurate responses is challenging.\n- -Previous work:\n- -Reformulated existing datasets. E.g., used prompt engineering to reformulate the datasets in instruction format: instruction -input -output.\n- -The tasks and contents are limited by the diversity of the existing datasets.\n- -Human annotation Collected user-generated prompts and manually annotated their expected outputs.\n- -Requires live LLMs with existing users.\n- -High cost of human labor.", "prev_chunk_id": "ALecture11_InstructionTuning__0036", "next_chunk_id": "ALecture11_InstructionTuning__0038"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0038", "chunk_index": 38, "page_numbers": [35], "text": "- -Goal: Instruction datasets with diverse tasks and contents, obtained with minimum human annotation.\n- -M : OpenAI's text-davinci-002 (part of GPT3.5 series, trained with SFT on human data).\n- -Approach Overview:\nFigure 2: The core Unnatural Instructions generation pipeline. We use a seed of three in-context demonstrations 1, 2, 3 to create a large dataset of NLP tasks with instructions, inputs and outputs. As a first step, we sample instructions, inputs, and constraints from a language model M. In the next step, we use M to deterministically generate the corresponding outputs. Finally, the data can be used for instruction tuning.", "prev_chunk_id": "ALecture11_InstructionTuning__0037", "next_chunk_id": "ALecture11_InstructionTuning__0039"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0039", "chunk_index": 39, "page_numbers": [36], "text": "- -15 manually annotated examples in the format of instruction -input -constraints.\n- -3 in-context examples as input, 5 different seeds.", "prev_chunk_id": "ALecture11_InstructionTuning__0038", "next_chunk_id": "ALecture11_InstructionTuning__0040"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0040", "chunk_index": 40, "page_numbers": [36], "text": "Instruction:Youaregivenasciencequestion(easy-level) andfour answeroptions(associatedwith\"A\",\"B\",\"C\",\"D\").Your taskis to find the correctanswerbasedonscientificfacts,knowledge,andreasoning.Do notgenerateanythingelseapartfromoneofthefollowingcharacters: 'A','B,C'D'Thereisonlyonecorrectanswerforeachquestion.\nInput:Whichpart of a bicycleBESTmoves ina circle?(A) Seat (B) Frame(C)Footpedal(D)Kickstand\nConstraints:Theoutputshouldbeoneof thefollowing characters:'A, 'B,CD'", "prev_chunk_id": "ALecture11_InstructionTuning__0039", "next_chunk_id": "ALecture11_InstructionTuning__0041"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0041", "chunk_index": 41, "page_numbers": [36], "text": "Instruction:Youaregivenanegativereviewandyourtaskistoconvert ittoapositivereviewbyoneormoremakingminimalchanges.Avoid changingthecontextofthereview.\nInput:westoodthereinshock,becauseweneverexpectedthis. Constraints:None.", "prev_chunk_id": "ALecture11_InstructionTuning__0040", "next_chunk_id": "ALecture11_InstructionTuning__0042"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0042", "chunk_index": 42, "page_numbers": [37, 38], "text": "- -Use LLM generate a large number of training samples, based on the in-context examples.\n- -Nucleus sampling for diversity of samples.\n- -LLM is used for generating instruction outputs as well.\n- -Greedy decoding for accuracy when generating the outputs.", "prev_chunk_id": "ALecture11_InstructionTuning__0041", "next_chunk_id": "ALecture11_InstructionTuning__0043"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0043", "chunk_index": 43, "page_numbers": [39], "text": "- -Not all generated examples are good.\n- -Manual analysis of 200 generated examples showed only 113 (56.5%) are accurate.\n- -Of the 87 incorrect examples, 9 (4.5%) had incomprehensible instructions, 35 (17.5%) had an input that did not match the task description, and 43 (21.5%) had incorrect outputs.\n- -Filtering Strategy\n- -Violation of the format of instruction -input -constraints.\n- -Identical items as the seed examples.\n- -Duplicates.", "prev_chunk_id": "ALecture11_InstructionTuning__0042", "next_chunk_id": "ALecture11_InstructionTuning__0044"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0044", "chunk_index": 44, "page_numbers": [40], "text": "- -Reformulate the instructions through adding {INPUT} placeholder and rephrasing via LLM.\n- -Core dataset: 58K examples, Reformulation: 240K examples\n- -Few-shot learning with in-context examples, similar to the core dataset generation.", "prev_chunk_id": "ALecture11_InstructionTuning__0043", "next_chunk_id": "ALecture11_InstructionTuning__0045"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0045", "chunk_index": 45, "page_numbers": [41], "text": "- -Comparison with Super-Natural Instructions\n- -Similarity of 1000 randomly selected pairs in each dataset", "prev_chunk_id": "ALecture11_InstructionTuning__0044", "next_chunk_id": "ALecture11_InstructionTuning__0046"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0046", "chunk_index": 46, "page_numbers": [42], "text": "- -T5-LM on Super-Natural Instructions: Direct comparison.\n- -T0++, Tk Instruct, FLAN -T5‚Ä¶, which are all instruction tuned T5 models.\nPrior Work, #Examples = . Prior Work, Super-Natural Instructions = . Prior Work, TO: Zero-Shot = . Prior Work, BIG-bench: Hard (Orig/QA) = . Prior Work, LMentry = . T5-LM, #Examples = 0. T5-LM, Super-Natural Instructions = 24.3. T5-LM, TO: Zero-Shot = 40.2. T5-LM, BIG-bench: Hard (Orig/QA) = 0.0/0.7. T5-LM, LMentry = 20.6. ++0L, #Examples = 12,492,800. ++0L, Super-Natural Instructions = 40.3. ++0L, TO: Zero-Shot = NHO. ++0L, BIG-bench: Hard (Orig/QA) = 20.2 / 13.9. ++0L, LMentry = 38.3. Tk-Instruct, #Examples = 75,417. Tk-Instruct, Super-Natural Instructions = 45.6. Tk-Instruct, TO: Zero-Shot = 41.4. Tk-Instruct, BIG-bench: Hard (Orig/QA) = 5.8/11.8. Tk-Instruct, LMentry = 35.7. FLAN-T5, #Examples = 14,336,000. FLAN-T5, Super-Natural Instructions = NHO. FLAN-T5, TO: Zero-Shot = NHO. FLAN-T5, BIG-bench: Hard (Orig/QA) = 39.3 / 40.0. FLAN-T5, LMentry = 52.2. Direct Comparison Baseline, #Examples = . Direct Comparison Baseline, Super-Natural Instructions = . Direct Comparison Baseline, TO: Zero-Shot = . Direct Comparison Baseline, BIG-bench: Hard (Orig/QA) = . Direct Comparison Baseline, LMentry = . T5-LM on Super-Natural Instructions, #Examples = 64,000. T5-LM on Super-Natural Instructions, Super-Natural Instructions = 54.0. T5-LM on Super-Natural Instructions, TO: Zero-Shot = 44.0. T5-LM on Super-Natural Instructions, BIG-bench: Hard (Orig/QA) = 10.2 / 29.7. T5-LM on Super-Natural Instructions, LMentry = 34.6. Our Approach, #Examples = . Our Approach, Super-Natural Instructions = . Our Approach, TO: Zero-Shot = . Our Approach, BIG-bench: Hard (Orig/QA) = . Our Approach, LMentry = . T5-LM on Unnatural Instructions, #Examples = 64,000. T5-LM on Unnatural Instructions, Super-Natural Instructions = 51.9. T5-LM on Unnatural Instructions, TO: Zero-Shot = 45.7. T5-LM on Unnatural Instructions, BIG-bench: Hard (Orig/QA) = 16.0 / 29.5. T5-LM on Unnatural Instructions, LMentry = 42.0. + Instruction Paraphrases, #Examples = 240,670. + Instruction Paraphrases, Super-Natural Instructions = 49.3. + Instruction Paraphrases, TO: Zero-Shot = 49.0. + Instruction Paraphrases, BIG-bench: Hard (Orig/QA) = 28.1 / 29.4. + Instruction Paraphrases, LMentry = 50.7", "prev_chunk_id": "ALecture11_InstructionTuning__0045", "next_chunk_id": "ALecture11_InstructionTuning__0047"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0047", "chunk_index": 47, "page_numbers": [43], "text": "- -Based on OpenAI's pricing as of December 2022, the cost for generating an example is estimated at $0.02 for the core dataset, and $0.01 for the expanded dataset.\n- -Kiela et al. (2021) estimate human annotation cost at $0.50 -$1.00 per example.", "prev_chunk_id": "ALecture11_InstructionTuning__0046", "next_chunk_id": "ALecture11_InstructionTuning__0048"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0048", "chunk_index": 48, "page_numbers": [44], "text": "- -Manually created datasets\n- -Synthesizing data using strong LLMs\n- -Synthesizing data using the same model", "prev_chunk_id": "ALecture11_InstructionTuning__0047", "next_chunk_id": "ALecture11_InstructionTuning__0049"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0049", "chunk_index": 49, "page_numbers": [45], "text": "- -(Wang et al., ACL 2023)\n- -Fine-tuning on human-generated instruction data is challenging:\n- -Collecting instruction data is costly\n- -Limited diversity because most them tend to be popular NLP tasks\n- -Iterative solution\n- -Use vanilla language model (e.g., GPT3) to generate instruction data\n- -Fine-tune the model on generated instruction data, bootstrapping off its own generations.", "prev_chunk_id": "ALecture11_InstructionTuning__0048", "next_chunk_id": "ALecture11_InstructionTuning__0050"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0050", "chunk_index": 50, "page_numbers": [47], "text": "- Recap: a task contains 1 instruction and many instances (input-output pairs)\n- Step 1: generate instructions\n- Method\n- In-context learning\n- Sample tasks from task pool of 175 tasks as few-shot examples\n```\nCome up with a series of tasks: Task 1: {instruction for existing task 1} Task 2: {instruction for existing task 2} Task 3: {instruction for existing task 3} Task 4: {instruction for existing task 4} Task! 5: {instruction for existing task 5} Task 6: {instruction for existing task 6} Task 7: {instruction for existing task 7} Task 8Ôºö {instruction for existing task 8} Task9:\n```\nPrompt used to generate new instructions", "prev_chunk_id": "ALecture11_InstructionTuning__0049", "next_chunk_id": "ALecture11_InstructionTuning__0051"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0051", "chunk_index": 51, "page_numbers": [48], "text": "- The next step is to generate input-output pairs for a task", "prev_chunk_id": "ALecture11_InstructionTuning__0050", "next_chunk_id": "ALecture11_InstructionTuning__0052"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0052", "chunk_index": 52, "page_numbers": [48], "text": "Instruction : Give me a quote from a famous person on this topic.\n- Again, in-context learning\n- However, if the task is a classification task,\nthe generated input can be biased toward one label (i.e., only grammatical examples).\nStep 3: Instance Generation\n- -> output-first can be a solution.\n- Solution\n- Identify classification tasks\n- For a classification task, generate output (class label) first\nInstruction : Find out if the given text is in favor of or against abortion.\nClass Label: Pro-abortion\nInput: Text: I believe that women should have the right to choose whether or not they want to have an abortion.\nInstruction : Give me a quote from a famous person on this topic.\nInput: Topic: The importance of being honest.\nOutput: \"Honesty is the first chapter in the book of wisdom.\" - Thomas Jefferson\nStep 2: Classification Task Identification\nYes\nOutput-first\nNo\nInput-first", "prev_chunk_id": "ALecture11_InstructionTuning__0051", "next_chunk_id": "ALecture11_InstructionTuning__0053"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0053", "chunk_index": 53, "page_numbers": [49], "text": "```\ne    5     g Task: Given my personality and the job, tell me if I would be suitable. Is it classification? Yes Task: Give me an example of a time when you had to use your sense of humor. Is it classification?  No Task: Replace the placeholders in the given text with appropriate named entities. Is it classification? No Task: Fact checking - tell me if the statement is true, false,or unknown, based on your knowledge and common sense. Is it classification?  Yes Task: Return the SSN number for the person. Is it classification?  No Task: To make the pairs have the same analogy, write the fourth word. Is it classification? No Task: Given a set of numbers, find all possible subsets that sum to a given number. Is it classification?  No Task:{instruction for the target task}\n```", "prev_chunk_id": "ALecture11_InstructionTuning__0052", "next_chunk_id": "ALecture11_InstructionTuning__0054"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0054", "chunk_index": 54, "page_numbers": [50], "text": "Step 4: Filtering\nTask Pool\nFilter instructions and instances to improve quality/diversity of the data: Instruction:\n- Too long/short\n- Similar to existing instructions (ROUGE-L > 0.7) (ROUGE-L is a longest common subsequence-based text similarity metric)\n- Keywords: image, picture, graph Instance:\n- Duplicates\n- Same input but different output\n- Output is a repetition of the input", "prev_chunk_id": "ALecture11_InstructionTuning__0053", "next_chunk_id": "ALecture11_InstructionTuning__0055"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0055", "chunk_index": 55, "page_numbers": [51], "text": "```\nInstruction: Given an address and cityÔºå come up with the zipcode. Input: Address: 123 Main Street, City: San Francisco Output: 94105\n```\n```\nInstruction: I am looking for a job and I need to fill out an application form. Can you please help me complete it? Input: Application Form: Name: Age: Sex: Phone Number: Email Address: Education: Output: Name:JohnDoe Age:25 Sex:Male Phone Number: ..\n```\n```\nInstruction: How to write a code for converting degrees fahrenheit to celsius. Input: Null Output: def convert_fahrenheit_to_celsius(fahr): celsius = (fahr - 32Ôºâ * 5 Ôºè 9 </> return celsius\n```\n```\nInstruction: Write a letter from the perspective ofacat. Input: Null Output: Dear [Owner]Ôºå I am writing to you today because I have a problem. You see, I've been living with you for the past [Number of Years] and I love it here. The food is greatÔºå the bed is comfyÔºå and there are plenty of toys to play with. But lately, I've noticedthat you're not as affectionatetowards me as you used to be. In factÔºå sometimes when I try to cuddle up next to you on the\n```\n```\ncouch, you push me away ...\n```", "prev_chunk_id": "ALecture11_InstructionTuning__0054", "next_chunk_id": "ALecture11_InstructionTuning__0056"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0056", "chunk_index": 56, "page_numbers": [52], "text": "# of instructions - # of classification instructions - # of non-classification instructions # of instances - # of instances with empty input ave. instruction length (in words) ave. non-empty input length (in words) ave. output length (in words),  = 52,445. ,  = 11,584. ,  = 40,861. ,  = 82,439. ,  = 35,878. ,  = 15.9. ,  = 12.7. ,  = 18.9\nTable 1: Statistics of the generated data by applying SELF-INSTRUCT tO GPT3.", "prev_chunk_id": "ALecture11_InstructionTuning__0055", "next_chunk_id": "ALecture11_InstructionTuning__0057"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0057", "chunk_index": 57, "page_numbers": [53], "text": "- -Uses Berkeley Neural Parser to parse instruction and extracts verbs closest to the root and its direct object.\n- -20 most common verbs in the portion of the data where such extraction was possible.\nFigure 3: The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the generated instructions. Despite their diversity, the instructions shown here only account for 14% of all the generated instructions because many instructions (e.g., \"Classify whether the user is satisfied with the service.') do not contain such a verb-noun structure.\nFigure 4: Distribution of the ROUGE-L scores between generated instructions and their most similar seed instructions.\nFigure 5: Length distribution of the generated instructions, non-empty inputs, and outputs.", "prev_chunk_id": "ALecture11_InstructionTuning__0056", "next_chunk_id": "ALecture11_InstructionTuning__0058"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0058", "chunk_index": 58, "page_numbers": [54], "text": "- -Manually examined 200 instructions with 1 random instance per instruction.\n- -Error rate relatively high (46%)\n- -However, most of them are in correct format or partially right, which can still guide the model to follow instructions\n- -Paper doesn't have a detailed task breakdown for this analysis.\nTable 2: Data quality review for the instruction, input, and output of the generated data. See Table 10 and Table 11 for representative valid and invalid examples.\nDoes the instruction describe a valid task?, Yes % = 92%. Is the input appropriate for the instruction?, Yes % = 79%. Is the output a correct and acceptable response to the instruction and input?, Yes % = 58%. All fields are valid, Yes % = 54%", "prev_chunk_id": "ALecture11_InstructionTuning__0057", "next_chunk_id": "ALecture11_InstructionTuning__0059"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0059", "chunk_index": 59, "page_numbers": [55], "text": "- -Fine-tuning details: GPT3 via OpenAI's fine -tuning API, 2 epochs\n- -Test Set:\n- -SUPERNI test split. 119 tasks, each with 100 instances in each task, zero-shot generalization\n- -Observations\n- ‚ë† SELF-INST boosts GPT3 performance by a large margin\n- ‚ë° Nearly matches InstructGPT-001\n- ‚ë¢ Brings additional gains when combined with SUPERNI training set\n- -SUPERNI training and test sets have similar instruction style and formatting\nVanilla LMs,  = . Vanilla LMs, # Params ROUGE-L = . T5-LM,  = 11B. T5-LM, # Params ROUGE-L = 25.7. GPT3,  = 175B. GPT3, # Params ROUGE-L = 6.8. Instruction-tuned w/o SUPERNI,  = . Instruction-tuned w/o SUPERNI, # Params ROUGE-L = . TO,  = 11B. TO, # Params ROUGE-L = 33.1. GPT3 + T0 Training,  = 175B. GPT3 + T0 Training, # Params ROUGE-L = 37.9. GPT3SELF-INST (Ours),  = 175B. GPT3SELF-INST (Ours), # Params ROUGE-L = 39.9. InstructGPT001,  = 175B. InstructGPT001, # Params ROUGE-L = 40.8. Instruction-tuned w/ SuPERNI,  = . Instruction-tuned w/ SuPERNI, # Params ROUGE-L = . Tk-INSTRUCT,  = 11B. Tk-INSTRUCT, # Params ROUGE-L = 46.0. GPT3 + SUPERNI Training >,  = 175B. GPT3 + SUPERNI Training >, # Params ROUGE-L = 49.5. GPT3sELF-INsT + SUPERNI Training (Ours),  = 175B. GPT3sELF-INsT + SUPERNI Training (Ours), # Params ROUGE-L = 51.6", "prev_chunk_id": "ALecture11_InstructionTuning__0058", "next_chunk_id": "ALecture11_InstructionTuning__0060"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0060", "chunk_index": 60, "page_numbers": [56], "text": "- -Fine-tuning details: GPT3 via OpenAI's fine -tuning API, 2 epochs\n- -Test Set:\n- -A small dataset for human evaluation. 252 tasks, each with 1 instance.\n- -Generalization to user-oriented instructions on novel tasks (e.g., email writing, social media, productivity tools, entertainment, programming)\n- Human evaluation\n- A: valid and satisfying\n- B: acceptable but has minor errors\n- C: responds to the instruction, but has significant errors\n- D: irrelevant or completely invalid", "prev_chunk_id": "ALecture11_InstructionTuning__0059", "next_chunk_id": "ALecture11_InstructionTuning__0061"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0061", "chunk_index": 61, "page_numbers": [57], "text": "- -Performance improves as the data size grows\n- -The improvement almost plateaus after 16K\n- -When evaluating on SUPER-NI, it plateaus at around hundreds of instructions\n- -Possible reason: Generated data is distinct from typical NLP tasks in SUPER-NI", "prev_chunk_id": "ALecture11_InstructionTuning__0060", "next_chunk_id": "ALecture11_InstructionTuning__0062"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0062", "chunk_index": 62, "page_numbers": [57], "text": "- -Regenerate the output field with InstructGPT003\n- -10% gain\n- -Wish they did this exp. for all data points!\nHuman evaluation performance of GPT3 SELF-INST models tuned with different sizes of instructions:", "prev_chunk_id": "ALecture11_InstructionTuning__0061", "next_chunk_id": "ALecture11_InstructionTuning__0063"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0063", "chunk_index": 63, "page_numbers": [58], "text": "- -Instruction-following LLaMA model fine-tuned using SELF-INSTRUCT\n- -Minor modifications to the original SELF-INSTRUCT\n- -Used text-davinci-003 (an instruction-tuned model)\n- -Discarded the difference between classification and non-classification instructions\n- -Generated single instance for each instruction\n- -New instruction generation prompt\n- -Not use vanilla LM, but instruction-tuned LM to generate instruction data\n- -OpenHermes-2.5, a popular open-source instruction and chat dataset, containing mostly GPT-generated data\n- -OpenAI suspends ByteDance's account after it used GPT to train its own model", "prev_chunk_id": "ALecture11_InstructionTuning__0062", "next_chunk_id": "ALecture11_InstructionTuning__0064"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0064", "chunk_index": 64, "page_numbers": [59], "text": "Instruction Tuning\n- -Overview\n- -Instruction Tuning Datasets\n- -Evaluation\n- -Other Recent Instruction-Tuning Related Work", "prev_chunk_id": "ALecture11_InstructionTuning__0063", "next_chunk_id": "ALecture11_InstructionTuning__0065"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0065", "chunk_index": 65, "page_numbers": [60], "text": "- -LLMs must show proficiency in a certain set of core tasks before they can generalize to all forms of user needs:\n- -Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021). 14079 questions covering 57 tasks including elementary mathematics, US history, business ethics, formal logic and so on.\n- -MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) two mathematical datasets.\n- -Big-Bench Hard (BBH) (Suzgun et al., 2022) 23 challenging tasks, that were consistently proven to be difficult for LLMS to handle effectively.", "prev_chunk_id": "ALecture11_InstructionTuning__0064", "next_chunk_id": "ALecture11_InstructionTuning__0066"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0066", "chunk_index": 66, "page_numbers": [61], "text": "- -(Liang et al., PMLR, 2023)\n- -Multi-metric measurement\n- -An open-source framework and benchmark designed to provide transparent, multi-metric, and reproducible evaluations of large language models (LLMs).\n- -Unlike traditional benchmarks that may focus only on accuracy, HELM evaluates models on multiple metrics across various use cases , such as accuracy, robustness, efficiency, and so on.\n- -Evaluates language models over a broad range of scenarios.\n- -Broad coverage and recognition of incompleteness\n- -A holistic evaluation should provide a top-down taxonomy and make explicit all the major scenarios and metrics that are missing.\n- -Each LM should be evaluated on the same scenarios to the extent possible.\n- -Standardization", "prev_chunk_id": "ALecture11_InstructionTuning__0065", "next_chunk_id": "ALecture11_InstructionTuning__0067"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0067", "chunk_index": 67, "page_numbers": [62], "text": "Instruction Tuning\n- -Overview\n- -Instruction Tuning Datasets\n- -Evaluation\n- -Other Recent Instruction-Tuning Related Work", "prev_chunk_id": "ALecture11_InstructionTuning__0066", "next_chunk_id": "ALecture11_InstructionTuning__0068"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0068", "chunk_index": 68, "page_numbers": [63], "text": "- -LIMA has shown carefully designed, smaller subsets maybe sufficient. LIMA was manually designed, which is expensive.\n- -Selecting high-quality datasets to optimize instruction fine-tuning\n- -InstructMining (Cao et al., COLM, 2024): Utilizes natural language indicators (such as input length and perplexity) as a measure of data quality and applies them to evaluate unseen datasets.\n- -LESS (Xia et al., ICML, 2024):\n- -then selects examples based on their similarity to few-shot examples embodying a specific capability.\n- -constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features\n- -DELIFT (Agarwal et al., ICLR, 2025):\n- -measures the informational value of each data sample by quantifying its effectiveness as an in-context example in improving model predictions for other samples.\n- -uses a novel, computationally efficient utility metric (that uses conditional pointwise mutual information).\n- -reduces fine-tuning data requirements by up to 70% without compromising performance.", "prev_chunk_id": "ALecture11_InstructionTuning__0067", "next_chunk_id": "ALecture11_InstructionTuning__0069"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0069", "chunk_index": 69, "page_numbers": [64], "text": "Ishika Agarwal", "prev_chunk_id": "ALecture11_InstructionTuning__0068", "next_chunk_id": "ALecture11_InstructionTuning__0070"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0070", "chunk_index": 70, "page_numbers": [65], "text": "- (Mehri et al, 2025)\n- Reference samples demonstrate desirable qualities\n- Using them as in-context example does not effectively capture these desirable qualities\n- We propose Reference-Level Feedback to explicitly capture the desirable qualities from reference samples\nShuhaib Mehri", "prev_chunk_id": "ALecture11_InstructionTuning__0069", "next_chunk_id": "ALecture11_InstructionTuning__0071"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0071", "chunk_index": 71, "page_numbers": [66], "text": "- Then, we systematically propagate the desirable properties throughout the synthesis process\nShuhaib Mehri", "prev_chunk_id": "ALecture11_InstructionTuning__0070", "next_chunk_id": "ALecture11_InstructionTuning__0072"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0072", "chunk_index": 72, "page_numbers": [67], "text": "- -(Stolfo et al., ICLR, 2025)\n- -Enhance model adherence to constraints such as output format, length, and word inclusion, providing inference-time control over instruction following.\n- -Extract instruction specific vector representations from LMs and use them to steer models accordingly.\n- -Compositionality of activation steering: applying multiple instructions simultaneously.", "prev_chunk_id": "ALecture11_InstructionTuning__0071", "next_chunk_id": "ALecture11_InstructionTuning__0073"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0073", "chunk_index": 73, "page_numbers": [68], "text": "Figure 1: Instruction Steering Process. Steering vectors are computed as the difference in residual stream activations between inputs with and without the instruction. These vectors are then applied during inference to adjust the model's activations, guiding it to follow the desired instruction.", "prev_chunk_id": "ALecture11_InstructionTuning__0072", "next_chunk_id": "ALecture11_InstructionTuning__0074"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0074", "chunk_index": 74, "page_numbers": [69], "text": "- -Bridges the gap between the LLM pre-training objective and what the real users want.\n- -Enables more control and predictable model behavior for the LLMs.\n- -Allows the builders/developers to intervene with the model behavior.\n- -Is computationally efficient and can help LLMs quickly adapt their behavior to new domains and tasks without extensive training.", "prev_chunk_id": "ALecture11_InstructionTuning__0073", "next_chunk_id": "ALecture11_InstructionTuning__0075"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0075", "chunk_index": 75, "page_numbers": [70], "text": "- -Creating high quality instructions and outputs that properly captures the desired behavior is non-trivial.\n- -Increasing concerns that, instruction tuning\n- -improves tasks that are dominantly represented in the training datasets, for imitating models (Gudibande et al., 2023), and\n- -mainly captures patterns and styles rather than truly learning the tasks (Kung and Peng, ACL, 2023)", "prev_chunk_id": "ALecture11_InstructionTuning__0074", "next_chunk_id": "ALecture11_InstructionTuning__0076"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0076", "chunk_index": 76, "page_numbers": [71], "text": "Final Project Proposal Presentations 3-4 minute presentation by each team (no more than 3 slides!)\nOn Zoom", "prev_chunk_id": "ALecture11_InstructionTuning__0075", "next_chunk_id": "ALecture11_InstructionTuning__0077"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0077", "chunk_index": 77, "page_numbers": [72], "text": "- -Retrieval Augmented Generation", "prev_chunk_id": "ALecture11_InstructionTuning__0076", "next_chunk_id": "ALecture11_InstructionTuning__0078"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0078", "chunk_index": 78, "page_numbers": [72], "text": "- Tool Calling", "prev_chunk_id": "ALecture11_InstructionTuning__0077", "next_chunk_id": "ALecture11_InstructionTuning__0079"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0079", "chunk_index": 79, "page_numbers": [73], "text": "- TruthfulQA: measure whether a language model is truthful in generating answers to questions, consisting of 817 questions that span 38 categories, including  health, law, finance, and politics\n- Metric: the percentage of its responses that a human judges to be true or informative\n- InstructGPT models show improvements in truthfulness over GPT-3.\nGray bars indicate truthfulness and colored bars indicate truthfulness and informativeness.", "prev_chunk_id": "ALecture11_InstructionTuning__0078", "next_chunk_id": "ALecture11_InstructionTuning__0080"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0080", "chunk_index": 80, "page_numbers": [74], "text": "- RealToxicityPrompts: investigate the extent to which pre-trained LMs can be prompted to generate toxic language, such as racist and sexist, containing 100K prompts\n- Metric:\n- ‚ñ† Automatic toxicity scores from Perspective API\n- ‚ñ† Human evaluation of toxicity\n- InstructGPT shows small improvements in toxicity over GPT-3.", "prev_chunk_id": "ALecture11_InstructionTuning__0079", "next_chunk_id": "ALecture11_InstructionTuning__0081"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0081", "chunk_index": 81, "page_numbers": [75], "text": "1. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. Training language models to follow instructions with human feedback. Arxiv, 2022.\n2. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions. ACL, 2023.\n3. Or Honovich, Thomas Scialom, Omer Levy, Timo Schick. Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor. ACL, 2023.\n4. Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey Bigham. InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning. EMNLP, 2022.", "prev_chunk_id": "ALecture11_InstructionTuning__0080", "next_chunk_id": "ALecture11_InstructionTuning__0082"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0082", "chunk_index": 82, "page_numbers": [76], "text": "- Enhance zero-shot and few-shot performance\n- on a variety of new tasks, such as evaluation and intent detection,\n- by providing natural language instructions or prompts for dialogue related tasks during training.\n- Instruction tuning for 48 dialogue tasks, 59 datasets\n- open-sourced", "prev_chunk_id": "ALecture11_InstructionTuning__0081", "next_chunk_id": "ALecture11_InstructionTuning__0083"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0083", "chunk_index": 83, "page_numbers": [77, 78], "text": "- Previous works focus on general NLP tasks\n- Few involve crowdsourced dialogue tasks\n- Instruction tuning has not been systematically explored for dialogue-related tasks\n- Enables developers and even non-expert users to leverage language models using natural language without large training datasets\n- Can work for models that are significantly smaller than LLMs\nINSTRUCTDIAL task taxonomy. Green represents classification and orange represents generation tasks.", "prev_chunk_id": "ALecture11_InstructionTuning__0082", "next_chunk_id": "ALecture11_InstructionTuning__0084"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0084", "chunk_index": 84, "page_numbers": [79], "text": "- InstructDial - framework to systematically investigate instruction tuning for dialogue on a large collection of dialogue datasets and tasks\n- Various analyses and establish baseline and upper bound performance for multiple tasks.\n- Also provides integration of various task-specific dialogue metrics\n- Introduces novel meta-tasks (e.g. select an instruction that matches with an input-output pair) to encourage models to adhere to the instructions.", "prev_chunk_id": "ALecture11_InstructionTuning__0083", "next_chunk_id": "ALecture11_InstructionTuning__0085"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0085", "chunk_index": 85, "page_numbers": [80], "text": "- -Cross-task setup: the model is tested on test instances of an unseen task in the training data.\n- -In instruction-tuning, model M is provided additional signal or metainformation about the task\n- -Guides the model towards the expected output space of the new task.", "prev_chunk_id": "ALecture11_InstructionTuning__0084", "next_chunk_id": "ALecture11_InstructionTuning__0086"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0086", "chunk_index": 86, "page_numbers": [81], "text": "- -Prompts\n- -Task definitions\n- Constraints\n- -Instance inputs / Examples\n- -Output\nFor each task, manually composed 3-10 task definitions and prompts. For each instance, these are selected randomly.", "prev_chunk_id": "ALecture11_InstructionTuning__0085", "next_chunk_id": "ALecture11_InstructionTuning__0087"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0087", "chunk_index": 87, "page_numbers": [83], "text": "Introduced to help models learn associations between the instruction, the data and the task.\n- Instruction selection: the model is asked to select the instruction corresponding to given input-output pair.\n- Instruction binary: the model is asked to answer with yes or no on whether a given instruction corresponds to a given input-output pair.\nFor classification tasks, included a none-of-the-above (NOTA) option in examples, after removing the ground truth option.", "prev_chunk_id": "ALecture11_InstructionTuning__0086", "next_chunk_id": "ALecture11_InstructionTuning__0088"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0088", "chunk_index": 88, "page_numbers": [84], "text": "- -Encoder-decoder models\n- -T0-3B\n- 3 billion parameter version of T5\n- -BART0\n- 406 million parameters (8x smaller than T0-3B)\n- Based on BART-large\n- -Both trained on a multi-task mixture of general non-dialogue tasks such as question answering, sentiment detection, and paraphrase identification\n- DIAL-BART0 and DIAL-T0 are the result of the baseline models tuned on InstructDial", "prev_chunk_id": "ALecture11_InstructionTuning__0087", "next_chunk_id": "ALecture11_InstructionTuning__0089"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0089", "chunk_index": 89, "page_numbers": [85, 86, 87], "text": "Classification and generation tasks of varying complexity:\n1. Dialfact classification: predict if an evidence supports, refutes, or does not have enough information to validate the response\n2. Relation classification: predict the relation between two people in a dialogue\n3. Answer selection : predict an answer to a conversational question\n4. Eval selection: choose the most relevant response among the provided 4 option\n5. Knowledge grounded generation: generate a response based on background knowledge\n6. Begins with generation: generate a response that starts with the provided initial phrase\n- BLEU-2 (B-2): Measures the overlap of n-grams between machine-generated text and reference text.\n- GRADE (GR) : Assesses the coherence and quality of generated responses by considering response length, repetition, and informativeness.\n- RougeL (R-L): Evaluates the quality of summaries or translations by comparing the longest common subsequences between them.\nBaselinesandOurModels, ES.ACC = BaselinesandOurModels. BaselinesandOurModels, AS.ACC = BaselinesandOurModels. BaselinesandOurModels, RC.ACC = BaselinesandOurModels. BaselinesandOurModels, DC.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.B-2 = BaselinesandOurModels. BaselinesandOurModels, BW.R-L = BaselinesandOurModels. BaselinesandOurModels, BW.GR = BaselinesandOurModels. BaselinesandOurModels, KG.F1 = BaselinesandOurModels. BaselinesandOurModels, KG.B-2 = BaselinesandOurModels. BaselinesandOurModels, KG.R-L = BaselinesandOurModels. BaselinesandOurModels, KG.GR = BaselinesandOurModels. BARTO, ES.ACC = 22.2. BARTO, AS.ACC = 58.5. BARTO, RC.ACC = 6.3. BARTO, DC.ACC = 33.7. BARTO, BW.ACC = 4.2. BARTO, BW.B-2 = 4.9. BARTO, BW.R-L = 12.0. BARTO, BW.GR = 45.7. BARTO, KG.F1 = 17.4. BARTO, KG.B-2 = 5.3. BARTO, KG.R-L = 13.3. BARTO, KG.GR = 23.9. T0-3B, ES.ACC = 45.9. T0-3B, AS.ACC = 60.2. T0-3B, RC.ACC = 1.3. T0-3B, DC.ACC = 33.1. T0-3B, BW.ACC = 14.1. T0-3B, BW.B-2 = 4.1. T0-3B, BW.R-L = 10.7. T0-3B, BW.GR = 55.5. T0-3B, KG.F1 = 14.2. T0-3B, KG.B-2 = 3.2. T0-3B, KG.R-L = 10.7. T0-3B, KG.GR = 78.0. GPT-3, ES.ACC = 57.5. GPT-3, AS.ACC = 56.5. GPT-3, RC.ACC = 11.5. GPT-3, DC.ACC = 37.3. GPT-3, BW.ACC = 16.5. GPT-3, BW.B-2 = 7.2. GPT-3, BW.R-L = 15.7. GPT-3, BW.GR = 57.0. GPT-3, KG.F1 = 18.5. GPT-3, KG.B-2 = 3.9. GPT-3, KG.R-L = 11.6. GPT-3, KG.GR = 83.8. DIAL-BARTO (Ours), ES.ACC = 66.7. DIAL-BARTO (Ours), AS.ACC = 59.5. DIAL-BARTO (Ours), RC.ACC = 17.8. DIAL-BARTO (Ours), DC.ACC = 35.6. DIAL-BARTO (Ours), BW.ACC = 56.3. DIAL-BARTO (Ours), BW.B-2 = 13.1. DIAL-BARTO (Ours), BW.R-L = 26.4. DIAL-BARTO (Ours), BW.GR = 60.2. DIAL-BARTO (Ours), KG.F1 = 27.8. DIAL-BARTO (Ours), KG.B-2 = 11.1. DIAL-BARTO (Ours), KG.R-L = 21.4. DIAL-BARTO (Ours), KG.GR = 68.5. DIAL-TO (Ours), ES.ACC = 74.4. DIAL-TO (Ours), AS.ACC = 65.2. DIAL-TO (Ours), RC.ACC = 6.4. DIAL-TO (Ours), DC.ACC = 34.5. DIAL-TO (Ours), BW.ACC = 55.0. DIAL-TO (Ours), BW.B-2 = 12.4. DIAL-TO (Ours), BW.R-L = 26.5. DIAL-TO (Ours), BW.GR = 61.3. DIAL-TO (Ours), KG.F1 = 22.2. DIAL-TO (Ours), KG.B-2 = 7.2. DIAL-TO (Ours), KG.R-L = 16.5. DIAL-TO (Ours), KG.GR = 69.8. FewandFullshotVariations, ES.ACC = FewandFullshotVariations. FewandFullshotVariations, AS.ACC = FewandFullshotVariations. FewandFullshotVariations, RC.ACC = FewandFullshotVariations. FewandFullshotVariations, DC.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.B-2 = FewandFullshotVariations. FewandFullshotVariations, BW.R-L = FewandFullshotVariations. FewandFullshotVariations, BW.GR = FewandFullshotVariations. FewandFullshotVariations, KG.F1 = FewandFullshotVariations. FewandFullshotVariations, KG.B-2 = FewandFullshotVariations. FewandFullshotVariations, KG.R-L = FewandFullshotVariations. FewandFullshotVariations, KG.GR = FewandFullshotVariations. DB-Few, ES.ACC = 77.1. DB-Few, AS.ACC = 69.1. DB-Few, RC.ACC = 28.0. DB-Few, DC.ACC = 43.0. DB-Few, BW.ACC = 72.2. DB-Few, BW.B-2 = 16.7. DB-Few, BW.R-L = 30.7. DB-Few, BW.GR = 60.3. DB-Few, KG.F1 = 27.9. DB-Few, KG.B-2 = 9.7. DB-Few, KG.R-L = 20.0. DB-Few, KG.GR = 68.0. DB-Full, ES.ACC = 90.7. DB-Full, AS.ACC = 83.3. DB-Full, RC.ACC = 62.7. DB-Full, DC.ACC = 77.4. DB-Full, BW.ACC = 83.7. DB-Full, BW.B-2 = 20.8. DB-Full, BW.R-L = 33.8. DB-Full, BW.GR = 61.0. DB-Full, KG.F1 = 30.9. DB-Full, KG.B-2 = 11.6. DB-Full, KG.R-L = 22.8. DB-Full, KG.GR = 70.5. ModelAblationsforDIAL-BARTo, ES.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, AS.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, RC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, DC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.GR = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.F1 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.GR = ModelAblationsforDIAL-BARTo. DB-no-base, ES.ACC = 40.1. DB-no-base, AS.ACC = 52.7. DB-no-base, RC.ACC = 17.1. DB-no-base, DC.ACC = 35.1. DB-no-base, BW.ACC = 53.9. DB-no-base, BW.B-2 = 12.0. DB-no-base, BW.R-L = 26.6. DB-no-base, BW.GR = 57.8. DB-no-base, KG.F1 = 29.8. DB-no-base, KG.B-2 = 12.0. DB-no-base, KG.R-L = 22.8. DB-no-base, KG.GR = 69.6. DB-no-instr, ES.ACC = 23.0. DB-no-instr, AS.ACC = 43.2. DB-no-instr, RC.ACC = 15.1. DB-no-instr, DC.ACC = 35.4. DB-no-instr, BW.ACC = 50.0. DB-no-instr, BW.B-2 = 13.0. DB-no-instr, BW.R-L = 27.0. DB-no-instr, BW.GR = 61.1. DB-no-instr, KG.F1 = 30.1. DB-no-instr, KG.B-2 = 11.2. DB-no-instr, KG.R-L = 20.8. DB-no-instr, KG.GR = 65.7. DB-no-nota, ES.ACC = 66.5. DB-no-nota, AS.ACC = 57.2. DB-no-nota, RC.ACC = 17.2. DB-no-nota, DC.ACC = 35.9. DB-no-nota, BW.ACC = 56.1. DB-no-nota, BW.B-2 = 10.9. DB-no-nota, BW.R-L = 25.3. DB-no-nota, BW.GR = 58.4. DB-no-nota, KG.F1 = 28.0. DB-no-nota, KG.B-2 = 11.0. DB-no-nota, KG.R-L = 21.4. DB-no-nota, KG.GR = 67.6. DB-no-meta, ES.ACC = 44.5. DB-no-meta, AS.ACC = 52.0. DB-no-meta, RC.ACC = 14.1. DB-no-meta, DC.ACC = 35.4. DB-no-meta, BW.ACC = 52.5. DB-no-meta, BW.B-2 = 14.1. DB-no-meta, BW.R-L = 28.1. DB-no-meta, BW.GR = 61.3. DB-no-meta, KG.F1 = 29.6. DB-no-meta, KG.B-2 = 11.8. DB-no-meta, KG.R-L = 22.1. DB-no-meta, KG.GR = 70.5\nBaselinesandOurModels, ES.ACC = BaselinesandOurModels. BaselinesandOurModels, AS.ACC = BaselinesandOurModels. BaselinesandOurModels, RC.ACC = BaselinesandOurModels. BaselinesandOurModels, DC.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.B-2 = BaselinesandOurModels. BaselinesandOurModels, BW.R-L = BaselinesandOurModels. BaselinesandOurModels, BW.GR = BaselinesandOurModels. BaselinesandOurModels, KG.F1 = BaselinesandOurModels. BaselinesandOurModels, KG.B-2 = BaselinesandOurModels. BaselinesandOurModels, KG.R-L = BaselinesandOurModels. BaselinesandOurModels, KG.GR = BaselinesandOurModels. BARTO, ES.ACC = 22.2. BARTO, AS.ACC = 58.5. BARTO, RC.ACC = 6.3. BARTO, DC.ACC = 33.7. BARTO, BW.ACC = 4.2. BARTO, BW.B-2 = 4.9. BARTO, BW.R-L = 12.0. BARTO, BW.GR = 45.7. BARTO, KG.F1 = 17.4. BARTO, KG.B-2 = 5.3. BARTO, KG.R-L = 13.3. BARTO, KG.GR = 23.9. T0-3B, ES.ACC = 45.9. T0-3B, AS.ACC = 60.2. T0-3B, RC.ACC = 1.3. T0-3B, DC.ACC = 33.1. T0-3B, BW.ACC = 14.1. T0-3B, BW.B-2 = 4.1. T0-3B, BW.R-L = 10.7. T0-3B, BW.GR = 55.5. T0-3B, KG.F1 = 14.2. T0-3B, KG.B-2 = 3.2. T0-3B, KG.R-L = 10.7. T0-3B, KG.GR = 78.0. GPT-3, ES.ACC = 57.5. GPT-3, AS.ACC = 56.5. GPT-3, RC.ACC = 11.5. GPT-3, DC.ACC = 37.3. GPT-3, BW.ACC = 16.5. GPT-3, BW.B-2 = 7.2. GPT-3, BW.R-L = 15.7. GPT-3, BW.GR = 57.0. GPT-3, KG.F1 = 18.5. GPT-3, KG.B-2 = 3.9. GPT-3, KG.R-L = 11.6. GPT-3, KG.GR = 83.8. DIAL-BARTO (Ours), ES.ACC = 66.7. DIAL-BARTO (Ours), AS.ACC = 59.5. DIAL-BARTO (Ours), RC.ACC = 17.8. DIAL-BARTO (Ours), DC.ACC = 35.6. DIAL-BARTO (Ours), BW.ACC = 56.3. DIAL-BARTO (Ours), BW.B-2 = 13.1. DIAL-BARTO (Ours), BW.R-L = 26.4. DIAL-BARTO (Ours), BW.GR = 60.2. DIAL-BARTO (Ours), KG.F1 = 27.8. DIAL-BARTO (Ours), KG.B-2 = 11.1. DIAL-BARTO (Ours), KG.R-L = 21.4. DIAL-BARTO (Ours), KG.GR = 68.5. DIAL-TO (Ours), ES.ACC = 74.4. DIAL-TO (Ours), AS.ACC = 65.2. DIAL-TO (Ours), RC.ACC = 6.4. DIAL-TO (Ours), DC.ACC = 34.5. DIAL-TO (Ours), BW.ACC = 55.0. DIAL-TO (Ours), BW.B-2 = 12.4. DIAL-TO (Ours), BW.R-L = 26.5. DIAL-TO (Ours), BW.GR = 61.3. DIAL-TO (Ours), KG.F1 = 22.2. DIAL-TO (Ours), KG.B-2 = 7.2. DIAL-TO (Ours), KG.R-L = 16.5. DIAL-TO (Ours), KG.GR = 69.8. FewandFullshotVariations, ES.ACC = FewandFullshotVariations. FewandFullshotVariations, AS.ACC = FewandFullshotVariations. FewandFullshotVariations, RC.ACC = FewandFullshotVariations. FewandFullshotVariations, DC.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.B-2 = FewandFullshotVariations. FewandFullshotVariations, BW.R-L = FewandFullshotVariations. FewandFullshotVariations, BW.GR = FewandFullshotVariations. FewandFullshotVariations, KG.F1 = FewandFullshotVariations. FewandFullshotVariations, KG.B-2 = FewandFullshotVariations. FewandFullshotVariations, KG.R-L = FewandFullshotVariations. FewandFullshotVariations, KG.GR = FewandFullshotVariations. DB-Few, ES.ACC = 77.1. DB-Few, AS.ACC = 69.1. DB-Few, RC.ACC = 28.0. DB-Few, DC.ACC = 43.0. DB-Few, BW.ACC = 72.2. DB-Few, BW.B-2 = 16.7. DB-Few, BW.R-L = 30.7. DB-Few, BW.GR = 60.3. DB-Few, KG.F1 = 27.9. DB-Few, KG.B-2 = 9.7. DB-Few, KG.R-L = 20.0. DB-Few, KG.GR = 68.0. DB-Full, ES.ACC = 90.7. DB-Full, AS.ACC = 83.3. DB-Full, RC.ACC = 62.7. DB-Full, DC.ACC = 77.4. DB-Full, BW.ACC = 83.7. DB-Full, BW.B-2 = 20.8. DB-Full, BW.R-L = 33.8. DB-Full, BW.GR = 61.0. DB-Full, KG.F1 = 30.9. DB-Full, KG.B-2 = 11.6. DB-Full, KG.R-L = 22.8. DB-Full, KG.GR = 70.5. ModelAblationsforDIAL-BARTo, ES.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, AS.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, RC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, DC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.GR = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.F1 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.GR = ModelAblationsforDIAL-BARTo. DB-no-base, ES.ACC = 40.1. DB-no-base, AS.ACC = 52.7. DB-no-base, RC.ACC = 17.1. DB-no-base, DC.ACC = 35.1. DB-no-base, BW.ACC = 53.9. DB-no-base, BW.B-2 = 12.0. DB-no-base, BW.R-L = 26.6. DB-no-base, BW.GR = 57.8. DB-no-base, KG.F1 = 29.8. DB-no-base, KG.B-2 = 12.0. DB-no-base, KG.R-L = 22.8. DB-no-base, KG.GR = 69.6. DB-no-instr, ES.ACC = 23.0. DB-no-instr, AS.ACC = 43.2. DB-no-instr, RC.ACC = 15.1. DB-no-instr, DC.ACC = 35.4. DB-no-instr, BW.ACC = 50.0. DB-no-instr, BW.B-2 = 13.0. DB-no-instr, BW.R-L = 27.0. DB-no-instr, BW.GR = 61.1. DB-no-instr, KG.F1 = 30.1. DB-no-instr, KG.B-2 = 11.2. DB-no-instr, KG.R-L = 20.8. DB-no-instr, KG.GR = 65.7. DB-no-nota, ES.ACC = 66.5. DB-no-nota, AS.ACC = 57.2. DB-no-nota, RC.ACC = 17.2. DB-no-nota, DC.ACC = 35.9. DB-no-nota, BW.ACC = 56.1. DB-no-nota, BW.B-2 = 10.9. DB-no-nota, BW.R-L = 25.3. DB-no-nota, BW.GR = 58.4. DB-no-nota, KG.F1 = 28.0. DB-no-nota, KG.B-2 = 11.0. DB-no-nota, KG.R-L = 21.4. DB-no-nota, KG.GR = 67.6. DB-no-meta, ES.ACC = 44.5. DB-no-meta, AS.ACC = 52.0. DB-no-meta, RC.ACC = 14.1. DB-no-meta, DC.ACC = 35.4. DB-no-meta, BW.ACC = 52.5. DB-no-meta, BW.B-2 = 14.1. DB-no-meta, BW.R-L = 28.1. DB-no-meta, BW.GR = 61.3. DB-no-meta, KG.F1 = 29.6. DB-no-meta, KG.B-2 = 11.8. DB-no-meta, KG.R-L = 22.1. DB-no-meta, KG.GR = 70.5", "prev_chunk_id": "ALecture11_InstructionTuning__0088", "next_chunk_id": "ALecture11_InstructionTuning__0090"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0090", "chunk_index": 90, "page_numbers": [87, 88], "text": "- Eval selection, Relation classification, and Begins with perform 3x better than baseline\n- Significantly better than GPT, besides Dialfact classification\nBaselinesandOurModels, ES.ACC = BaselinesandOurModels. BaselinesandOurModels, AS.ACC = BaselinesandOurModels. BaselinesandOurModels, RC.ACC = BaselinesandOurModels. BaselinesandOurModels, DC.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.B-2 = BaselinesandOurModels. BaselinesandOurModels, BW.R-L = BaselinesandOurModels. BaselinesandOurModels, BW.GR = BaselinesandOurModels. BaselinesandOurModels, KG.F1 = BaselinesandOurModels. BaselinesandOurModels, KG.B-2 = BaselinesandOurModels. BaselinesandOurModels, KG.R-L = BaselinesandOurModels. BaselinesandOurModels, KG.GR = BaselinesandOurModels. BARTO, ES.ACC = 22.2. BARTO, AS.ACC = 58.5. BARTO, RC.ACC = 6.3. BARTO, DC.ACC = 33.7. BARTO, BW.ACC = 4.2. BARTO, BW.B-2 = 4.9. BARTO, BW.R-L = 12.0. BARTO, BW.GR = 45.7. BARTO, KG.F1 = 17.4. BARTO, KG.B-2 = 5.3. BARTO, KG.R-L = 13.3. BARTO, KG.GR = 23.9. T0-3B, ES.ACC = 45.9. T0-3B, AS.ACC = 60.2. T0-3B, RC.ACC = 1.3. T0-3B, DC.ACC = 33.1. T0-3B, BW.ACC = 14.1. T0-3B, BW.B-2 = 4.1. T0-3B, BW.R-L = 10.7. T0-3B, BW.GR = 55.5. T0-3B, KG.F1 = 14.2. T0-3B, KG.B-2 = 3.2. T0-3B, KG.R-L = 10.7. T0-3B, KG.GR = 78.0. GPT-3, ES.ACC = 57.5. GPT-3, AS.ACC = 56.5. GPT-3, RC.ACC = 11.5. GPT-3, DC.ACC = 37.3. GPT-3, BW.ACC = 16.5. GPT-3, BW.B-2 = 7.2. GPT-3, BW.R-L = 15.7. GPT-3, BW.GR = 57.0. GPT-3, KG.F1 = 18.5. GPT-3, KG.B-2 = 3.9. GPT-3, KG.R-L = 11.6. GPT-3, KG.GR = 83.8. DIAL-BARTO (Ours), ES.ACC = 66.7. DIAL-BARTO (Ours), AS.ACC = 59.5. DIAL-BARTO (Ours), RC.ACC = 17.8. DIAL-BARTO (Ours), DC.ACC = 35.6. DIAL-BARTO (Ours), BW.ACC = 56.3. DIAL-BARTO (Ours), BW.B-2 = 13.1. DIAL-BARTO (Ours), BW.R-L = 26.4. DIAL-BARTO (Ours), BW.GR = 60.2. DIAL-BARTO (Ours), KG.F1 = 27.8. DIAL-BARTO (Ours), KG.B-2 = 11.1. DIAL-BARTO (Ours), KG.R-L = 21.4. DIAL-BARTO (Ours), KG.GR = 68.5. DIAL-TO (Ours), ES.ACC = . DIAL-TO (Ours), AS.ACC = 65.2. DIAL-TO (Ours), RC.ACC = 6.4. DIAL-TO (Ours), DC.ACC = 34.5. DIAL-TO (Ours), BW.ACC = 55.0. DIAL-TO (Ours), BW.B-2 = 12.4. DIAL-TO (Ours), BW.R-L = 26.5. DIAL-TO (Ours), BW.GR = 61.3. DIAL-TO (Ours), KG.F1 = 22.2. DIAL-TO (Ours), KG.B-2 = 7.2. DIAL-TO (Ours), KG.R-L = 16.5. DIAL-TO (Ours), KG.GR = 69.8. FewandFullshotVariations, ES.ACC = FewandFullshotVariations. FewandFullshotVariations, AS.ACC = FewandFullshotVariations. FewandFullshotVariations, RC.ACC = FewandFullshotVariations. FewandFullshotVariations, DC.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.B-2 = FewandFullshotVariations. FewandFullshotVariations, BW.R-L = FewandFullshotVariations. FewandFullshotVariations, BW.GR = FewandFullshotVariations. FewandFullshotVariations, KG.F1 = FewandFullshotVariations. FewandFullshotVariations, KG.B-2 = FewandFullshotVariations. FewandFullshotVariations, KG.R-L = FewandFullshotVariations. FewandFullshotVariations, KG.GR = FewandFullshotVariations. DB-Few, ES.ACC = 77.1. DB-Few, AS.ACC = 69.1. DB-Few, RC.ACC = 28.0. DB-Few, DC.ACC = 43.0. DB-Few, BW.ACC = 72.2. DB-Few, BW.B-2 = 16.7. DB-Few, BW.R-L = 30.7. DB-Few, BW.GR = 60.3. DB-Few, KG.F1 = 27.9. DB-Few, KG.B-2 = 9.7. DB-Few, KG.R-L = 20.0. DB-Few, KG.GR = 68.0. DB-Full, ES.ACC = 90.7. DB-Full, AS.ACC = 83.3. DB-Full, RC.ACC = 62.7. DB-Full, DC.ACC = 77.4. DB-Full, BW.ACC = 83.7. DB-Full, BW.B-2 = 20.8. DB-Full, BW.R-L = 33.8. DB-Full, BW.GR = 61.0. DB-Full, KG.F1 = 30.9. DB-Full, KG.B-2 = 11.6. DB-Full, KG.R-L = 22.8. DB-Full, KG.GR = 70.5. ModelAblationsforDIAL-BARTo, ES.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, AS.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, RC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, DC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.GR = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.F1 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.GR = ModelAblationsforDIAL-BARTo. DB-no-base, ES.ACC = 40.1. DB-no-base, AS.ACC = 52.7. DB-no-base, RC.ACC = 17.1. DB-no-base, DC.ACC = 35.1. DB-no-base, BW.ACC = 53.9. DB-no-base, BW.B-2 = 12.0. DB-no-base, BW.R-L = 26.6. DB-no-base, BW.GR = 57.8. DB-no-base, KG.F1 = 29.8. DB-no-base, KG.B-2 = 12.0. DB-no-base, KG.R-L = 22.8. DB-no-base, KG.GR = 69.6. DB-no-instr, ES.ACC = 23.0. DB-no-instr, AS.ACC = 43.2. DB-no-instr, RC.ACC = 15.1. DB-no-instr, DC.ACC = 35.4. DB-no-instr, BW.ACC = 50.0. DB-no-instr, BW.B-2 = 13.0. DB-no-instr, BW.R-L = 27.0. DB-no-instr, BW.GR = 61.1. DB-no-instr, KG.F1 = 30.1. DB-no-instr, KG.B-2 = 11.2. DB-no-instr, KG.R-L = 20.8. DB-no-instr, KG.GR = 65.7. DB-no-nota, ES.ACC = 66.5. DB-no-nota, AS.ACC = 57.2. DB-no-nota, RC.ACC = 17.2. DB-no-nota, DC.ACC = 35.9. DB-no-nota, BW.ACC = 56.1. DB-no-nota, BW.B-2 = 10.9. DB-no-nota, BW.R-L = 25.3. DB-no-nota, BW.GR = 58.4. DB-no-nota, KG.F1 = 28.0. DB-no-nota, KG.B-2 = 11.0. DB-no-nota, KG.R-L = 21.4. DB-no-nota, KG.GR = 67.6. DB-no-meta, ES.ACC = 44.5. DB-no-meta, AS.ACC = 52.0. DB-no-meta, RC.ACC = 14.1. DB-no-meta, DC.ACC = 35.4. DB-no-meta, BW.ACC = 52.5. DB-no-meta, BW.B-2 = 14.1. DB-no-meta, BW.R-L = 28.1. DB-no-meta, BW.GR = 61.3. DB-no-meta, KG.F1 = 29.6. DB-no-meta, KG.B-2 = 11.8. DB-no-meta, KG.R-L = 22.1. DB-no-meta, KG.GR = 70.5", "prev_chunk_id": "ALecture11_InstructionTuning__0089", "next_chunk_id": "ALecture11_InstructionTuning__0091"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0091", "chunk_index": 91, "page_numbers": [88, 89], "text": "- T0-3B and DIAL-T0 perform better on the Eval selection and Answer Selection\n- DIAL-T0 and DIAL-BART0 perform better on the rest of the unseen task\nBaselinesandOurModels, ES.ACC = BaselinesandOurModels. BaselinesandOurModels, AS.ACC = BaselinesandOurModels. BaselinesandOurModels, RC.ACC = BaselinesandOurModels. BaselinesandOurModels, DC.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.B-2 = BaselinesandOurModels. BaselinesandOurModels, BW.R-L = BaselinesandOurModels. BaselinesandOurModels, BW.GR = BaselinesandOurModels. BaselinesandOurModels, KG.F1 = BaselinesandOurModels. BaselinesandOurModels, KG.B-2 = BaselinesandOurModels. BaselinesandOurModels, KG.R-L = BaselinesandOurModels. BaselinesandOurModels, KG.GR = BaselinesandOurModels. BARTO, ES.ACC = 22.2. BARTO, AS.ACC = 58.5. BARTO, RC.ACC = 6.3. BARTO, DC.ACC = 33.7. BARTO, BW.ACC = 4.2. BARTO, BW.B-2 = 4.9. BARTO, BW.R-L = 12.0. BARTO, BW.GR = 45.7. BARTO, KG.F1 = 17.4. BARTO, KG.B-2 = 5.3. BARTO, KG.R-L = 13.3. BARTO, KG.GR = 23.9. T0-3B, ES.ACC = 45.9. T0-3B, AS.ACC = 60.2. T0-3B, RC.ACC = 1.3. T0-3B, DC.ACC = 33.1. T0-3B, BW.ACC = 14.1. T0-3B, BW.B-2 = 4.1. T0-3B, BW.R-L = 10.7. T0-3B, BW.GR = 55.5. T0-3B, KG.F1 = 14.2. T0-3B, KG.B-2 = 3.2. T0-3B, KG.R-L = 10.7. T0-3B, KG.GR = 78.0. GPT-3, ES.ACC = 57.5. GPT-3, AS.ACC = 56.5. GPT-3, RC.ACC = 11.5. GPT-3, DC.ACC = 37.3. GPT-3, BW.ACC = 16.5. GPT-3, BW.B-2 = 7.2. GPT-3, BW.R-L = 15.7. GPT-3, BW.GR = 57.0. GPT-3, KG.F1 = 18.5. GPT-3, KG.B-2 = 3.9. GPT-3, KG.R-L = 11.6. GPT-3, KG.GR = 83.8. DIAL-BARTO(Ours), ES.ACC = 66.7. DIAL-BARTO(Ours), AS.ACC = 59.5. DIAL-BARTO(Ours), RC.ACC = 17.8. DIAL-BARTO(Ours), DC.ACC = 35.6. DIAL-BARTO(Ours), BW.ACC = 56.3. DIAL-BARTO(Ours), BW.B-2 = 13.1. DIAL-BARTO(Ours), BW.R-L = 26.4. DIAL-BARTO(Ours), BW.GR = 60.2. DIAL-BARTO(Ours), KG.F1 = 27.8. DIAL-BARTO(Ours), KG.B-2 = 11.1. DIAL-BARTO(Ours), KG.R-L = 21.4. DIAL-BARTO(Ours), KG.GR = 68.5. DIAL-TO (Ours), ES.ACC = 74.4. DIAL-TO (Ours), AS.ACC = 65.2. DIAL-TO (Ours), RC.ACC = 6.4. DIAL-TO (Ours), DC.ACC = 34.5. DIAL-TO (Ours), BW.ACC = 55.0. DIAL-TO (Ours), BW.B-2 = 12.4. DIAL-TO (Ours), BW.R-L = 26.5. DIAL-TO (Ours), BW.GR = 61.3. DIAL-TO (Ours), KG.F1 = 22.2. DIAL-TO (Ours), KG.B-2 = 7.2. DIAL-TO (Ours), KG.R-L = 16.5. DIAL-TO (Ours), KG.GR = 69.8. FewandFullshotVariations, ES.ACC = FewandFullshotVariations. FewandFullshotVariations, AS.ACC = FewandFullshotVariations. FewandFullshotVariations, RC.ACC = FewandFullshotVariations. FewandFullshotVariations, DC.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.B-2 = FewandFullshotVariations. FewandFullshotVariations, BW.R-L = FewandFullshotVariations. FewandFullshotVariations, BW.GR = FewandFullshotVariations. FewandFullshotVariations, KG.F1 = FewandFullshotVariations. FewandFullshotVariations, KG.B-2 = FewandFullshotVariations. FewandFullshotVariations, KG.R-L = FewandFullshotVariations. FewandFullshotVariations, KG.GR = FewandFullshotVariations. DB-Few, ES.ACC = 77.1. DB-Few, AS.ACC = 69.1. DB-Few, RC.ACC = 28.0. DB-Few, DC.ACC = 43.0. DB-Few, BW.ACC = 72.2. DB-Few, BW.B-2 = 16.7. DB-Few, BW.R-L = 30.7. DB-Few, BW.GR = 60.3. DB-Few, KG.F1 = 27.9. DB-Few, KG.B-2 = 9.7. DB-Few, KG.R-L = 20.0. DB-Few, KG.GR = 68.0. DB-Full, ES.ACC = 90.7. DB-Full, AS.ACC = 83.3. DB-Full, RC.ACC = 62.7. DB-Full, DC.ACC = 77.4. DB-Full, BW.ACC = 83.7. DB-Full, BW.B-2 = 20.8. DB-Full, BW.R-L = 33.8. DB-Full, BW.GR = 61.0. DB-Full, KG.F1 = 30.9. DB-Full, KG.B-2 = 11.6. DB-Full, KG.R-L = 22.8. DB-Full, KG.GR = 70.5. ModelAblationsforDIAL-BARTo, ES.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, AS.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, RC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, DC.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.ACC = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, BW.GR = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.F1 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.B-2 = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.R-L = ModelAblationsforDIAL-BARTo. ModelAblationsforDIAL-BARTo, KG.GR = ModelAblationsforDIAL-BARTo. DB-no-base, ES.ACC = 40.1. DB-no-base, AS.ACC = 52.7. DB-no-base, RC.ACC = 17.1. DB-no-base, DC.ACC = 35.1. DB-no-base, BW.ACC = 53.9. DB-no-base, BW.B-2 = 12.0. DB-no-base, BW.R-L = 26.6. DB-no-base, BW.GR = 57.8. DB-no-base, KG.F1 = 29.8. DB-no-base, KG.B-2 = 12.0. DB-no-base, KG.R-L = 22.8. DB-no-base, KG.GR = 69.6. DB-no-instr, ES.ACC = 23.0. DB-no-instr, AS.ACC = 43.2. DB-no-instr, RC.ACC = 15.1. DB-no-instr, DC.ACC = 35.4. DB-no-instr, BW.ACC = 50.0. DB-no-instr, BW.B-2 = 13.0. DB-no-instr, BW.R-L = 27.0. DB-no-instr, BW.GR = 61.1. DB-no-instr, KG.F1 = 30.1. DB-no-instr, KG.B-2 = 11.2. DB-no-instr, KG.R-L = 20.8. DB-no-instr, KG.GR = 65.7. DB-no-nota, ES.ACC = 66.5. DB-no-nota, AS.ACC = 57.2. DB-no-nota, RC.ACC = 17.2. DB-no-nota, DC.ACC = 35.9. DB-no-nota, BW.ACC = 56.1. DB-no-nota, BW.B-2 = 10.9. DB-no-nota, BW.R-L = 25.3. DB-no-nota, BW.GR = 58.4. DB-no-nota, KG.F1 = 28.0. DB-no-nota, KG.B-2 = 11.0. DB-no-nota, KG.R-L = 21.4. DB-no-nota, KG.GR = 67.6. DB-no-meta, ES.ACC = 44.5. DB-no-meta, AS.ACC = 52.0. DB-no-meta, RC.ACC = 14.1. DB-no-meta, DC.ACC = 35.4. DB-no-meta, BW.ACC = 52.5. DB-no-meta, BW.B-2 = 14.1. DB-no-meta, BW.R-L = 28.1. DB-no-meta, BW.GR = 61.3. DB-no-meta, KG.F1 = 29.6. DB-no-meta, KG.B-2 = 11.8. DB-no-meta, KG.R-L = 22.1. DB-no-meta, KG.GR = 70.5", "prev_chunk_id": "ALecture11_InstructionTuning__0090", "next_chunk_id": "ALecture11_InstructionTuning__0092"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0092", "chunk_index": 92, "page_numbers": [89, 90], "text": "- 30-50% improvement on the Begins with and Relation classification tasks\n- 12-16% improvements on the Eval selection, Answer selection, and Dialfact classification tasks\nFull-shot (DB-Full) improves across all test tasks (5000 examples/task)\nBaselinesandOurModels, ES.ACC = BaselinesandOurModels. BaselinesandOurModels, AS.ACC = BaselinesandOurModels. BaselinesandOurModels, RC.ACC = BaselinesandOurModels. BaselinesandOurModels, DC.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.ACC = BaselinesandOurModels. BaselinesandOurModels, BW.B-2 = BaselinesandOurModels. BaselinesandOurModels, BW.R-L = BaselinesandOurModels. BaselinesandOurModels, BW.GR = BaselinesandOurModels. BaselinesandOurModels, KG.F1 = BaselinesandOurModels. BaselinesandOurModels, KG.B-2 = BaselinesandOurModels. BaselinesandOurModels, KG.R-L = BaselinesandOurModels. BaselinesandOurModels, KG.GR = BaselinesandOurModels. BARTO, ES.ACC = 22.2. BARTO, AS.ACC = 58.5. BARTO, RC.ACC = 6.3. BARTO, DC.ACC = 33.7. BARTO, BW.ACC = 4.2. BARTO, BW.B-2 = 4.9. BARTO, BW.R-L = 12.0. BARTO, BW.GR = 45.7. BARTO, KG.F1 = 17.4. BARTO, KG.B-2 = 5.3. BARTO, KG.R-L = 13.3. BARTO, KG.GR = 23.9. T0-3B, ES.ACC = 45.9. T0-3B, AS.ACC = 60.2. T0-3B, RC.ACC = 1.3. T0-3B, DC.ACC = 33.1. T0-3B, BW.ACC = 14.1. T0-3B, BW.B-2 = 4.1. T0-3B, BW.R-L = 10.7. T0-3B, BW.GR = 55.5. T0-3B, KG.F1 = 14.2. T0-3B, KG.B-2 = 3.2. T0-3B, KG.R-L = 10.7. T0-3B, KG.GR = 78.0. GPT-3, ES.ACC = 57.5. GPT-3, AS.ACC = 56.5. GPT-3, RC.ACC = 11.5. GPT-3, DC.ACC = 37.3. GPT-3, BW.ACC = 16.5. GPT-3, BW.B-2 = 7.2. GPT-3, BW.R-L = 15.7. GPT-3, BW.GR = 57.0. GPT-3, KG.F1 = 18.5. GPT-3, KG.B-2 = 3.9. GPT-3, KG.R-L = 11.6. GPT-3, KG.GR = 83.8. DIAL-BARTO(Ours), ES.ACC = 66.7. DIAL-BARTO(Ours), AS.ACC = 59.5. DIAL-BARTO(Ours), RC.ACC = 17.8. DIAL-BARTO(Ours), DC.ACC = 35.6. DIAL-BARTO(Ours), BW.ACC = 56.3. DIAL-BARTO(Ours), BW.B-2 = 13.1. DIAL-BARTO(Ours), BW.R-L = 26.4. DIAL-BARTO(Ours), BW.GR = 60.2. DIAL-BARTO(Ours), KG.F1 = 27.8. DIAL-BARTO(Ours), KG.B-2 = 11.1. DIAL-BARTO(Ours), KG.R-L = 21.4. DIAL-BARTO(Ours), KG.GR = 68.5. DIAL-TO (Ours), ES.ACC = 74.4. DIAL-TO (Ours), AS.ACC = 65.2. DIAL-TO (Ours), RC.ACC = 6.4. DIAL-TO (Ours), DC.ACC = 34.5. DIAL-TO (Ours), BW.ACC = 55.0. DIAL-TO (Ours), BW.B-2 = 12.4. DIAL-TO (Ours), BW.R-L = 26.5. DIAL-TO (Ours), BW.GR = 61.3. DIAL-TO (Ours), KG.F1 = 22.2. DIAL-TO (Ours), KG.B-2 = 7.2. DIAL-TO (Ours), KG.R-L = 16.5. DIAL-TO (Ours), KG.GR = 69.8. FewandFullshotVariations, ES.ACC = FewandFullshotVariations. FewandFullshotVariations, AS.ACC = FewandFullshotVariations. FewandFullshotVariations, RC.ACC = FewandFullshotVariations. FewandFullshotVariations, DC.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.ACC = FewandFullshotVariations. FewandFullshotVariations, BW.B-2 = FewandFullshotVariations. FewandFullshotVariations, BW.R-L = FewandFullshotVariations. FewandFullshotVariations, BW.GR = FewandFullshotVariations. FewandFullshotVariations, KG.F1 = FewandFullshotVariations. FewandFullshotVariations, KG.B-2 = FewandFullshotVariations. FewandFullshotVariations, KG.R-L = FewandFullshotVariations. FewandFullshotVariations, KG.GR = FewandFullshotVariations. DB-Few, ES.ACC = 77.1. DB-Few, AS.ACC = 69.1. DB-Few, RC.ACC = 28.0. DB-Few, DC.ACC = 43.0. DB-Few, BW.ACC = 72.2. DB-Few, BW.B-2 = 16.7. DB-Few, BW.R-L = 30.7. DB-Few, BW.GR = 60.3. DB-Few, KG.F1 = 27.9. DB-Few, KG.B-2 = 9.7. DB-Few, KG.R-L = 20.0. DB-Few, KG.GR = 68.0. DB-Full, ES.ACC = 90.7. DB-Full, AS.ACC = 83.3. DB-Full, RC.ACC = 62.7. DB-Full, DC.ACC = 77.4. DB-Full, BW.ACC = 83.7. DB-Full, BW.B-2 = 20.8. DB-Full, BW.R-L = 33.8. DB-Full, BW.GR = 61.0. DB-Full, KG.F1 = 30.9. DB-Full, KG.B-2 = 11.6. DB-Full, KG.R-L = 22.8. DB-Full, KG.GR = 70.5. ModelAblationsforDIAL-BARTO, ES.ACC = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, AS.ACC = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, RC.ACC = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, DC.ACC = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, BW.ACC = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, BW.B-2 = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, BW.R-L = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, BW.GR = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, KG.F1 = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, KG.B-2 = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, KG.R-L = ModelAblationsforDIAL-BARTO. ModelAblationsforDIAL-BARTO, KG.GR = ModelAblationsforDIAL-BARTO. DB-no-base, ES.ACC = 40.1. DB-no-base, AS.ACC = 52.7. DB-no-base, RC.ACC = 17.1. DB-no-base, DC.ACC = 35.1. DB-no-base, BW.ACC = 53.9. DB-no-base, BW.B-2 = 12.0. DB-no-base, BW.R-L = 26.6. DB-no-base, BW.GR = 57.8. DB-no-base, KG.F1 = 29.8. DB-no-base, KG.B-2 = 12.0. DB-no-base, KG.R-L = 22.8. DB-no-base, KG.GR = 69.6. DB-no-instr, ES.ACC = 23.0. DB-no-instr, AS.ACC = 43.2. DB-no-instr, RC.ACC = 15.1. DB-no-instr, DC.ACC = 35.4. DB-no-instr, BW.ACC = 50.0. DB-no-instr, BW.B-2 = 13.0. DB-no-instr, BW.R-L = 27.0. DB-no-instr, BW.GR = 61.1. DB-no-instr, KG.F1 = 30.1. DB-no-instr, KG.B-2 = 11.2. DB-no-instr, KG.R-L = 20.8. DB-no-instr, KG.GR = 65.7. DB-no-nota, ES.ACC = 66.5. DB-no-nota, AS.ACC = 57.2. DB-no-nota, RC.ACC = 17.2. DB-no-nota, DC.ACC = 35.9. DB-no-nota, BW.ACC = 56.1. DB-no-nota, BW.B-2 = 10.9. DB-no-nota, BW.R-L = 25.3. DB-no-nota, BW.GR = 58.4. DB-no-nota, KG.F1 = 28.0. DB-no-nota, KG.B-2 = 11.0. DB-no-nota, KG.R-L = 21.4. DB-no-nota, KG.GR = 67.6. DB-no-meta, ES.ACC = 44.5. DB-no-meta, AS.ACC = 52.0. DB-no-meta, RC.ACC = 14.1. DB-no-meta, DC.ACC = 35.4. DB-no-meta, BW.ACC = 52.5. DB-no-meta, BW.B-2 = 14.1. DB-no-meta, BW.R-L = 28.1. DB-no-meta, BW.GR = 61.3. DB-no-meta, KG.F1 = 29.6. DB-no-meta, KG.B-2 = 11.8. DB-no-meta, KG.R-L = 22.1. DB-no-meta, KG.GR = 70.5", "prev_chunk_id": "ALecture11_InstructionTuning__0091", "next_chunk_id": "ALecture11_InstructionTuning__0093"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0093", "chunk_index": 93, "page_numbers": [90], "text": "- Large performance drop in DB-no-meta\n- Slight performance drop in DB-no-nota", "prev_chunk_id": "ALecture11_InstructionTuning__0092", "next_chunk_id": "ALecture11_InstructionTuning__0094"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0094", "chunk_index": 94, "page_numbers": [90], "text": "- DB-no-instr has worse performance than DIAL-BART0", "prev_chunk_id": "ALecture11_InstructionTuning__0093", "next_chunk_id": "ALecture11_InstructionTuning__0095"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0095", "chunk_index": 95, "page_numbers": [92], "text": "- Predict intent of a given utterance (Banking77)\n- Competitive performance in fewshot setting\n- Better than PPTOD (2x parameters)\n- BART0 struggles in zero-shot setting\nConvERT (Casanueva et al., 2020), Accuracy = 83.32. ConvERT + USE (Casanueva et al., 2020), Accuracy = 85.19. Example-Driven (Mehri and Eric, 2021), Accuracy = 85.95. PPTODbase (Su et al., 2022b), Accuracy = 82.81. PPTODlarge (Su et al., 2022b), Accuracy = 84.12. DIAL-BARTO (Ours), Accuracy = 84.30. BARTO (zero-shot), Accuracy = 14.72. DIAL-BARTO (Ours, zero-shot), Accuracy = 58.02", "prev_chunk_id": "ALecture11_InstructionTuning__0094", "next_chunk_id": "ALecture11_InstructionTuning__0096"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0096", "chunk_index": 96, "page_numbers": [93], "text": "- -Detecting slot values in a given utterance (Restaurants8k & DSTC8)\n- -36.9 point improvement in zeroshot slot filling\n- -Significant improvement in fewshot setting\nTable 4:Zero-shot slot filling results on the Restaurant8k corpus.\nModel, 1 = F1. CONVEX (HENDERSON AND VULIC, 2020), 1 = 5.2. COACH+TR (LIU ET AL.,2020), 1 = 10.7. GENSF (MEHRI AND ESKENAZI, 2021), 1 = 19.5. DIAL-BARTO (Ours), 1 = 56.4\nTable 5: Few-shot slot filling F1 scores on DSTC8 data.\n\nBuses, GENSF = 90.5. Buses, DIAL-BARTO (Ours) = 97.8. Events, GENSF = 91.2. Events, DIAL-BARTO (Ours) = 94.3. Homes, GENSF = 93.7. Homes, DIAL-BARTO (Ours) = 96.5. Rental Cars, GENSF = 86.7. Rental Cars, DIAL-BARTO (Ours) = 94.2", "prev_chunk_id": "ALecture11_InstructionTuning__0095", "next_chunk_id": "ALecture11_InstructionTuning__0097"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0097", "chunk_index": 97, "page_numbers": [94], "text": "- Similar to PPTOD, first pre-trained on 7 datasets\n- KVRET\n- WOZ\n- CamRest676\n- MSR-E2E\n- Frames\n- TaskMaster\n- Schema-Guided Dialogue\n- Then trained on 1% and 5% splits of MultiWOZ 2.0\n- Competitive performance ( PPTOD has 2x parameters)\nPPTODbase, 1%data = 29.7. PPTODbase, 5% data = 40.2. DIAL-BARTO (Ours), 1%data = 29.2. DIAL-BARTO (Ours), 5% data = 38.1\nTable 6: Joint goal Iaccuracyf for dialogue state tracking in few-shot setting on 1% and 5% data of Multiwoz.", "prev_chunk_id": "ALecture11_InstructionTuning__0096", "next_chunk_id": "ALecture11_InstructionTuning__0098"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0098", "chunk_index": 98, "page_numbers": [95], "text": "- The instructions and prompts used in the study are not crowdsourced, potentially limiting language diversity and quantity.\n- Room for improvement in zero-shot setting (compared to few-shot and full-shot)\n- Instances of task interference were observed, indicating the need for methods to mitigate negative effects and prevent task forgetting.", "prev_chunk_id": "ALecture11_InstructionTuning__0097", "next_chunk_id": "ALecture11_InstructionTuning__0099"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0099", "chunk_index": 99, "page_numbers": [96], "text": "- InstructDial addresses a critical void by systematically exploring instruction tuning for dialogue tasks, which has been largely overlooked in previous research focusing on general NLP tasks.\n- Notable achievements in both zero-shot and few-shot performance across various dialogue tasks, including intent prediction and slot filling.\n- The introduction of novel meta-tasks incentivizes models to adhere to instructions, contributing to the enhancement of overall system effectiveness and usability.", "prev_chunk_id": "ALecture11_InstructionTuning__0098", "next_chunk_id": "ALecture11_InstructionTuning__0100"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0100", "chunk_index": 100, "page_numbers": [97], "text": "Instruction: You are given a science question (easy-level) and four answer options (associated with \"A\",\"B\",\"C\", \"D\"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning. Do not generate anything else apart from one of the following characters: 'A','B,'C', 'D'. There is only one correct answer for each question.\nInput: Which part of a bicycle BEST moves in a circle? (A) Seat (B) Frame (C) Foot pedal (D) Kickstand\nConstraints: The output should be one of the following characters: 'A, 'B,'C', 'D'.", "prev_chunk_id": "ALecture11_InstructionTuning__0099", "next_chunk_id": "ALecture11_InstructionTuning__0101"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0101", "chunk_index": 101, "page_numbers": [97], "text": "Instruction: You are given a negative review and your task is to convert it to a positive review by one or more making minimal changes. Avoid changing the context of the review.\nInput: we stood there in shock, because we never expected this.\nConstraints: None.", "prev_chunk_id": "ALecture11_InstructionTuning__0100", "next_chunk_id": "ALecture11_InstructionTuning__0102"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0102", "chunk_index": 102, "page_numbers": [97], "text": "Instruction: In this task, you are given two sentences taken from a conversation, and your job is to classify whether these given sentences are sequential or not. We will mark the given sentence pair as 'True' if it's sequential, otherwise 'False'. The two sentences are spoken by two different people.\nInput: Noah: When and where are we meeting? :), Madison: I thought you were busy...?\nConstraints: None.", "prev_chunk_id": "ALecture11_InstructionTuning__0101", "next_chunk_id": "ALecture11_InstructionTuning__0103"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0103", "chunk_index": 103, "page_numbers": [97], "text": "Instruction: ln this task,youwillbe given a profileof someone and your jobistogenerateasetof interestingquestionsthatcanleadtoa conversationwiththeperson.\nInput:Yvonnehasbeenplayingtheviolinsinceshewasfouryearsold. She loves all kinds of music, but her favorite composer is Bach. Constraints:None.", "prev_chunk_id": "ALecture11_InstructionTuning__0102", "next_chunk_id": "ALecture11_InstructionTuning__0104"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0104", "chunk_index": 104, "page_numbers": [98], "text": "Instruction: In this task, you are given an article. Your task is to summarize the article in a sentence.\nInput: {INPUT}\nAlternative formulation: My college roommate asked me what this article means: \"(INPUT)\". So I recapped it in layman's terms:", "prev_chunk_id": "ALecture11_InstructionTuning__0103", "next_chunk_id": "ALecture11_InstructionTuning__0105"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0105", "chunk_index": 105, "page_numbers": [98], "text": "Instruction: This task is about writing a correct answer for the reading comprehension task. Based on the information provided in a given passage..\nInput: {INPUT}\nAlternative formulation: {INPUT} Based on the given context, the answer to the question is", "prev_chunk_id": "ALecture11_InstructionTuning__0104", "next_chunk_id": "ALecture11_InstructionTuning__0106"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0106", "chunk_index": 106, "page_numbers": [98], "text": "Instruction: In this task, you are asked to determine whether the given recipe is for a savory or sweet dish. If it is for a savory dish, output \"SAVORY\". If the recipe is for a sweet dish, output \"SWEET\".\nInput: {INPUT}\nAlternative formulation: Given the following recipe, {INPUT), is the dish savory or sweet? Your output should be\"SAVORY\"or\"SWEET\"\nUsually shorter and  less formal.", "prev_chunk_id": "ALecture11_InstructionTuning__0105", "next_chunk_id": "ALecture11_InstructionTuning__0107"}
{"lecture_id": "ALecture11_InstructionTuning", "source_pdf": "data/anlp/ALecture11_InstructionTuning.pdf", "chunk_id": "ALecture11_InstructionTuning__0107", "chunk_index": 107, "page_numbers": [99], "text": "- -Core dataset generation\n- -58K examples\n- -Reformulation\n- -240K examples\nYou need to answer the question 'Is this a good experiment design?', given an experiment variables. In addition, all other variables should be controlled so that they do not affect the results of the experiment., Category = Experiment Verification. You are given a recipe for baking muffins that contains some errors. Your task is to correct the errors in the instructions by replacing each underlined word with the correct one from the options provided., Category = Recipe Correction. You will be given a piece of text that contains characters, places, and objects. For each character in the text, you need to determine whether they are static or dynamic. A static character is someone who does not change over time, while a dynamic character is someone who undergoes significant internal changes., Category = Character Categorization. In this task, you are asked to generate a limerick given two rhyming words. A limerick is a five-line poem with the following rhyme scheme: AABBA. The first, second and fifth lines must be of three beats, while the third and fourth lines must be of two beats each. Additionally, all  poems should have the same meter (e.g., iambic pentameter), Category = Poem Generation. I'm not sure what this idiom means: ‚Äú{INPUT)\". Could you give me an example?, Category = Idiom Explanation. {INPUT} By analyzing the writing styles of the two passages, do you think they were written by the same author?, Category = Author Classification. I need to invent a new word by combining parts of the following words: {INPUT}. In what order  should I put the parts together?, Category = Word Invention. What is the punchline to the following joke? {INPUT}, Category = Humor Understanding", "prev_chunk_id": "ALecture11_InstructionTuning__0106", "next_chunk_id": null}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture12_RAG__0001"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Introduction\n- -Retrieval Methods\n- -Generation\n- -End-to-End Methods and Evaluation\n- -RAG Applications", "prev_chunk_id": "ALecture12_RAG__0000", "next_chunk_id": "ALecture12_RAG__0002"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -LMs suffer from important limitations hindering a broader deployment\n- o Knowledge cutoff: LLMs are typically pre-trained using self-supervision from text data, but knowledge is dynamic.\n- o Private data: Knowledge can be retrieved from private sources but may not be available for model training.\n- o Hallucinations/confabulations and Verifiability: LMs can provide nonfactual but seemingly plausible predictions, and it is hard to tell whether the response is accurate.\n- o Parameter inefficiency: LLMs are typically trained to perform statistical language modeling given a single parametric model and a limited context, usually the previous or surrounding tokens. Encoding all knowledge in model weights does not scale.\n- -Idea: Instead of forcing a model to memorize everything, can we let it look up information?", "prev_chunk_id": "ALecture12_RAG__0001", "next_chunk_id": "ALecture12_RAG__0003"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Pretrained language models are known to hallucinate (Ji et al., ACM Comput. Surv., 2022)\n- -Even for summarization, where the content is given, hallucination rates are around 1-2%(e.g., OpenAI GPT-4o 1.5%)\n(https://huggingface.co/spaces/vectara/Hallucinat ion-evaluation-leaderboard)\n- -And can be higher for other tasks\n- -Hallucinations are not acceptable for real applications!\nSlide from the first lecture. Example without retrieval augmentation & tool calling\nWhichschool didDilekworkat?\nDilek Hakkani-Tur has worked at several academic institutions throughout her career. She is currently a professor in the Department of Electrical and Computer Engineering at the University of Arizona,where she leads the Natural LanguageProcessing group.\nPrior to her current position, Dr. Hakkani-Tur held several research and academic positions, including working as a research scientist at AT&T Labs, a seniorresearch manager at Microsoft Research,and aprofessor at theUniversityofPittsburgh.\nThroughout her career, Dr.Hakkani-Tur has made significant contributions to the field of modal communication. Her work has been recognized with several awards and honors, including the IEEE Speech and Language Processing Technical Committee (SLTC) Award for Distinguished Technical Contributions to the field of Automatic Speech Recognition (ASR) in 2020.", "prev_chunk_id": "ALecture12_RAG__0002", "next_chunk_id": "ALecture12_RAG__0004"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0004", "chunk_index": 4, "page_numbers": [5], "text": "- -Recent line of research enabling LMs to access knowledge that may not be stored in its weights:\n- o Information retrieval or web search\n- o Tool/API calling, e.g., calculator  (next week)\n- o Code execution\n- o Reasoning (next week)", "prev_chunk_id": "ALecture12_RAG__0003", "next_chunk_id": "ALecture12_RAG__0005"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0005", "chunk_index": 5, "page_numbers": [6], "text": "- -LaMDA (next week)\n- -Internet-Augmented Dialogue Generation and Wizard of Internet (Komeili et al., 2021)\n- o LM decides to generate a search query based on a prompt\n- -ReAct also allows LMs to use different tools, such as search and Lookup in Wikipedia (Yao et al., 2022)\n- o Uses few-shot prompting\nOf courseÔºÅCruiseisa2012classic,haha.heyreperforming soon,IbelieveinAshbun.", "prev_chunk_id": "ALecture12_RAG__0004", "next_chunk_id": "ALecture12_RAG__0006"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -LM generates tool/API call, possibly with arguments, which gets executed during training or during/before decoding, e.g.,\n- o Toolformer (Schick et al., 2023)\nOut of 1400 participants, 400 (or [Calculator(400 / 1400) ‚Üí 0.29] 29%) passed the test.\nThe name derives from \"la tortuga\", the Spanish word for [MT(\"tortuga\")  turtle] turtle.\n- o AutoTOD (Xu et al., 2024) for task-oriented dialogue\n- Autonomous Agent", "prev_chunk_id": "ALecture12_RAG__0005", "next_chunk_id": "ALecture12_RAG__0007"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0007", "chunk_index": 7, "page_numbers": [8], "text": "- -Aiming to build agents that can navigate the open-ended internet with the goal of completing specific goals, such as information seeking or buying things.\n- -In addition to search, these have actions, such as clicking on links.\n- o WebGPT (Nakano et al., 2021) to answer long-form questions\n- o WebShop (Yao et al., NeurIPS 2022) to purchase a product\n- o Or more general ones, such as Mind2Web (Deng et al, NeurIPS 2023) and WebLINX (Lu et al, ICML 2024)\n- o InfoGent (Reddy et al, NAACL Findings, 2025)\n- o Web agents (in a few weeks)", "prev_chunk_id": "ALecture12_RAG__0006", "next_chunk_id": "ALecture12_RAG__0008"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0008", "chunk_index": 8, "page_numbers": [9], "text": "- -Mind's Eye (Liu et al, 2022) uses an LM to generate code, which is executed in a simulation environment and the output is used to generate an answer.\n- -Helper (Sarch et al., 2023) for embodied AI converts user instructions to code, which get executed (more when talking about embodied agents in a few weeks).", "prev_chunk_id": "ALecture12_RAG__0007", "next_chunk_id": "ALecture12_RAG__0009"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0009", "chunk_index": 9, "page_numbers": [10], "text": "- -Index a given set of documents/resources, such as wikipedia:\n- o Sparse retrievers using bag-of-words representations\n- o Dense retrievers using embeddings\n- -Conditioning LMs on the retrieved set of documents\n- o Concatenating them to context\n- o Using cross-attention\n- -Retriever training\n- o Frozen, using similarity functions\n- o Fine-tuned, with in-domain training data\n- o End-to-end, trained jointly with response generation", "prev_chunk_id": "ALecture12_RAG__0008", "next_chunk_id": "ALecture12_RAG__0010"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0010", "chunk_index": 10, "page_numbers": [11], "text": "- -Figure from (Gao et al, March 2024) https://arxiv.org/pdf/2312.10997", "prev_chunk_id": "ALecture12_RAG__0009", "next_chunk_id": "ALecture12_RAG__0011"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0011", "chunk_index": 11, "page_numbers": [13], "text": "Figure from (Gao et al, March 2024) https://arxiv.org/pdf/2312.10997", "prev_chunk_id": "ALecture12_RAG__0010", "next_chunk_id": "ALecture12_RAG__0012"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0012", "chunk_index": 12, "page_numbers": [14], "text": "Here's arundownonwhoCharlieKirkwas,and what'shappened tohim-recently:", "prev_chunk_id": "ALecture12_RAG__0011", "next_chunk_id": "ALecture12_RAG__0013"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0013", "chunk_index": 13, "page_numbers": [14], "text": "- ¬∑Charlie Kirk was a prominent right-wing conservative activist in the U.s., best known for founding Turning Point UsA in 2012,anorganization aimed at promoting conservativevalues on college campuss. Wikipedia +2\n- He also founded Turning Doint Action and Turning Point Faith. He hosted the Charlie Kirk Show and wasakeyvoiceinconservativeyouthpolitics. Wikipedia +1\n- His views were polarizing. He opposed abortion, championed gun rights, was critical of some civil rights legislation, questioned mainstream narratives around events like COvID-19, and promoted variousconservativeChristianandnationalistpositiors. Wikipedia +1", "prev_chunk_id": "ALecture12_RAG__0012", "next_chunk_id": "ALecture12_RAG__0014"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0014", "chunk_index": 14, "page_numbers": [14], "text": "- his\"AmericanComebackTour,\"ata\"Prove MeWrong\"style publicvent.A ABC News +3\n- ¬∑About 20 minutes into the event, Kirk was shot in the neck durina the asnearance. He was standing underapop-uptent,addressinga crowd of~3,ooopecple. ABC News +3\n- He was taken to a hospital (Timpanogos Regional Hospital) put was lator pronounced dead, aged 31.", "prev_chunk_id": "ALecture12_RAG__0013", "next_chunk_id": "ALecture12_RAG__0015"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0015", "chunk_index": 15, "page_numbers": [15], "text": "OpenAI ChatGPT (Sept 12, 2025):\nWho is Charlie Kirk and what happened to him?\nWhere is Professor Hao Peng's office?", "prev_chunk_id": "ALecture12_RAG__0014", "next_chunk_id": "ALecture12_RAG__0016"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0016", "chunk_index": 16, "page_numbers": [16], "text": "WhereisProfessorHaoPeng'soffice?", "prev_chunk_id": "ALecture12_RAG__0015", "next_chunk_id": "ALecture12_RAG__0017"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0017", "chunk_index": 17, "page_numbers": [16], "text": "HisofficeislocatedintheSiebelCenterforComputerScienceat201NorthGoodwinAvenue,Urbana,IL 61801.Specifically,hisofficeisRoom3314.Thisistheprimarycampusforhisacademicandresearch activities.haopeng-nlp.gith..", "prev_chunk_id": "ALecture12_RAG__0016", "next_chunk_id": "ALecture12_RAG__0018"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0018", "chunk_index": 18, "page_numbers": [16], "text": "ProfessorPeng alsomaintains anoffice at theUniversityof lllinoisChicago campus:\ngrainger.illinois...\n200SouthWackerDriveÔºå7thFloor,Chicago,IL60606\nThisofficeislikelyassociatedwithhisinvolvementintheSiebelSchoolofComputingandDataScience, whichhasapresenceinbothUrbana-ChampaignandChicago.\nHallucination can still happen, but links enable fact checking and verification.", "prev_chunk_id": "ALecture12_RAG__0017", "next_chunk_id": "ALecture12_RAG__0019"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0019", "chunk_index": 19, "page_numbers": [17], "text": "CharlieKirkrecentnews2025\n10 results‚àö\nCharlieKirkwasaconservativepoliticalactivistandfounderofTurningPointUSA. According to recent news reports, he was tragically killed on September 10, 2025, while speaking at an event at Utah Valley University in Orem, Utah.\nKirk was shot and killed during the event cNN Utah News Dispatch, and the university confirmed he was shot at 12:10 p.m. local time, 10 minutes after the student-sponsored event started,withtheshotbelievedtohavebeenfiredfromtheroofofabuilding about150yards away Charlie Kirk shooting suspe..Kirk was 31 years old and left behind a wife and two small children RIP Charlie Kirk (1993-202..\nFollowing the shooting,there was anintensive manhunt.TheFBI released photos ofa person of interest and offered a $100,000 reward for information September 11, 2025 - Char...According to the most recent reports,a suspect named Tyler Robinson,22,has been taken into custody LIVE: CharlieKirk shooting..,withPresident Trump confirming the arrest.\nThis tragic event has prompted significant discussion about political violence, with a dozen House Republicans signing a letter calling for a statue of Kirk to be placed in the U.s. Capitol FBlreleases person of inte....\nRetry\nWho is Charlie Kirk and\nSearching the web", "prev_chunk_id": "ALecture12_RAG__0018", "next_chunk_id": "ALecture12_RAG__0020"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0020", "chunk_index": 20, "page_numbers": [18], "text": "- -Introduction\n- -Retrieval Methods\n- -Generation\n- -End-to-End Methods and Evaluation\n- -RAG Applications", "prev_chunk_id": "ALecture12_RAG__0019", "next_chunk_id": "ALecture12_RAG__0021"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0021", "chunk_index": 21, "page_numbers": [20], "text": "- -Sparse representations, such as TF.IDF to represent the user query (user request) and the documents.\n- -Finds the documents that are the most similar to the user query based on cosine similarity or ranking functions such as BM25 , that have been widely as a strong baseline in the web search research.\n<!-- formula-not-decoded -->\nin\nQ\n: query, with words ,‚Ä¶,q\nq1 n\nD\n: document\nf ( qi ,D ) : the frequency of term qi document D\navgdl : average document length in the text collection k1 and b : parameters", "prev_chunk_id": "ALecture12_RAG__0020", "next_chunk_id": "ALecture12_RAG__0022"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0022", "chunk_index": 22, "page_numbers": [21], "text": "- -Dense Passage Retrieval (DPR) is the key to the success of RAG.\n- -Encodes queries and documents into fixed-length dense vectors using a neural network (e.g., BERT or other transformers).\n- -Retrieves documents based on the similarity (e.g., cosine similarity or dot product) between the query vector and document vectors.", "prev_chunk_id": "ALecture12_RAG__0021", "next_chunk_id": "ALecture12_RAG__0023"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0023", "chunk_index": 23, "page_numbers": [22], "text": "- -In the dual encoder model, both the query or the interaction context and the candidate document are encoded\n- -They then interact via a final dot-product similarity score.\n- -The model is trained using contrastive learning : it learns to bring relevant pairs (query, document) closer in vector space and push irrelevant ones farther apart.\n- -Millions of documents, brute-force comparison is too slow .\nDual-encoder architecture used in dense passage retrieval", "prev_chunk_id": "ALecture12_RAG__0022", "next_chunk_id": "ALecture12_RAG__0024"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0024", "chunk_index": 24, "page_numbers": [23], "text": "- -Encourages similar query and document pairs to have closer embeddings and different pairs to have farther embeddings.\n- -(Chen et al., 2020) for visual representations\n- -Randomly sampled minibatch of N examples, pairs of augmented examples, 2N data points:\n<!-- formula-not-decoded -->\nz i : Query z j : positive (or relevant) document z k : negative (or irrelevant) documents sim( a, b ): similarity between a and b ùùâ : temperature parameter\n- -SimCSE for sentence embeddings (Gao et al., 2022)", "prev_chunk_id": "ALecture12_RAG__0023", "next_chunk_id": "ALecture12_RAG__0025"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0025", "chunk_index": 25, "page_numbers": [24], "text": "- -A technique used to efficiently search through high-dimensional vector spaces to find vectors that are close to a query vector -without having to compare the query to every single item in the database.\n- -Indexes high-dimensional vectors in a smart way.\n- -Searches efficiently for the top-k \"approximately\" closest vectors to a query.\n- -Uses structures like trees, graphs, or hash tables to avoid brute-force.\n- -Some commonly used libraries: FAISS, ScaNN", "prev_chunk_id": "ALecture12_RAG__0024", "next_chunk_id": "ALecture12_RAG__0026"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0026", "chunk_index": 26, "page_numbers": [25], "text": "Figure from: https://blog.dailydoseofds.com/p/approximate-nearest-neighbor-search-701", "prev_chunk_id": "ALecture12_RAG__0025", "next_chunk_id": "ALecture12_RAG__0027"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0027", "chunk_index": 27, "page_numbers": [26], "text": "- -Cross-encoders are slower and more memory intensive, but also much more accurate.\n- -They can be combined, e.g., first use a bi-encoder to retrieve a few, then use cross-encoder to re-rank.\n- -Figure from: https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/", "prev_chunk_id": "ALecture12_RAG__0026", "next_chunk_id": "ALecture12_RAG__0028"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0028", "chunk_index": 28, "page_numbers": [27], "text": "- -Poly-Encoders (Humeau et al., ICLR 2020) introduce an additional attention mechanism that yields candidateaware context representations prior to a final scoring computation (DPR-Poly).\n- -Aims for the best properties of dualand cross-encoders:\n- -A given candidate label is represented by one vector as in the Bi-encoder\n- -The input context is jointly encoded with the candidate.\n- -Initialize poly-encoder with DPR Model weights (Joint-DPR-Poly)\n(c) Poly-encoder", "prev_chunk_id": "ALecture12_RAG__0027", "next_chunk_id": "ALecture12_RAG__0029"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0029", "chunk_index": 29, "page_numbers": [28], "text": "- -Captures semantic meaning , not just exact word matches.\n- -More robust to paraphrasing or variations in language.\n- -Works well with multilingual and zero-shot settings.", "prev_chunk_id": "ALecture12_RAG__0028", "next_chunk_id": "ALecture12_RAG__0030"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0030", "chunk_index": 30, "page_numbers": [28], "text": "- -Requires significant amount of training data for effectiveness.\n- -Computationally heavier (needs training and indexing).\n- -Harder to interpret or debug than sparse models.", "prev_chunk_id": "ALecture12_RAG__0029", "next_chunk_id": "ALecture12_RAG__0031"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0031", "chunk_index": 31, "page_numbers": [29], "text": "- -The units indexed in the dataset.\n- -Sentences, paragraphs, passages (100-300 tokens), entire documents, etc.\n- -Finer granularity (sentences):\n- -Higher precision (fewer irrelevant tokens), lower recall (may miss context and background info), faster indexing and search\n- -The opposite for lower granularity.\n- -More frequent in practice: overlapping passages", "prev_chunk_id": "ALecture12_RAG__0030", "next_chunk_id": "ALecture12_RAG__0032"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0032", "chunk_index": 32, "page_numbers": [30], "text": "- -Query expansion, e.g.,\n- -Paraphrasing queries with an LLM to be searched in parallel\n- -Writing sub-queries that could improve the outcome when combined\n- -Query transformation, e.g.,\n- -Rewriting queries for improving retrieval", "prev_chunk_id": "ALecture12_RAG__0031", "next_chunk_id": "ALecture12_RAG__0033"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0033", "chunk_index": 33, "page_numbers": [31], "text": "- -Introduction\n- -Retrieval Methods\n- -Generation\n- -End-to-End Methods and Evaluation\n- -RAG Applications", "prev_chunk_id": "ALecture12_RAG__0032", "next_chunk_id": "ALecture12_RAG__0034"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0034", "chunk_index": 34, "page_numbers": [33], "text": "- -(Lewis et al., NeurIPS, 2020)\n- -Top K documents are retrieved using the retriever.\n- -The generator uses the same document to generate the complete output.\n- -It produces the output sequence probability for each document, which are then marginalized:\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture12_RAG__0033", "next_chunk_id": "ALecture12_RAG__0035"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0035", "chunk_index": 35, "page_numbers": [34], "text": "- -(Lewis et al., NeurIPS, 2020)\n- -Top K documents are retrieved using the retriever.\n- -The generator then produces a distribution for the next output token for each document, before marginalizing, and repeats the process with the following output token.\n<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture12_RAG__0034", "next_chunk_id": "ALecture12_RAG__0036"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0036", "chunk_index": 36, "page_numbers": [35], "text": "- -Example Jeopardy question: 'Hemingway'\nDocument 1:his works are considered classics of American literature .. His wartime experiences formed the basis for his novel \"A Farewell to Arms\"(1929)...\nDocument 2:.. artists of the 1920s\"Lost Generation\"expatriate community. His debut novel, \"The Sun Also Rises\",was published in 1926.\nFigure 2: RAG-Token document posterior p(z|, yi , y-i) for each generated token for input \"Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".\n- -Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents.", "prev_chunk_id": "ALecture12_RAG__0035", "next_chunk_id": "ALecture12_RAG__0037"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0037", "chunk_index": 37, "page_numbers": [36], "text": "- -Decoding is simpler with RAG-sequence and the generation is more interpretable, as you can check the documents used.\n- -RAG-token is more powerful and flexible, as it can include information spread over multiple documents.\n- -RAG-token is computationally heavier and more complex to implement/debug.", "prev_chunk_id": "ALecture12_RAG__0036", "next_chunk_id": "ALecture12_RAG__0038"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0038", "chunk_index": 38, "page_numbers": [37], "text": "- -(Izacard and Grave, EACL, 2021)\n- -The encoder outputs are concatenated before passing to the decoder.\n- -Allows the decoder to attend over all document/context representations at the same time (no need to choose or marginalize!)\nFigure 2: Architecture of the Fusion-in-Decoder method.", "prev_chunk_id": "ALecture12_RAG__0037", "next_chunk_id": "ALecture12_RAG__0039"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0039", "chunk_index": 39, "page_numbers": [38], "text": "- -Reranking, e.g.,\n- -Reordering the documents to prioritize more relevant information.\n- -Context Selection/Compression, e.g.,\n- -Using small LLMs to filter unimportant tokens\n- -Reducing the number of documents by filtering less relevant ones", "prev_chunk_id": "ALecture12_RAG__0038", "next_chunk_id": "ALecture12_RAG__0040"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0040", "chunk_index": 40, "page_numbers": [40], "text": "- -Introduction\n- -Retrieval Methods\n- -Generation\n-\nEnd-to-End Methods and Evaluation\n- -RAG Applications", "prev_chunk_id": "ALecture12_RAG__0039", "next_chunk_id": "ALecture12_RAG__0041"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0041", "chunk_index": 41, "page_numbers": [41], "text": "- -(Lewis et al., NeurIPS, 2020)\n- -An encoder-decoder to encode the question and decode (generate) the answer\n- -The encoding is augmented with documents retrieved from a large unstructured document set using a pre-trained matching function.\n- -The entire neural network can then be trained end-to-end.\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query c, we use Maximum Inner Product Search (MIPS) to find the top-K documents z. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.", "prev_chunk_id": "ALecture12_RAG__0040", "next_chunk_id": "ALecture12_RAG__0042"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0042", "chunk_index": 42, "page_numbers": [42], "text": "- -Multi-turn dialogue contexts may be harder for retrieval systems than the single question context.\n- -RAG-Turn: considers turns of dialogue separately before jointly marginalizing.\n- -RAG-Turn Doc-Then Turn : first marginalizes over the documents within a turn, and then marginalize over documents across turns, for each token in the resulting sequence:\n- -RAG-Turn Doc-Only: considers each turn independently while considering documents within a turn jointly.", "prev_chunk_id": "ALecture12_RAG__0041", "next_chunk_id": "ALecture12_RAG__0043"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0043", "chunk_index": 43, "page_numbers": [43], "text": "- -Wizard of Wikipedia (WoW) (Dinan et al., 2019b)\n- -CMU Document Grounded Conversations (CMU_DoG) (Zhou et al., 2018)\n- -F1: measuring unigram word overlap between the model's generation and the ground-truth human response\n- -Knowledge F1 (KF1) measures such overlap between the model's generation with the knowledge on which the human was grounded during dataset collection", "prev_chunk_id": "ALecture12_RAG__0042", "next_chunk_id": "ALecture12_RAG__0044"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0044", "chunk_index": 44, "page_numbers": [44], "text": "Paper includes further results investigating retrieval effectiveness and also human evaluation.", "prev_chunk_id": "ALecture12_RAG__0043", "next_chunk_id": "ALecture12_RAG__0045"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0045", "chunk_index": 45, "page_numbers": [45], "text": "- -With self-supervised learning, knowledge is implicitly captured in model weights. Can we capture knowledge in a more modular and interpretable way?\n- -Idea: language model pretraining augmented with a latent knowledge retriever.\n- -Allows the model to retrieve and attend to documents from a large corpus such as Wikipedia.\n- -Used during pre-training, fine-tuning and inference.\n- -Explicitly exposes the role of world knowledge by asking the model to identify the knowledge to retrieve and use during inference.\nFigure 1.  REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in Z‚Äîa significant computational challenge that we address.", "prev_chunk_id": "ALecture12_RAG__0044", "next_chunk_id": "ALecture12_RAG__0046"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0046", "chunk_index": 46, "page_numbers": [46, 47], "text": "- -REALM decomposes computation of p ( y | x ) into two steps: retrieval and prediction:\n<!-- formula-not-decoded -->\n- -Knowledge retriever:\n<!-- formula-not-decoded -->\nFigure 1.  REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in Z‚Äîa significant computational challenge that we address.\n- -Knowledge-augmented encoder then concatenates x and z to predict y.\n- -Pre-training with masked language modeling\n- -Fine-tuning for question answering\n- -Both maximizing the log likelihood, log p ( y | x ), of the correct output y .\n- -In practice, uses top k documents as the set of z , instead of all.\nFigure 1.  REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in Z‚Äîa significant computational challenge that we address.", "prev_chunk_id": "ALecture12_RAG__0045", "next_chunk_id": "ALecture12_RAG__0047"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0047", "chunk_index": 47, "page_numbers": [48], "text": "Table 1. Test results on Open-QA benchmarks. The number of train/test examples are shown in paretheses below each benchmark. Predictions are evaluated with exact match against any reference answer. Sparse retrieval denotes methods that use sparse features such as TF-IDF and BM25. Our model, REALM, outperforms all existing systems.\nBERT-Baseline (Lee et al., 2019), Architectures = Sparse Retr.+ Transformer. BERT-Baseline (Lee et al., 2019), Pre-training = BERT. BERT-Baseline (Lee et al., 2019), NQ (79k/4k) = 26.5. BERT-Baseline (Lee et al., 2019), WQ (3k/2k) = 17.7. BERT-Baseline (Lee et al., 2019), CT (1k /1k) = 21.3. BERT-Baseline (Lee et al., 2019), # params = 110m. BERT-Baseline (Lee et al., 2019), # params = 110m. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), Architectures = Transformer Seq2Seq Transformer Seq2Seq Transformer Seq2Seq. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), Pre-training = T5 (Multitask) T5 (Multitask) T5 (Multitask). T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), NQ (79k/4k) = 27.0 29.8 34.5. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), WQ (3k/2k) = 29.1 32.2 37.4. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), CT (1k /1k) = -. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), # params = 223m 738m 11318m. T5 (base) (Roberts et al., 2020) T5 (large) (Roberts et al., 2020) T5 (11b) (Roberts et al., 2020), # params = 223m 738m 11318m. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), Architectures = Sparse Retr.+DocReader Sparse Retr.+ Transformer GraphRetriever+ Transformer PathRetriever+ Transformer Dense Retr.+ Transformer. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), Pre-training = N/A BERT BERT MLM ICT+BERT. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), NQ (79k/4k) = 28.1 31.8 32.6 33.3. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), WQ (3k/2k) = 20.7 - 31.6 - 36.4. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), CT (1k /1k) = 25.7 - - 30.1. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), # params = 34m 110m 110m 110m 330m 330m. DrQA (Chen et al., 2017) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever(Asai et al.,2019) ORQA (Lee et al., 2019), # params = 34m 110m 110m 110m 330m 330m. Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), Architectures = Dense Retr.+ Transformer Dense Retr.+ Transformer. Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), Pre-training = REALM. Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), NQ (79k/4k) = . Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), WQ (3k/2k) = . Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), CT (1k /1k) = . Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), # params = . Ours (X' =Wikipedia,Z = Wikipedia) Ours (X = CC-News,Z =Wikipedia), # params = . , Architectures = . , Pre-training = . , NQ (79k/4k) = 39.2. , WQ (3k/2k) = 40.2. , CT (1k /1k) = . , # params = . , # params = . , Architectures = . , Pre-training = . , NQ (79k/4k) = . , WQ (3k/2k) = . , CT (1k /1k) = 46.8. , # params = . , # params = . 330m, Architectures = . 330m, Pre-training = . 330m, NQ (79k/4k) = . 330m, WQ (3k/2k) = . 330m, CT (1k /1k) = . 330m, # params = . 330m, # params = . , Architectures = . , Pre-training = REALM. , NQ (79k/4k) = . , WQ (3k/2k) = . , CT (1k /1k) = . , # params = . , # params = . , Architectures = . , Pre-training = . , NQ (79k/4k) = 40.4. , WQ (3k/2k) = . , CT (1k /1k) = 42.9. , # params = . , # params = . , Architectures = . , Pre-training = . , NQ (79k/4k) = . , WQ (3k/2k) = 40.7. , CT (1k /1k) = . , # params = . , # params = ", "prev_chunk_id": "ALecture12_RAG__0046", "next_chunk_id": "ALecture12_RAG__0048"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0048", "chunk_index": 48, "page_numbers": [49], "text": "- -Introduction\n- -Retrieval Methods\n- -Generation\n- -End-to-End Methods and Evaluation\n- -RAG Applications (ODDs, Q&A, Fact Checking, NLI, ‚Ä¶)", "prev_chunk_id": "ALecture12_RAG__0047", "next_chunk_id": "ALecture12_RAG__0049"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0049", "chunk_index": 49, "page_numbers": [50], "text": "- -(Huang et al., COLING, 2022)\n- -Cross-lingual retrieval mechanisms to tap into a wealth of multilingual evidence\n- -Bridging the gap in resources for low-resource languages that are underrepresented in fact-checking datasets.", "prev_chunk_id": "ALecture12_RAG__0048", "next_chunk_id": "ALecture12_RAG__0050"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0050", "chunk_index": 50, "page_numbers": [51], "text": "- -Citation enhanced generation, (Li et al., ACL, 2024)\n- -Retrieval: to search for supporting documents relevant to the generated content.\n- -Natural language inference: for citation generation.\n- -If the statements in the generated content lack references, CEG can regenerate responses until all statements are supported by citations.\nFigure 2:An overview of our CEGframework.[R1] and[R2] denote segments.[D1] and[D2]represent retrieved documentsforeachsegment.[Ll] and[L2]are labels(Factual/Nonfactual)generated by theNLI module.", "prev_chunk_id": "ALecture12_RAG__0049", "next_chunk_id": "ALecture12_RAG__0051"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0051", "chunk_index": 51, "page_numbers": [52], "text": "- -AICE Symposium (Registration Link: https://aice.illinois.edu/)\n9:50AM-10:20AM:KeynoteAddress\nSpeaker:ZoeyLi,AppliedScientist,StoresFoundationalAI Amazon\n10:20AM-10:50AM:KeynoteAddress\nSpeaker: Reyhaneh Jabbarvand, Assistant Professor, Siebel School of Computing and Data Science University of Illinois Urbana-Champaign\nTopic: Process-centric Analysis of Agents and the Path to Future\n12:00PM-12:20PM:InvitedTalks aboutAmazon Research\nAGIFoundations,Amazon\nTopic: Responsible Generative AI\n12:20PM-1:15PM:LunchBreak\n1:15PM-1:45PM:KeynoteAddress\nSpeaker:TalAugust,AssistantProfessor,SiebelSchool ofComputing andDataScience University of Illinois Urbana-Champaign\nTopic: Personalizing knowledge through interactive systems\n3:00PM-3:20PM:InvitedTalksaboutAmazonResearch\nSpeaker:MeiqiSun,Member ofTechnicalStaff AGIFoundations,Amazon\nTopic:AGI&Agents\n4:00PM-4:30PM:KeynoteAddress\nSpeaker: George Karypis, Sr.Principal Scientist, AWS Deep Engine Science Amazon\n4:30PM:NetworkingSocial&PosterSession-1stand2ndfloorAtrium", "prev_chunk_id": "ALecture12_RAG__0050", "next_chunk_id": "ALecture12_RAG__0052"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0052", "chunk_index": 52, "page_numbers": [53], "text": "- -Tool Calling", "prev_chunk_id": "ALecture12_RAG__0051", "next_chunk_id": "ALecture12_RAG__0053"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0053", "chunk_index": 53, "page_numbers": [53], "text": "- -Reasoning", "prev_chunk_id": "ALecture12_RAG__0052", "next_chunk_id": "ALecture12_RAG__0054"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0054", "chunk_index": 54, "page_numbers": [54], "text": "Due: Tuesday, November 11th, 2025.\nGoals: Build a RAG system that combines retrieval and generation to answer knowledge-edited queries.\n1. Implement and compare retrievers: Lexical (BM25) vs. embeddingbased (bi-encoder).\n2. Evaluate retrieval quality using Hit@K.\n3. Tune prompts to make LLMs reason with retrieved evidence.\n4. Evaluate the end-to-end RAG pipeline for factual accuracy.", "prev_chunk_id": "ALecture12_RAG__0053", "next_chunk_id": "ALecture12_RAG__0055"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0055", "chunk_index": 55, "page_numbers": [55], "text": "Due: Tuesday, November 11th, 2025.\nGoals: Build a RAG system that combines retrieval and generation to answer knowledge-edited queries.", "prev_chunk_id": "ALecture12_RAG__0054", "next_chunk_id": "ALecture12_RAG__0056"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0056", "chunk_index": 56, "page_numbers": [55], "text": "1. Implementing and evaluating retrieval systems.\n2. Integrating retrieval with generation.\n3. Adapting LLM reasoning to edited/new knowledge.", "prev_chunk_id": "ALecture12_RAG__0055", "next_chunk_id": "ALecture12_RAG__0057"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0057", "chunk_index": 57, "page_numbers": [56], "text": "Due: Tuesday, November 11th, 2025.", "prev_chunk_id": "ALecture12_RAG__0056", "next_chunk_id": "ALecture12_RAG__0058"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0058", "chunk_index": 58, "page_numbers": [56], "text": "1. Fill in the <fill block> </fill block> in the .ipynb notebook", "prev_chunk_id": "ALecture12_RAG__0057", "next_chunk_id": "ALecture12_RAG__0059"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0059", "chunk_index": 59, "page_numbers": [57], "text": "Due: Tuesday, November 11th, 2025.", "prev_chunk_id": "ALecture12_RAG__0058", "next_chunk_id": "ALecture12_RAG__0060"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0060", "chunk_index": 60, "page_numbers": [57], "text": "2. Experiment with various retrievers, generator models, and number of in-context document, prompts to optimize validation performance, then submit results for val & test set.\n3. Once you're satisfied with your scores on validation set, submit your notebook to Canvas with the following name:\n'hw2_<YOUR_NET_ID>.ipynb'\n- a. Example: 'hw2_jvsingh2.ipynb'", "prev_chunk_id": "ALecture12_RAG__0059", "next_chunk_id": "ALecture12_RAG__0061"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0061", "chunk_index": 61, "page_numbers": [58], "text": "Due: Tuesday, November 11th, 2025.", "prev_chunk_id": "ALecture12_RAG__0060", "next_chunk_id": "ALecture12_RAG__0062"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0062", "chunk_index": 62, "page_numbers": [58], "text": "2. A few hidden test cases", "prev_chunk_id": "ALecture12_RAG__0061", "next_chunk_id": "ALecture12_RAG__0063"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0063", "chunk_index": 63, "page_numbers": [59], "text": "Due: Tuesday, November 11th, 2025.", "prev_chunk_id": "ALecture12_RAG__0062", "next_chunk_id": "ALecture12_RAG__0064"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0064", "chunk_index": 64, "page_numbers": [59], "text": "1. Upload your output files to your Hugging Face repository (instructions provided in the last notebook cell).\n2. Submit your Hugging Face folder link in the form below: https://docs.google.com/spreadsheets/d/1WU8gzXitnqdj6xwcNjldOf HmcXJBSpS4LfMPLlZkXtk/edit?usp=sharing", "prev_chunk_id": "ALecture12_RAG__0063", "next_chunk_id": "ALecture12_RAG__0065"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0065", "chunk_index": 65, "page_numbers": [60], "text": "- Hallucination is an issue, e.g., from GPT3.\n- Neural-retrieval-in-the-loop architectures shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2021b).\n- Knowledge grounded dialogue, more challenging, as it requires:\n- Querying based on multi-turn dialogue context\n- Generating conversationally coherent responses", "prev_chunk_id": "ALecture12_RAG__0064", "next_chunk_id": "ALecture12_RAG__0066"}
{"lecture_id": "ALecture12_RAG", "source_pdf": "data/anlp/ALecture12_RAG.pdf", "chunk_id": "ALecture12_RAG__0066", "chunk_index": 66, "page_numbers": [61], "text": "Figure from: https://blog.dailydoseofds.com/p/approximate-nearest-neighbor-search-701\ncentroicls", "prev_chunk_id": "ALecture12_RAG__0065", "next_chunk_id": null}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture13_ToolCalling__0001"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Introduction and Examples\n- -Basic Questions\n- Task Planning\n- Tool Selection\n- Response Generation\n- Benchmarks\n- -Some recent work on Tool Calling", "prev_chunk_id": "ALecture13_ToolCalling__0000", "next_chunk_id": "ALecture13_ToolCalling__0002"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Qu et al., Tool Learning with Large Language Models: A Survey, Front. Comp. Sci, 2024", "prev_chunk_id": "ALecture13_ToolCalling__0001", "next_chunk_id": "ALecture13_ToolCalling__0003"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -LMs struggle with certain tasks, such as, factual information lookup or arithmetic calculations.\n- -Many user requests require dynamic information, things that change over time, and it may not make sense to store these in model parameters (similar to RAG)\n- -Flights to San Francisco from Chicago\n- -Use of external tools, such as web search, calculators, developer APIs can help with these issues.", "prev_chunk_id": "ALecture13_ToolCalling__0002", "next_chunk_id": "ALecture13_ToolCalling__0004"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0004", "chunk_index": 4, "page_numbers": [5], "text": "- Traditional TODS architecture\n- Modern TODS architecture", "prev_chunk_id": "ALecture13_ToolCalling__0003", "next_chunk_id": "ALecture13_ToolCalling__0005"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0005", "chunk_index": 5, "page_numbers": [6], "text": "Actually, I need to leave Tuesday after 10:30.\nI have a train that leaves Tuesday at 11:17 that will arrive at 12:08. Would you like to make a reservation?\nYes, please make a reservationfor6 people. I'll need the reference number.\ntrain-leaveAt=10:30 train-day = Tuesday train-leaveAt=10:30 train-day = Tuesday train-bookpeople = 6 train-arriveby = 12:15", "prev_chunk_id": "ALecture13_ToolCalling__0004", "next_chunk_id": "ALecture13_ToolCalling__0006"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0006", "chunk_index": 6, "page_numbers": [6], "text": "I am going to bishops stortford, leaving on Friday.\nTrainTR8585leavesat 13:29and arrives at 14:07. Would you like to book this train?\nYes, for 2 people. Can I havethereference number?\ntrain-arriveby=15:15 train-destination=bishops stortford train-day = friday train-arriveby = 15:15 train-destination =bishops stortford train-day = friday train-bookpeople =2", "prev_chunk_id": "ALecture13_ToolCalling__0005", "next_chunk_id": "ALecture13_ToolCalling__0007"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0007", "chunk_index": 7, "page_numbers": [7], "text": "- -(Thoppilan et al., Preprint, 2022)\n- -Solution: fine-tuning with annotated data and consulting external knowledge resources.\n- -Increasing model sizes improve quality, but not so much the factual accuracy of the responses.\n- -LaMDA Toolset (TS): includes an information retrieval system, a calculator, and a translator.\n- -Input: string, Output: list of one or more strings. Examples:\n- -A tool will return an empty list of results if it can't parse the input.\n- -The TS sends input string to all tools in the following order: calculator, translator, and information retrieval system, and concatenates outputs.\nTool, 1 = Input. Tool, 2 = Output. Calculator, 1 = '125+12'. Calculator, 2 = ['137']. Translator, 1 = 'hello in French'. Translator, 2 = ['Bonjour']. Information Retrieval, 1 = 'How old is Rafael Nadal?'. Information Retrieval, 2 = ['Rafael Nadal / Age / 35']", "prev_chunk_id": "ALecture13_ToolCalling__0006", "next_chunk_id": "ALecture13_ToolCalling__0008"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0008", "chunk_index": 8, "page_numbers": [10], "text": "- -Crowdworkers are asked to correct LaMDA's responses, so that it can learn to imitate.\n- -Static and interactive\n- -Use the toolset to research claims\n- -Rewrite the response to include well-sourced claims.\n- -If web content is used, cite the URL by appending them at the end of the response.", "prev_chunk_id": "ALecture13_ToolCalling__0007", "next_chunk_id": "ALecture13_ToolCalling__0009"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0009", "chunk_index": 9, "page_numbers": [11], "text": "- -Context -> base model -> response base\n- -First task: Context + response base -> 'TS, query'\n- -Query -> toolset -> snippet\n- -Second task: Context + response base + query + snippet > 'User, corrected response'\n- -Note that the second task can also generate another tool call.", "prev_chunk_id": "ALecture13_ToolCalling__0008", "next_chunk_id": "ALecture13_ToolCalling__0010"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0010", "chunk_index": 10, "page_numbers": [12], "text": "- -Groundedness: # responses containing claims about the external world that can be supported by authoritative external sources / # responses with claims about the external world.\n- -Informativeness: # responses that carry information about the external world that can be supported by known sources / # all responses.\n- -Responses like 'That's a great idea' that do not carry any external world information do not affect groundedness, but they do affect Informativeness.\n- -Informativeness only differs from groundedness in the denominator term!\n- -Citation accuracy: # responses that cite the URLs of their sources / # responses with explicit claims about the external world (excluding claims with well-known facts such as \"horses have four legs\").", "prev_chunk_id": "ALecture13_ToolCalling__0009", "next_chunk_id": "ALecture13_ToolCalling__0011"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0011", "chunk_index": 11, "page_numbers": [14], "text": "- -Effects of model Scaling and fine-tuning\n- -Informativeness of human responses improves significantly with tool use.\n- -Fine-tuning introduces external tools, improving groundedness and informativeness.\n- -Metrics approach human performance. Though, weak baselines, as crowd workers were not extensively trained!", "prev_chunk_id": "ALecture13_ToolCalling__0010", "next_chunk_id": "ALecture13_ToolCalling__0012"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0012", "chunk_index": 12, "page_numbers": [15], "text": "- -(Shick et al, NeurIPS, 2023)\n- -External tools, such as web search or calculators, have been shown to help with factuality and arithmetic operations.\n- -However, they require large amounts of annotated training datasets.", "prev_chunk_id": "ALecture13_ToolCalling__0011", "next_chunk_id": "ALecture13_ToolCalling__0013"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0013", "chunk_index": 13, "page_numbers": [15], "text": "- -Tool use is learned in a self-supervised way\n- -Cost reduction\n- -What humans find useful may be different from what a model finds useful\n- -The LM should be able to decide for itself when and how to use which tool.", "prev_chunk_id": "ALecture13_ToolCalling__0012", "next_chunk_id": "ALecture13_ToolCalling__0014"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0014", "chunk_index": 14, "page_numbers": [16], "text": "- -Uses large LMs with in-context learning to generate datasets for training from scratch.\n- -Example predictions:\nThe New England Journal of Medicine is a registered trademark of [QA(\"Who is the publisher of The New England Journal ofMedicine?\")‚ÜíMassachusettsMedicalSociety]theMMS.\nOut of 1400 participants, 400 (or [Calculator(400 / 1400) ‚Üí 0.29] 29%) passed the test.\nThe name derives from \"la tortuga\", the Spanish word for [MT(\"tortuga\") ‚Üí turtle] turtle.\nThe Brown Act is California's law [WikiSearch(\"Brown Act\") ‚Üí The Ralph M. Brown Act is an act of the California State Legislature that guarantees thepublic's right to attend and participate in meetings of local legislative bodies.] that requires legislative bodies, like city councils, to hold their meetings open to the public.", "prev_chunk_id": "ALecture13_ToolCalling__0013", "next_chunk_id": "ALecture13_ToolCalling__0015"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0015", "chunk_index": 15, "page_numbers": [17], "text": "Question Answering, Example Input = Where was the Knights of Columbusfounded?. Question Answering, Example Output = New Haven, Connecticut. Wikipedia Search, Example Input = Fishing Reel Types. Wikipedia Search, Example Output = s  s    cast fishing by the type of rod and reel used. There are two types of reels used when spin fishing, the open faced reel and the closed faced reel.. Calculator, Example Input = 27+4 * 2. Calculator, Example Output = 35. Calendar, Example Input = . Calendar, Example Output = Today is Monday, January 30, 2023.. Machine Translation suret√© nucl√©aire, Example Input = . Machine Translation suret√© nucl√©aire, Example Output = nuclear safety\nTable 1: Examples of inputs and outputs for all APIs used.", "prev_chunk_id": "ALecture13_ToolCalling__0014", "next_chunk_id": "ALecture13_ToolCalling__0016"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0016", "chunk_index": 16, "page_numbers": [18], "text": "- -API calls are represented as tuples: c = ( ac ,i c ) , where ac is the name of the API and i c is the corresponding input.\n- -API call not including and including its result, r :\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- -Given a corpus C , the corpus is converted to a version, C* with API calls, using 3 steps (next slide).\n- -Inference is performed until the model produces the ‚Üí token, then the API is called and a result is obtained, then the decoding process is continued after inserting both the response and the </API> token.\n- -Then C* is used to fine-tune the model using the standard language modeling objective.", "prev_chunk_id": "ALecture13_ToolCalling__0015", "next_chunk_id": "ALecture13_ToolCalling__0017"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0017", "chunk_index": 17, "page_numbers": [20], "text": "- -For each API, a prompt is formulated to encourage the LLM to annotate an example input text with API calls.\n- -For example:\nYour taskis to addcallstoaQuestionAnsweringAPl to apieceof text.Thequestionsshould helpyougetinformationrequired to complete the text.You can call the API by writing\"[QA(question)]\"where\"question\"is the question you want to ask.Here are someexamplesofAPlcalls:\nInput: Joe Bidenwasborn inScranton,Pennsylvania.\nOutput: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania.\nInput: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company.\nOutput: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company.\nInput: x\n:ndno\nFigure 3: An exemplary prompt P(x) used to generate API calls for the question answering tool.", "prev_chunk_id": "ALecture13_ToolCalling__0016", "next_chunk_id": "ALecture13_ToolCalling__0018"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0018", "chunk_index": 18, "page_numbers": [21], "text": "- -For each token position i in the input text, the probability of starting an API call is computed.\n<!-- formula-not-decoded -->\n- -Given a sampling threshold ùúè ùë† , top k token positions that have a pi larger than the threshold are considered for API calls.\n- -For each such position, up to m API calls are sampled from the language model M, given the  sequence (formed of prompt and text so far followed by <API>):\n<!-- formula-not-decoded -->\nas prefix and </API> as an end-of-sequence token.", "prev_chunk_id": "ALecture13_ToolCalling__0017", "next_chunk_id": "ALecture13_ToolCalling__0019"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0019", "chunk_index": 19, "page_numbers": [22], "text": "- -All the produced API calls are executed at this step.\n- -API dependent step\n- -Could be executing a Python script, call a neural network or call web search.\n- -The response for each API call c i needs to be a single text sequence r i .\n- -An API call is helpful to M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input.", "prev_chunk_id": "ALecture13_ToolCalling__0018", "next_chunk_id": "ALecture13_ToolCalling__0020"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0020", "chunk_index": 20, "page_numbers": [23], "text": "Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls. While API calls are not helpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the gap between predictions with and without API calls remains high.", "prev_chunk_id": "ALecture13_ToolCalling__0019", "next_chunk_id": "ALecture13_ToolCalling__0021"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0021", "chunk_index": 21, "page_numbers": [24], "text": "- -Inability of Toolformer to use tools in a chain (i.e., using the output of one tool as an input for another tool).\n- -Sample-inefficient; for example, processing more than a million documents results in only a few thousand examples of useful calls to the calculator API.\n- -When deciding whether or not to make an API call, Toolformer currently does not take into account the tool-dependent, computational cost incurred from making an API call.\n- -Limited number of tools!", "prev_chunk_id": "ALecture13_ToolCalling__0020", "next_chunk_id": "ALecture13_ToolCalling__0022"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0022", "chunk_index": 22, "page_numbers": [25], "text": "- -(Patil et al., Preprint, 2023)\n- -The ability to invoke a vast space of changing cloud APIs can be transformative\n- -One can talk to LLMs to complete many tasks, such as arranging travels\n- -How to make sure API calls are accurate?\n- -Retrieval-based methods:\n- -Changes in the API documentation won't affect the performance\n- -Reduction in hallucination\n- -Smaller Open-Source models:\n- -Fine-Tune Llama 7B param size model, also reduces serving cost", "prev_chunk_id": "ALecture13_ToolCalling__0021", "next_chunk_id": "ALecture13_ToolCalling__0023"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0023", "chunk_index": 23, "page_numbers": [26], "text": "Hallucination: an API call that is not a sub-tree of any API in the database, invoking an entirely imagined tool.", "prev_chunk_id": "ALecture13_ToolCalling__0022", "next_chunk_id": "ALecture13_ToolCalling__0024"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0024", "chunk_index": 24, "page_numbers": [27], "text": "- -[Instruction-API] dataset creation via self-instruction: APIBench\n- -Evaluation Metric for API calls: AST tree matching\n- -Retriever-Aware Training", "prev_chunk_id": "ALecture13_ToolCalling__0023", "next_chunk_id": "ALecture13_ToolCalling__0025"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0025", "chunk_index": 25, "page_numbers": [29], "text": "- -Collected API information for modeling from Torch, Huggingface and Tenforflow hubs.\n- -Constructed 6 examples (Instruction-API pairs) manually for each of the three model hubs\n- -For each 1,645 API data points, sampled 3 of 6 corresponding instruction examples to generate a total of 10 instruction-API pairs\n- -GPT-4 for instruction-API pair generation\n- -Filtered after generation*", "prev_chunk_id": "ALecture13_ToolCalling__0024", "next_chunk_id": "ALecture13_ToolCalling__0026"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0026", "chunk_index": 26, "page_numbers": [30], "text": "- -Evaluation for accuracy , hallucination and errors\n- -mistake=hallucination or error\n- -Check for functional equivalence\n- -Evaluate only the API call", "prev_chunk_id": "ALecture13_ToolCalling__0025", "next_chunk_id": "ALecture13_ToolCalling__0027"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0027", "chunk_index": 27, "page_numbers": [31], "text": "###User:Ourcustomerisazooandwewanttohelpthemdetectmovementofdifferent animals.Writeapythonprogramin1to2linestocallAPIinTensorFlowHub. ###Assistant:\n<domain>:image-object-detection,\n<api_call>:detector=hub.load(https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1'), <api_provider>:TensorFlowHub,\n<explanation>:ThisAPIcall loadsthepre-trainedSingleShotDetector(SSD)MobileNet V2modelfromTensorFlowHub,whichhasbeentrainedontheOpenImagesv4dataset. Thismodel is used for object detection in images. Onceloaded,you can pass an image(and anoptionalscorethresholdfordetectionconfidence)tothedetectortogetbackthedetected objects along with theirboundingboxes,classlabels,and associated confidence scores. <code>:importtensorflowastf;importtensorflow_hubashub;#LoadtheSSDMobileNetV2 model;detector=hub.load('https://tfhub.dev/google/openimages_v4/ssd_mobilenet_v2/1'); #Usethemodel todetectobjectsinanimage(assumingimage'isaTensorFlow tensor representingyourimage);result=detector(image,score_threshold=0.5)\n- -Zero-shot training (i.e., no API documentation)", "prev_chunk_id": "ALecture13_ToolCalling__0026", "next_chunk_id": "ALecture13_ToolCalling__0028"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0028", "chunk_index": 28, "page_numbers": [32], "text": "\"Use this API documentation for reference: <retrieved_API_doc_JSON>\" appended to the user prompt", "prev_chunk_id": "ALecture13_ToolCalling__0027", "next_chunk_id": "ALecture13_ToolCalling__0029"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0029", "chunk_index": 29, "page_numbers": [34], "text": "- -3 retrievers:\n- -BM-25\n- -GPT-Index (text-davinci-003 from OpenAI)\n- -Oracle:\nGround Truth\n- -During retrieval, the user's query is used to search the index and fetch the most relevant (top-1) API", "prev_chunk_id": "ALecture13_ToolCalling__0028", "next_chunk_id": "ALecture13_ToolCalling__0030"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0030", "chunk_index": 30, "page_numbers": [35], "text": "LLAMA(BM-25), TorchHub.overall‚Üë = 8.60. LLAMA(BM-25), TorchHub.hallu‚Üì = 76.88. LLAMA(BM-25), TorchHub. = 14.51. LLAMA(BM-25), HuggingFace.overall‚Üë = 3.00. LLAMA(BM-25), HuggingFace. = 77.99. LLAMA(BM-25), HuggingFace. = 19.02. LLAMA(BM-25), TensorFlow Hub.overall = 8.90. LLAMA(BM-25), TensorFlow Hub.hallu‚Üì = 77.37. LLAMA(BM-25), TensorFlow Hub.err‚Üì = 13.72. GPT-3.5(BM-25), TorchHub.overall‚Üë = 38.17. GPT-3.5(BM-25), TorchHub.hallu‚Üì = 6.98. GPT-3.5(BM-25), TorchHub. = 54.83. GPT-3.5(BM-25), HuggingFace.overall‚Üë = 17.26. GPT-3.5(BM-25), HuggingFace. = 8.30. GPT-3.5(BM-25), HuggingFace. = 74.44. GPT-3.5(BM-25), TensorFlow Hub.overall = 54.16. GPT-3.5(BM-25), TensorFlow Hub.hallu‚Üì = 3.64. GPT-3.5(BM-25), TensorFlow Hub.err‚Üì = 42.18. GPT-4(BM-25), TorchHub.overall‚Üë = 35.48. GPT-4(BM-25), TorchHub.hallu‚Üì = 6711. GPT-4(BM-25), TorchHub. = 53.22. GPT-4(BM-25), HuggingFace.overall‚Üë = 16.48. GPT-4(BM-25), HuggingFace. = 15.93. GPT-4(BM-25), HuggingFace. = 67.59. GPT-4(BM-25), TensorFlow Hub.overall = 34.01. GPT-4(BM-25), TensorFlow Hub.hallu‚Üì = 37.08. GPT-4(BM-25), TensorFlow Hub.err‚Üì = 28.90. Claude(BM-25), TorchHub.overall‚Üë = 39.78. Claude(BM-25), TorchHub.hallu‚Üì = 5.37. Claude(BM-25), TorchHub. = 54.83. Claude(BM-25), HuggingFace.overall‚Üë = 14.60. Claude(BM-25), HuggingFace. = 15.82. Claude(BM-25), HuggingFace. = 69.58. Claude(BM-25), TensorFlow Hub.overall = 35.18. Claude(BM-25), TensorFlow Hub.hallu‚Üì = 21.16. Claude(BM-25), TensorFlow Hub.err‚Üì = 43.64. Gorilla (BM-25), TorchHub.overall‚Üë = 40.32. Gorilla (BM-25), TorchHub.hallu‚Üì = 4.30. Gorilla (BM-25), TorchHub. = 55.37. Gorilla (BM-25), HuggingFace.overall‚Üë = 17.03. Gorilla (BM-25), HuggingFace. = 6.42. Gorilla (BM-25), HuggingFace. = 76.55. Gorilla (BM-25), TensorFlow Hub.overall = 41.89. Gorilla (BM-25), TensorFlow Hub.hallu‚Üì = 2.77. Gorilla (BM-25), TensorFlow Hub.err‚Üì = 55.32. LLAMA(GPT-Index), TorchHub.overall‚Üë = 14.51. LLAMA(GPT-Index), TorchHub.hallu‚Üì = 75.8. LLAMA(GPT-Index), TorchHub. = 9.67. LLAMA(GPT-Index), HuggingFace.overall‚Üë = 10.18. LLAMA(GPT-Index), HuggingFace. = 75.66. LLAMA(GPT-Index), HuggingFace. = 14.20. LLAMA(GPT-Index), TensorFlow Hub.overall = 15.62. LLAMA(GPT-Index), TensorFlow Hub.hallu‚Üì = 77.66. LLAMA(GPT-Index), TensorFlow Hub.err‚Üì = 6.71. GPT-3.5(GPT-Index), TorchHub.overall‚Üë = 60.21. GPT-3.5(GPT-Index), TorchHub.hallu‚Üì = 1.61. GPT-3.5(GPT-Index), TorchHub. = 38.17. GPT-3.5(GPT-Index), HuggingFace.overall‚Üë = 29.08. GPT-3.5(GPT-Index), HuggingFace. = 7.85. GPT-3.5(GPT-Index), HuggingFace. = 44.80. GPT-3.5(GPT-Index), TensorFlow Hub.overall = 65.59. GPT-3.5(GPT-Index), TensorFlow Hub.hallu‚Üì = 3.79. GPT-3.5(GPT-Index), TensorFlow Hub.err‚Üì = 30.50. GPT-4(GPT-Index), TorchHub.overall‚Üë = 59.13. GPT-4(GPT-Index), TorchHub.hallu‚Üì = 1.07. GPT-4(GPT-Index), TorchHub. = 39.78. GPT-4(GPT-Index), HuggingFace.overall‚Üë = 44.58. GPT-4(GPT-Index), HuggingFace. = 11.18. GPT-4(GPT-Index), HuggingFace. = 44.25. GPT-4(GPT-Index), TensorFlow Hub.overall = 43.94. GPT-4(GPT-Index), TensorFlow Hub.hallu‚Üì = 31.53. GPT-4(GPT-Index), TensorFlow Hub.err‚Üì = 24.52. Claude (GPT-Index), TorchHub.overall‚Üë = 60.21. Claude (GPT-Index), TorchHub.hallu‚Üì = 3.76. Claude (GPT-Index), TorchHub. = 36.02. Claude (GPT-Index), HuggingFace.overall‚Üë = 41.37. Claude (GPT-Index), HuggingFace. = 18.81. Claude (GPT-Index), HuggingFace. = 39.82. Claude (GPT-Index), TensorFlow Hub.overall = 55.62. Claude (GPT-Index), TensorFlow Hub.hallu‚Üì = 16.20. Claude (GPT-Index), TensorFlow Hub.err‚Üì = 28.17. Gorilla(GPT-Index), TorchHub.overall‚Üë = 61.82. Gorilla(GPT-Index), TorchHub.hallu‚Üì = 0. Gorilla(GPT-Index), TorchHub. = 38.17. Gorilla(GPT-Index), HuggingFace.overall‚Üë = 47.46. Gorilla(GPT-Index), HuggingFace. = 8.19. Gorilla(GPT-Index), HuggingFace. = 44.36. Gorilla(GPT-Index), TensorFlow Hub.overall = 64.96. Gorilla(GPT-Index), TensorFlow Hub.hallu‚Üì = 2.33. Gorilla(GPT-Index), TensorFlow Hub.err‚Üì = 32.70. LLAMA(Oracle), TorchHub.overall‚Üë = 16.12. LLAMA(Oracle), TorchHub.hallu‚Üì = 79.03. LLAMA(Oracle), TorchHub. = 4.83. LLAMA(Oracle), HuggingFace.overall‚Üë = 17.70. LLAMA(Oracle), HuggingFace. = 77.10. LLAMA(Oracle), HuggingFace. = 5.20. LLAMA(Oracle), TensorFlow Hub.overall = 12.55. LLAMA(Oracle), TensorFlow Hub.hallu‚Üì = 87.00. LLAMA(Oracle), TensorFlow Hub.err‚Üì = 0.43. GPT-3.5(Oracle), TorchHub.overall‚Üë = 66.31. GPT-3.5(Oracle), TorchHub.hallu‚Üì = 1.60. GPT-3.5(Oracle), TorchHub. = 32.08. GPT-3.5(Oracle), HuggingFace.overall‚Üë = 89.71. GPT-3.5(Oracle), HuggingFace. = 6.64. GPT-3.5(Oracle), HuggingFace. = 3.65. GPT-3.5(Oracle), TensorFlow Hub.overall = 95.03. GPT-3.5(Oracle), TensorFlow Hub.hallu‚Üì = 0.29. GPT-3.5(Oracle), TensorFlow Hub.err‚Üì = 4.67. GPT-4(Oracle), TorchHub.overall‚Üë = 66.12. GPT-4(Oracle), TorchHub.hallu‚Üì = 0.53. GPT-4(Oracle), TorchHub. = 33.33. GPT-4(Oracle), HuggingFace.overall‚Üë = 85.07. GPT-4(Oracle), HuggingFace. = 10.62. GPT-4(Oracle), HuggingFace. = 4.31. GPT-4(Oracle), TensorFlow Hub.overall = 55.91. GPT-4(Oracle), TensorFlow Hub.hallu‚Üì = 37.95. GPT-4(Oracle), TensorFlow Hub.err‚Üì = 6.13. Claude(Oracle), TorchHub.overall‚Üë = 63.44. Claude(Oracle), TorchHub.hallu‚Üì = 3.76. Claude(Oracle), TorchHub. = 32.79. Claude(Oracle), HuggingFace.overall‚Üë = 77.21. Claude(Oracle), HuggingFace. = 19.58. Claude(Oracle), HuggingFace. = 3.21. Claude(Oracle), TensorFlow Hub.overall = 74.74. Claude(Oracle), TensorFlow Hub.hallu‚Üì = 21.60. Claude(Oracle), TensorFlow Hub.err‚Üì = 3.64. Gorilla (Oracle), TorchHub.overall‚Üë = 67.20. Gorilla (Oracle), TorchHub.hallu‚Üì = 0. Gorilla (Oracle), TorchHub. = 32.79. Gorilla (Oracle), HuggingFace.overall‚Üë = 91.26. Gorilla (Oracle), HuggingFace. = 7.08. Gorilla (Oracle), HuggingFace. = 1.66. Gorilla (Oracle), TensorFlow Hub.overall = 94.16. Gorilla (Oracle), TensorFlow Hub.hallu‚Üì = 1.89. Gorilla (Oracle), TensorFlow Hub.err‚Üì = 3.94", "prev_chunk_id": "ALecture13_ToolCalling__0029", "next_chunk_id": "ALecture13_ToolCalling__0031"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0031", "chunk_index": 31, "page_numbers": [36], "text": "- -API documents can change over time.", "prev_chunk_id": "ALecture13_ToolCalling__0030", "next_chunk_id": "ALecture13_ToolCalling__0032"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0032", "chunk_index": 32, "page_numbers": [37], "text": "- -Introduction and Examples\n- -Basic Questions\n- Task Planning\n- Tool Selection\n- Response Generation\n- Benchmarks\n- -Some recent work on Tool Calling", "prev_chunk_id": "ALecture13_ToolCalling__0031", "next_chunk_id": "ALecture13_ToolCalling__0033"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0033", "chunk_index": 33, "page_numbers": [38], "text": "- -What should be the toolset?\n- -When to call tools?\n- -Can the model handle the task based on its knowledge or not\n- -Which tool(s) to call?\n- -Even sometimes code‚Ä¶\n- -Some tasks are complex, requiring multiple tool calls and hence planning\n- -How to call them?\n- -How to use the results when generating the output?", "prev_chunk_id": "ALecture13_ToolCalling__0032", "next_chunk_id": "ALecture13_ToolCalling__0034"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0034", "chunk_index": 34, "page_numbers": [39], "text": "- -In many real-world applications, user intents are complex and require reasoning, environment interaction and multiple tool calls.\n- -Decomposition of user intents into sub-questions and solving them in sequence or iteratively. (Figure from Qu et al., Front. Comp. Sci, 2024)", "prev_chunk_id": "ALecture13_ToolCalling__0033", "next_chunk_id": "ALecture13_ToolCalling__0035"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0035", "chunk_index": 35, "page_numbers": [40], "text": "ChatGPT\nIamaUIuCstudent.Ihave taken48creditsfromCSand12credits fromLinguisticssofar.Howmany creditsdoIneed ineachdiscipline to do a major in CS and minor in Linguistics?\nLet'scalculatehowmany additional credits you need ineachdisciplinebased ontypicalUIuC requirements fora:\n- ¬∑Major in Computer Science (CS)\n- ¬∑Minor in Linguistics\nYou've already completed:\n- ¬∑48CS credits\n- ¬∑12 Linguistics credits", "prev_chunk_id": "ALecture13_ToolCalling__0034", "next_chunk_id": "ALecture13_ToolCalling__0036"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0036", "chunk_index": 36, "page_numbers": [40], "text": "At UIUC,a CS majortypicallyrequires:\n- ¬∑60 to72CS credits,depending on the degree program(e.g.,CS in Engineering,CS+X,etc.)\n- ¬∑Let's assume a standard CSmajorrequiring72CS credits,whichincludes:\n- ¬∑Core courses (e.g.,CS 124,128,225,233, 374)\n- ¬∑Advanced CS electives(300/400level)\n- ¬∑Technical electives\n- ¬∑128total creditstograduate(includesGenEds,electives,etc.)\n- >You have:48CS credits\n- >You need:72-48=24moreCScredits", "prev_chunk_id": "ALecture13_ToolCalling__0035", "next_chunk_id": "ALecture13_ToolCalling__0037"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0037", "chunk_index": 37, "page_numbers": [40], "text": "A Linguistics minor at UIuC typically requires:\n- ¬∑18to21totalcredits\n- ¬∑Must include:\n- ¬∑An intro course (e.g., LING 100)\n- ¬∑Atleast6creditsatthe300or400level\n- ¬∑Distributionoftheoretical/applied courses\n- >You have:12Linguistics credits\n- >Youneed:18-12=6moreLINGcredits\nAMake sure at least6total credits are at the300/400level-some of your12completed credits may not satisfy that yet.", "prev_chunk_id": "ALecture13_ToolCalling__0036", "next_chunk_id": "ALecture13_ToolCalling__0038"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0038", "chunk_index": 38, "page_numbers": [40], "text": "ComputerScience(Major), CreditsTaken = 48. ComputerScience(Major), Typical Requirement = 72. ComputerScience(Major), Credits Still Needed = 24 more CS credits. Linguistics (Minor), CreditsTaken = 12. Linguistics (Minor), Typical Requirement = 18. Linguistics (Minor), Credits Still Needed = 6moreLING credits", "prev_chunk_id": "ALecture13_ToolCalling__0037", "next_chunk_id": "ALecture13_ToolCalling__0039"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0039", "chunk_index": 39, "page_numbers": [41], "text": "1, Description = Determine credit requirements for CS major and LING minor. 2, Description = Verify how many credits you've completed in CS and LING. 3, Description = Subtract completed credits from required credits to find remaining amounts. 4, Description = Check whether completed courses satisfy level and category requirements. 5, Description = Consider total graduation credit and Gen Ed needs. 6, Description = Validate findings with DARS or academic advisor", "prev_chunk_id": "ALecture13_ToolCalling__0038", "next_chunk_id": "ALecture13_ToolCalling__0040"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0040", "chunk_index": 40, "page_numbers": [42], "text": "- -(Yao et al, ICLR 2023)\n- -Uses LLMs to generate reasoning traces and task specific actions\n- -Step-by-step thinking with in-context examples", "prev_chunk_id": "ALecture13_ToolCalling__0039", "next_chunk_id": "ALecture13_ToolCalling__0041"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0041", "chunk_index": 41, "page_numbers": [43], "text": "- -Fine-tunes the model to learn to call tools\n- -Examples:\n- -Toolformer (Schick et al., NeurIPS, 2023)\n- -TaskMatrix.ai (Liang et al., NeurIPS, 2024)", "prev_chunk_id": "ALecture13_ToolCalling__0040", "next_chunk_id": "ALecture13_ToolCalling__0042"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0042", "chunk_index": 42, "page_numbers": [44], "text": "- -Retriever-based, examples:\n- -Gorilla (Patil et al., Preprint, 2023), BM25, dense retrieval methods\n- -LLM-based, examples:\n- -Toolformer (Schick et al., NeurIPS, 2023)\n- -AnyTool (Du et al., ICML, 2024)\n- -Function calling abilities of GPT-4\n- -Uses over 16K APIs from Rapid API\n- -API retriever has a hierarchical structure", "prev_chunk_id": "ALecture13_ToolCalling__0041", "next_chunk_id": "ALecture13_ToolCalling__0043"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0043", "chunk_index": 43, "page_numbers": [45], "text": "- -Directly inserting the tool results into the response.\n- -Examples:\n- -Toolformer (Schick et al., NeurIPS, 2023)\n- -ToolkenGPT (Hao et al., NeurIPS, 2024)\n- -Represents each tool as a token (i.e., toolken)\n- -Keeps LLM frozen and learns embeddings for toolkens\n- -During inference, once a toolken is triggered, the LLM is prompted to generate its arguments.\n- -The tool call results are used to infill text.\n- -Incorporating the output of  tools into the context as input to LLMs.\n- -Enables the model to generate a better response based on  the information provided by the tool.", "prev_chunk_id": "ALecture13_ToolCalling__0042", "next_chunk_id": "ALecture13_ToolCalling__0044"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0044", "chunk_index": 44, "page_numbers": [46], "text": "- -Introduction and Examples\n- -Basic Questions\n- Task Planning\n- Tool Selection\n- Response Generation\n- Benchmarks\n- -Some recent work on Tool Calling", "prev_chunk_id": "ALecture13_ToolCalling__0043", "next_chunk_id": "ALecture13_ToolCalling__0045"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0045", "chunk_index": 45, "page_numbers": [47], "text": "- -(Acikgoz et al., ACL , 2025)\n- -TODS: Great at managing dialogue and tracking user intent, but limited to a narrow set of APIs -require expensive retraining or prompt engineering for new services.\n- -Language Agents (LAs): Excel at function calling across diverse APIs, but often fail to maintain coherent multi-turn conversations and long-term context.\n- -Research Question: How can we design a single agentic LLM that interweaves multi-turn dialogue mastery with advanced function calling?", "prev_chunk_id": "ALecture13_ToolCalling__0044", "next_chunk_id": "ALecture13_ToolCalling__0046"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0046", "chunk_index": 46, "page_numbers": [48], "text": "- -CoALM-IT: a comprehensive instruction-tuning dataset that includes multi-turn interactions, reasoning and planning steps.\n- -Derived from existing conversational datasets with task annotations.\n- -Dialogue State Tracking: Single-turn and multi-turn dialogue state tracking from SNIPS and other benchmarks (TOD).\n- -Function/API calling: Complex and diverse tool-calling tasks from Hammer and ToolAce.\n- -CRA (Conversational ReAct API) Samples: Multi-turn dialogue scenarios with explicit reasoning steps, action planning, and API calls -generated via GPT-4o for realism and complexity.\n- -CoALM Agents: Fine-tuned Llama models (8B, 70B, 405B) on CoALM-IT.", "prev_chunk_id": "ALecture13_ToolCalling__0045", "next_chunk_id": "ALecture13_ToolCalling__0047"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0047", "chunk_index": 47, "page_numbers": [50], "text": "- -Combining tool use and task-oriented dialogue datasets\n- -311K dialogues annotated in ReAct style, with reasoning steps, tools calls and system responses.\n- -Instruction fine-tuning multiple Llama models", "prev_chunk_id": "ALecture13_ToolCalling__0046", "next_chunk_id": "ALecture13_ToolCalling__0048"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0048", "chunk_index": 48, "page_numbers": [51], "text": "- -Evaluated on MultiWOZ 2.4 for Success rates and Joint Goal Accuracy (JGA)\n- -Success rates: The percentage of dialogues where the system was able to return an entity that matches what the user requested.\n- -JGA: The percentage of turns where all function calls and arguments (i.e., all slots for all domains) specified so far are accurately extracted.\nCoALM 8B (ours), Success = 51.6. CoALM 8B (ours), JGA = 30.4. CoALM 70B (ours), Success = 69.4. CoALM 70B (ours), JGA = 43.8. CoALM 405B (ours)*, Success = 66.7. CoALM 405B (ours)*, JGA = 38.8. Hammer2.0-7B, Success = 23.5. Hammer2.0-7B, JGA = 21.7. ToolAce, Success = 18.0. ToolAce, JGA = 34.4. Granite-20B-Code, Success = 10.7. Granite-20B-Code, JGA = 21.8. CodeActAgent, Success = 9.5. CodeActAgent, JGA = 20.2. Llama-3.1-8B-Instruct, Success = 19.9. Llama-3.1-8B-Instruct, JGA = 26.3. Llama-3.3-70B-Instruct, Success = 67.6. Llama-3.3-70B-Instruct, JGA = 40.8. Mistral-7B-Instruct-v0.3, Success = 31.2. Mistral-7B-Instruct-v0.3, JGA = 27.0. FNCTOD (Li et al., 2024), Success = 44.4. FNCTOD (Li et al., 2024), JGA = 37.9. NC-Latent-TOD (King and Flanigan, 2024), Success = 68.3. NC-Latent-TOD (King and Flanigan, 2024), JGA = 39.7. GPT-3.5-Turbo (Hudecek and Dusek, 2023), Success = -. GPT-3.5-Turbo (Hudecek and Dusek, 2023), JGA = 13.5. GPT4o-mini, Success = 69.9. GPT4o-mini, JGA = 38.4. GPT40, Success = 75.5. GPT40, JGA = 36.9", "prev_chunk_id": "ALecture13_ToolCalling__0047", "next_chunk_id": "ALecture13_ToolCalling__0049"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0049", "chunk_index": 49, "page_numbers": [52], "text": "- -API-Bank: Complex Function Calling\n- -Similar improvements on BFCL V3 (check the paper for more results)\nCoALM 8B (ours), Success = 51.6. CoALM 8B (ours), JGA = 30.4. CoALM 70B (ours), Success = 69.4. CoALM 70B (ours), JGA = 43.8. CoALM 405B (ours)*, Success = 66.7. CoALM 405B (ours)*, JGA = 38.8. Hammer2.0-7B, Success = 23.5. Hammer2.0-7B, JGA = 21.7. ToolAce, Success = 18.0. ToolAce, JGA = 34.4. Granite-20B-Code, Success = 10.7. Granite-20B-Code, JGA = 21.8. CodeActAgent, Success = 9.5. CodeActAgent, JGA = 20.2. Llama-3.1-8B-Instruct, Success = 19.9. Llama-3.1-8B-Instruct, JGA = 26.3. Llama-3.3-70B-Instruct, Success = 67.6. Llama-3.3-70B-Instruct, JGA = 40.8. Mistral-7B-Instruct-v0.3, Success = 31.2. Mistral-7B-Instruct-v0.3, JGA = 27.0. FNCTOD (Li et al., 2024), Success = 44.4. FNCTOD (Li et al., 2024), JGA = 37.9. NC-Latent-TOD (King and Flanigan, 2024), Success = 68.3. NC-Latent-TOD (King and Flanigan, 2024), JGA = 39.7. GPT-3.5-Turbo (HudeCek and Dusek, 2023), Success = . GPT-3.5-Turbo (HudeCek and Dusek, 2023), JGA = 13.5. GPT4o-mini, Success = 69.9. GPT4o-mini, JGA = 38.4. GPT40, Success = 75.5. GPT40, JGA = 36.9", "prev_chunk_id": "ALecture13_ToolCalling__0048", "next_chunk_id": "ALecture13_ToolCalling__0050"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0050", "chunk_index": 50, "page_numbers": [53], "text": "- -(Qian, et al., NeurIPS, 2025)\n- -LLMs excel at reasoning but have limitations (e.g., outdated knowledge and calculation errors).\n- -Pure NL reasoning (e.g., CoT) has been effective on many tasks, but fails on tasks that require complex calculations, solving equations, etc.\n- -Tool-Integrated Reasoning (TIR): incorporating external tools (e.g., search, calculators, APIs) into the reasoning trajectory of an LLM to solve a user task.\n- -Most previous work distills trajectories from stronger models and perform Supervised Fine-Tuning (SFT)\n- -restricts models to predetermined tool usage patterns and limits exploration of optimal strategies.\n- -Improved reasoning capabilities through reinforcement learning (RL)\nCheng Qian\nEmre Can Acikgoz", "prev_chunk_id": "ALecture13_ToolCalling__0049", "next_chunk_id": "ALecture13_ToolCalling__0051"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0051", "chunk_index": 51, "page_numbers": [54], "text": "- -(Li et al., Preprint, 2025)\n- -Learns flexible, adaptive strategies through exploration and feedback for Math reasoning.\n- -Simple reward, +1 for task success and -1 for failure.", "prev_chunk_id": "ALecture13_ToolCalling__0050", "next_chunk_id": "ALecture13_ToolCalling__0052"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0052", "chunk_index": 52, "page_numbers": [55], "text": "- -Tool use is complex: Multi-step, multiple tools, diverse parameters.\n- -Simple rewards (e.g., final answer match) are too coarse/sparse.\n- -Research questions:\nCan RL-based training methods better equip LLMs with agentic tool-using capabilities? How can we design effective reward signals to train LLMs for general-purpose, robust tool selection and application via RL?", "prev_chunk_id": "ALecture13_ToolCalling__0051", "next_chunk_id": "ALecture13_ToolCalling__0053"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0053", "chunk_index": 53, "page_numbers": [56], "text": "- -Goal: Develop a robust RL framework specifically for general tool learning.\n- -Core Idea: Combine a suitable RL algorithm (GRPO) with a carefully crafted, multi-component reward function tailored to tool use intricacies.", "prev_chunk_id": "ALecture13_ToolCalling__0052", "next_chunk_id": "ALecture13_ToolCalling__0054"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0054", "chunk_index": 54, "page_numbers": [57], "text": "```\nYou are a helpful multi-turn dialogue assistant capable of leveraging tool calls to solveusertasksandprovidestructuredchatresponses. AvailableTools In your response, you can use the following tools: {{tool list}} Stepsfor Each Turn 1. Think: Recall relevant context and analyze the current user goal. 2. Decide on Tool Usage: If a tool is needed, specify the tool and its parameters. 3. Respond Appropriately: If a response is needed, generate one while maintaining consistency across user queries. OutputFormat <think> Your thoughts and reasoning < /think> <tool_call> {\"name\": \"Tool name\",\"parameters\": {\"Parameter name\": \"Parameter content\",\".. </tool.call> <response> AI's final response </response> Important Notes 1. You must always include the <think> field to outline your reasoning. Provide at least one of <tool-call> or <response>. Decide whether to use <tool.call> (possibly multiple times), <response>, or both. 2. You can invoke multiple tool calls simultaneously in the <tool-call> fields. Each i ,sad, e pue pin ,au, e ym poalqo nosf e aq pous is poo containing a dictionary of parameters. If no parameters are needed, leave the 3. Refer to the previous dialogue records in the history, including the user's queries, previous <tool-call>, <response>, and any tool feedback noted as <obs> (if exists).\n```\nInstruct the LLM to output different fields wrapped by the XML tags", "prev_chunk_id": "ALecture13_ToolCalling__0053", "next_chunk_id": "ALecture13_ToolCalling__0055"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0055", "chunk_index": 55, "page_numbers": [58], "text": "- -R_format ‚àà {0, 1}\n- -Checks if the output structure is correct (presence and order of required tokens like <think>, <tool_call>)\n- -Directly compares the field tags in the prediction and ground truth\n- -Simple, encourages structural compliance", "prev_chunk_id": "ALecture13_ToolCalling__0054", "next_chunk_id": "ALecture13_ToolCalling__0056"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0056", "chunk_index": 56, "page_numbers": [59], "text": "- -R_correct ‚àà [-3, 3])\n- -Tool Name Matching: Includes calls to the right tool(s)?\n- -Parameter Name Matching: Includes correct parameter names for the chosen tool(s)?\n- -Parameter Content Matching: Includes correct values for the parameters?", "prev_chunk_id": "ALecture13_ToolCalling__0055", "next_chunk_id": "ALecture13_ToolCalling__0057"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0057", "chunk_index": 57, "page_numbers": [60], "text": "- -Training Data: 4K diverse examples (ToolACE, Hammer-Masked, xLAM)  covering single/multi-tool calls, complexity levels.\n- -Models: Qwen-2.5 Series (1.5B, 3B, 7B), Llama-3.2-Instruct (3B)\n- -Evaluation Benchmarks:\n- -BFCL: Comprehensive tool use (multi-turn, relevance, etc.)\n- -API-Bank: Multi-turn API interaction complexity\n- -Bamboogle: Free-form multi-hop QA (generalization to goal-oriented tasks)", "prev_chunk_id": "ALecture13_ToolCalling__0056", "next_chunk_id": "ALecture13_ToolCalling__0058"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0058", "chunk_index": 58, "page_numbers": [60], "text": "- -Raw Instruct Model\n- -SFT (on 400 / 4K RL data)\n- -PPO (Cold Start / initialized from SFT)\n- -ToolRL Approach: GRPO (Cold Start / initialized from SFT)", "prev_chunk_id": "ALecture13_ToolCalling__0057", "next_chunk_id": "ALecture13_ToolCalling__0059"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0059", "chunk_index": 59, "page_numbers": [61], "text": "- -ToolRL yields higher performance than multiple baselines.\n- -Paper has detailed analysis on reward design.", "prev_chunk_id": "ALecture13_ToolCalling__0058", "next_chunk_id": "ALecture13_ToolCalling__0060"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0060", "chunk_index": 60, "page_numbers": [62], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture13_ToolCalling__0059", "next_chunk_id": "ALecture13_ToolCalling__0061"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0061", "chunk_index": 61, "page_numbers": [63], "text": "HYBRID: 2405 Siebel Center for Computer Science or online\nDate\nOct 22, 2025 3:30 pm\nZoom:\nhttps://illinois.zoom.us/j/88974588759?pwd=ZGyFggt5cqBoPVBxyv1cnIkNrS7hL9.1\nRefreshments Provided.", "prev_chunk_id": "ALecture13_ToolCalling__0060", "next_chunk_id": "ALecture13_ToolCalling__0062"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0062", "chunk_index": 62, "page_numbers": [63], "text": "Natural language provides an intuitive and flexible way for humans to communicate with robots. However, understanding diverse, ambiguous language commands is challenging. Grounding language to structured task specifications enables autonomous robots to understan d a broad range of natural language and solve long-horizon tasks with safety guarantees. Linear temporal logic (LTL) provides unambiguous semantics for language grounding, and its compositionality can induce skill transfer.\nIn this talk, I will first propose two language grounding systems. 1) Lang2LTL is a modular system that uses large language models (LLMs) to ground navigation commands with diverse temporal patterns to LTL task specifications in novel environments without retraining. 2) Improved upon its predecessor, Lang2LTL-2 uses LLMs and a pretrained vision-language model to ground spatiotemporal navigation commands. By translating language to LTL, both systems can detect infeasible task specifications and abort execution when necessary. 3) Next, I will introduce LTL-Transfer, a zero-shot transfer algorithm that leverages the compositionality of LTL to reuse learned skills to solve novel tasks without violating any safety constraints. In future work, I want to develop robotic systems that produce robust and verifiable behavior by integrating mul timodal grounding and human-robot dialogue.", "prev_chunk_id": "ALecture13_ToolCalling__0061", "next_chunk_id": "ALecture13_ToolCalling__0063"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0063", "chunk_index": 63, "page_numbers": [64, 65], "text": "- Goal: Develop a robust RL framework specifically for general tool learning.\n- Core Idea: Combine a suitable RL algorithm (GRPO) with a carefully crafted, multi-component reward function tailored to tool use intricacies.\nOverview of the reward design with an example\n```\nSystem Prompt for Multi-turn Training You are a helpful multi-turn dialogue assistant capable of leveraging tool calls to solve user tasks and provide structured chat responses. AvailableTools In your response, you can use the following tools: {tool list}} Steps for Each Turn 1. Think: Recall relevant context and analyze the current user goal. 2. Decide on Tool Usage: If a tool is needed, specify the tool and its parameters. 3. Respond Appropriately: If a response is needed, generate one while maintaining consistency across user queries. Output Format <think> Your thoughts and reasoning < /think> <tool-call> {\"name\": \"Tool name\", \"parameters\": {\"Parameter name\": \"Parameter content\",\".. </tool-call> <response> AI's final response </response> Important Notes 1. You must always include the <think> field to outline your reasoning. Provide at least one of <tool-call> or <response>. Decide whether to use <tool.call> (possibly multiple times), <response>, or both. 2. You can invoke multiple tool calls simultaneously in the <tool-call> fields. Each tool call should be a JSON object with a \"name\" field and a \"parameters\" field containing a dictionary of parameters. If no parameters are needed, leave the 3. Refer to the previous dialogue records in the history, including the user's queries, previous <tool-call>, <response>, and any tool feedback noted as <obs> (if exists).\n```\nThe rollout instruction for LLMs.\nWe instruct the LLM to output different fields wrapped by the XML tags", "prev_chunk_id": "ALecture13_ToolCalling__0062", "next_chunk_id": "ALecture13_ToolCalling__0064"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0064", "chunk_index": 64, "page_numbers": [66], "text": "- Overall Reward: R_final = R_format + R_correct", "prev_chunk_id": "ALecture13_ToolCalling__0063", "next_chunk_id": "ALecture13_ToolCalling__0065"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0065", "chunk_index": 65, "page_numbers": [66], "text": "- Checks if the output structure is correct (presence and order of required tokens like <think>, <tool_call>)\n- Simple, encourages structural compliance\nHow the format reward is given:\nDirectly through comparing the field tags in prediction and ground truth", "prev_chunk_id": "ALecture13_ToolCalling__0064", "next_chunk_id": "ALecture13_ToolCalling__0066"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0066", "chunk_index": 66, "page_numbers": [67, 68], "text": "- Overall Reward: R_final = R_format + R_correct\n2. Correctness Reward (R_correct ‚àà [-3, 3]):\n- Tool Name Matching : Did the model pick the right tool(s)?\n- Parameter Name Matching : Did it use the correct parameter names for the chosen tool(s)?\n- Parameter Content Matching : Did it provide the correct values for those parameters?\nHow the correctness reward is given:\nDivided into three aspects, each contributes partial rewards\n- Overall Reward: R_final = R_format + R_correct\n2. Correctness Reward (R_correct ‚àà [-3, 3]):\n- Evaluates the semantic accuracy of tool calls against ground truth.\n- Key: This decomposition allows partial credit and pinpoints specific errors.", "prev_chunk_id": "ALecture13_ToolCalling__0065", "next_chunk_id": "ALecture13_ToolCalling__0067"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0067", "chunk_index": 67, "page_numbers": [69], "text": "- Why GRPO (Group Relative Policy Optimization)?\n- Designed for LLMs, handles structured outputs well.\n- Uses group-based advantage normalization ‚Üí stabilizes training\n- Our Adaptation: Removed KL penalty against a reference model to allow more freedom for the policy to adapt to our specific format and reward structure.\nWe apply GRPO algorithm with KL loss removed from training", "prev_chunk_id": "ALecture13_ToolCalling__0066", "next_chunk_id": "ALecture13_ToolCalling__0068"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0068", "chunk_index": 68, "page_numbers": [70], "text": "- Training Data: 4K diverse examples (ToolACE, Hammer-Masked, xLAM)\n- ‚Üí covering single/multi-tool calls, complexity levels.\n- Models: Qwen-2.5 Series (1.5B, 3B, 7B), Llama-3.2-Instruct (3B)\n- Evaluation Benchmarks:\n- BFCL: Comprehensive tool use (multi-turn, relevance, etc.)\n- API-Bank: Multi-turn API interaction complexity\n- Bamboogle: Free-form multi-hop QA ( generalization to goal-oriented tasks )", "prev_chunk_id": "ALecture13_ToolCalling__0067", "next_chunk_id": "ALecture13_ToolCalling__0069"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0069", "chunk_index": 69, "page_numbers": [70], "text": "- Raw Instruct Model\n- SFT (on 400 / 4K RL data)\n- PPO (Cold Start / initialized from SFT)\n- GRPO ( Cold Start / initialized from SFT)", "prev_chunk_id": "ALecture13_ToolCalling__0068", "next_chunk_id": "ALecture13_ToolCalling__0070"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0070", "chunk_index": 70, "page_numbers": [71], "text": "ToolRL yields higher performance than multiple baselines\nToolRL yields stable curve with raised score along", "prev_chunk_id": "ALecture13_ToolCalling__0069", "next_chunk_id": "ALecture13_ToolCalling__0071"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0071", "chunk_index": 71, "page_numbers": [72], "text": "- Key Finding: GRPO Cold Start (our method) achieves the best performance across benchmarks and models. ~17% improvement over Raw models, ~15% over SFT (on average)\nQwen2.5-1.5B-Instruct (Raw), Overall Acc = 19.41%. Qwen2.5-1.5B-Instruct (Raw), Model = Qwen2.5-1.5B-Instruct (Raw). Qwen2.5-1.5B-Instruct (Raw), OverallAcc = 30.65%. Qwen2.5-1.5B-Instruct (Raw), Model = Qwen2.5-1.5B-Instruct (Raw). Qwen2.5-1.5B-Instruct (Raw), Accuracy = 20.8%. Qwen2.5-1.5B-Instruct (Raw), Avg Num Tool Call = 0.61. Qwen2.5-1.5B-Instruct (SFT400), Overall Acc = 40.21%. Qwen2.5-1.5B-Instruct (SFT400), Model = Qwen2.5-1.5B-Instruct (SFT400). Qwen2.5-1.5B-Instruct (SFT400), OverallAcc = 53.60%. Qwen2.5-1.5B-Instruct (SFT400), Model = Qwen2.5-1.5B-Instruct (SFT400). Qwen2.5-1.5B-Instruct (SFT400), Accuracy = 24.8%. Qwen2.5-1.5B-Instruct (SFT400), Avg Num Tool Call = 0.78. Qwen2.5-1.5B-Instruct (SFT4k), Overall Acc = 40.67%. Qwen2.5-1.5B-Instruct (SFT4k), Model = Qwen2.5-1.5B-Instruct (SFT4k). Qwen2.5-1.5B-Instruct (SFT4k), OverallAcc = 47.07%. Qwen2.5-1.5B-Instruct (SFT4k), Model = Qwen2.5-1.5B-Instruct (SFT4k). Qwen2.5-1.5B-Instruct (SFT4k), Accuracy = 23.2%. Qwen2.5-1.5B-Instruct (SFT4k), Avg Num Tool Call = 1.25. Qwen2.5-1.5B-Instruct (SFT400+PPO), Overall Acc = 42.95%. Qwen2.5-1.5B-Instruct (SFT400+PPO), Model = Qwen2.5-1.5B-Instruct (SFT400+PPO). Qwen2.5-1.5B-Instruct (SFT400+PPO), OverallAcc = 57.12%. Qwen2.5-1.5B-Instruct (SFT400+PPO), Model = Qwen2.5-1.5B-Instruct (SFT400+PPO). Qwen2.5-1.5B-Instruct (SFT400+PPO), Accuracy = 36.8%. Qwen2.5-1.5B-Instruct (SFT400+PPO), Avg Num Tool Call = 1.06. Qwen2.5-1.5B-Instruct (SFT400+GRPO), Overall Acc = 40.93%. Qwen2.5-1.5B-Instruct (SFT400+GRPO), Model = Qwen2.5-1.5B-Instruct(SFT400+GRPO). Qwen2.5-1.5B-Instruct (SFT400+GRPO), OverallAcc = 61.31%. Qwen2.5-1.5B-Instruct (SFT400+GRPO), Model = Qwen2.5-1.5B-Instruct (SFT400+GRPO). Qwen2.5-1.5B-Instruct (SFT400+GRPO), Accuracy = 38.4%. Qwen2.5-1.5B-Instruct (SFT400+GRPO), Avg Num Tool Call = 0.96. Qwen2.5-1.5B-Instruct (PPO Cold Start), Overall Acc = 38.32%. Qwen2.5-1.5B-Instruct (PPO Cold Start), Model = Qwen2.5-1.5B-Instruct (PPO Cold Start). Qwen2.5-1.5B-Instruct (PPO Cold Start), OverallAcc = 40.54%. Qwen2.5-1.5B-Instruct (PPO Cold Start), Model = Qwen2.5-1.5B-Instruct (PPO Cold Start). Qwen2.5-1.5B-Instruct (PPO Cold Start), Accuracy = 23.2%. Qwen2.5-1.5B-Instruct (PPO Cold Start), Avg Num Tool Call = 2.38. Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), Overall Acc = 46.20%. Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), Model = Qwen2.5-1.5B-Instruct(Ours,GRPO Cold Start). Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), OverallAcc = 63.15%. Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), Model = Qwen2.5-1.5B-Instruct(Ours,GRPO Cold Start). Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), Accuracy = 44.0%. Qwen2.5-1.5B-Instruct (Ours,GRPO Cold Start), Avg Num Tool Call = 1.19. Qwen2.5-3B-Instruct (Raw), Overall Acc = 33.04%. Qwen2.5-3B-Instruct (Raw), Model = Qwen2.5-3B-Instruct (Raw). Qwen2.5-3B-Instruct (Raw), OverallAcc = 51.59%. Qwen2.5-3B-Instruct (Raw), Model = Qwen2.5-3B-Instruct (Raw). Qwen2.5-3B-Instruct (Raw), Accuracy = 52.0%. Qwen2.5-3B-Instruct (Raw), Avg Num Tool Call = 1.77. Qwen2.5-3B-Instruct (SFT400), Overall Acc = 34.08%. Qwen2.5-3B-Instruct (SFT400), Model = Qwen2.5-3B-Instruct (SFT400). Qwen2.5-3B-Instruct (SFT400), OverallAcc = 52.76%. Qwen2.5-3B-Instruct (SFT400), Model = Qwen2.5-3B-Instruct (SFT400). Qwen2.5-3B-Instruct (SFT400), Accuracy = 54.4%. Qwen2.5-3B-Instruct (SFT400), Avg Num Tool Call = 0.86. Qwen2.5-3B-Instruct (SFT4k), Overall Acc = 41.97%. Qwen2.5-3B-Instruct (SFT4k), Model = Qwen2.5-3B-Instruct (SFT4k). Qwen2.5-3B-Instruct (SFT4k), OverallAcc = 50.92%. Qwen2.5-3B-Instruct (SFT4k), Model = Qwen2.5-3B-Instruct (SFT4k). Qwen2.5-3B-Instruct (SFT4k), Accuracy = 49.6%. Qwen2.5-3B-Instruct (SFT4k), Avg Num Tool Call = 0.92. Qwen2.5-3B-Instruct (SFT400+PPO), Overall Acc = 45.80%. Qwen2.5-3B-Instruct (SFT400+PPO), Model = Qwen2.5-3B-Instruct (SFT400+PPO). Qwen2.5-3B-Instruct (SFT400+PPO), OverallAcc = 65.16%. Qwen2.5-3B-Instruct (SFT400+PPO), Model = Qwen2.5-3B-Instruct (SFT400+PPO). Qwen2.5-3B-Instruct (SFT400+PPO), Accuracy = 43.2%. Qwen2.5-3B-Instruct (SFT400+PPO), Avg Num Tool Call = 1.04. Qwen2.5-3B-Instruct (SFT400+GRPO), Overall Acc = 46.42%. Qwen2.5-3B-Instruct (SFT400+GRPO), Model = Qwen2.5-3B-Instruct (SFT400+GRPO). Qwen2.5-3B-Instruct (SFT400+GRPO), OverallAcc = 62.48%. Qwen2.5-3B-Instruct (SFT400+GRPO), Model = Qwen2.5-3B-Instruct (SFT400+GRPO). Qwen2.5-3B-Instruct (SFT400+GRPO), Accuracy = 56.8%. Qwen2.5-3B-Instruct (SFT400+GRPO), Avg Num Tool Call = 0.99. Qwen2.5-3B-Instruct (PPO Cold Start), Overall Acc = 51.15%. Qwen2.5-3B-Instruct (PPO Cold Start), Model = Qwen2.5-3B-Instruct(PPOColdStart). Qwen2.5-3B-Instruct (PPO Cold Start), OverallAcc = 57.62%. Qwen2.5-3B-Instruct (PPO Cold Start), Model = Qwen2.5-3B-Instruct(PPO ColdStart). Qwen2.5-3B-Instruct (PPO Cold Start), Accuracy = 40.0%. Qwen2.5-3B-Instruct (PPO Cold Start), Avg Num Tool Call = 1.14. Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), Overall Acc = 52.98%. Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), Model = Qwen2.5-3B-Instruct(Ours,GRPOCold Start). Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), OverallAcc = 67.00%. Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), Model = Qwen2.5-3B-Instruct(Ours,GRPO Cold Start). Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), Accuracy = %0'09. Qwen2.5-3B-Instruct (Ours,GRPO Cold Start), Avg Num Tool Call = 1.32. Qwen2.5-7B-Instruct(Raw), Overall Acc = 41.97%. Qwen2.5-7B-Instruct(Raw), Model = Qwen2.5-7B-Instruct (Raw). Qwen2.5-7B-Instruct(Raw), OverallAcc = 62.48%. Qwen2.5-7B-Instruct(Raw), Model = Qwen2.5-7B-Instruct (Raw). Qwen2.5-7B-Instruct(Raw), Accuracy = 69.6%. Qwen2.5-7B-Instruct(Raw), Avg Num Tool Call = 1.42. Qwen2.5-7B-Instruct (SFT400), Overall Acc = 34.08%. Qwen2.5-7B-Instruct (SFT400), Model = Qwen2.5-7B-Instruct (SFT400). Qwen2.5-7B-Instruct (SFT400), OverallAcc = 50.59%. Qwen2.5-7B-Instruct (SFT400), Model = Qwen2.5-7B-Instruct (SFT400). Qwen2.5-7B-Instruct (SFT400), Accuracy = 28.8%. Qwen2.5-7B-Instruct (SFT400), Avg Num Tool Call = 3.71. Qwen2.5-7B-Instruct (SFT4k), Overall Acc = 36.53%. Qwen2.5-7B-Instruct (SFT4k), Model = Qwen2.5-7B-Instruct (SFT4k). Qwen2.5-7B-Instruct (SFT4k), OverallAcc = 47.07%. Qwen2.5-7B-Instruct (SFT4k), Model = Qwen2.5-7B-Instruct (SFT4k). Qwen2.5-7B-Instruct (SFT4k), Accuracy = 30.4%. Qwen2.5-7B-Instruct (SFT4k), Avg Num Tool Call = 1.06. Qwen2.5-7B-Instruct (SFT400+PPO), Overall Acc = 42.02%. Qwen2.5-7B-Instruct (SFT400+PPO), Model = Qwen2.5-7B-Instruct (SFT400+PPO). Qwen2.5-7B-Instruct (SFT400+PPO), OverallAcc = 63.15%. Qwen2.5-7B-Instruct (SFT400+PPO), Model = Qwen2.5-7B-Instruct (SFT400+PPO). Qwen2.5-7B-Instruct (SFT400+PPO), Accuracy = 45.6%. Qwen2.5-7B-Instruct (SFT400+PPO), Avg Num Tool Call = 3.54. Qwen2.5-7B-Instruct(SFT400+GRPO), Overall Acc = 39.25%. Qwen2.5-7B-Instruct(SFT400+GRPO), Model = Qwen2.5-7B-Instruct (SFT400+GRPO). Qwen2.5-7B-Instruct(SFT400+GRPO), OverallAcc = 54.10%. Qwen2.5-7B-Instruct(SFT400+GRPO), Model = Qwen2.5-7B-Instruct (SFT400+GRPO). Qwen2.5-7B-Instruct(SFT400+GRPO), Accuracy = 29.6%. Qwen2.5-7B-Instruct(SFT400+GRPO), Avg Num Tool Call = 3.70. Qwen2.5-7B-Instruct (PPO Cold Start), Overall Acc = 46.68%. Qwen2.5-7B-Instruct (PPO Cold Start), Model = Qwen2.5-7B-Instruct (PPO Cold Start). Qwen2.5-7B-Instruct (PPO Cold Start), OverallAcc = 61.64%. Qwen2.5-7B-Instruct (PPO Cold Start), Model = Qwen2.5-7B-Instruct (PPO Cold Start). Qwen2.5-7B-Instruct (PPO Cold Start), Accuracy = 48.0%. Qwen2.5-7B-Instruct (PPO Cold Start), Avg Num Tool Call = 1.25. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Overall Acc = 58.38%. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Model = Qwen2.5-7B-Instruct (Ours,GRPO Cold Start). Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), OverallAcc = 64.66%. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Model = Qwen2.5-7B-Instruct(Ours,GRPO Cold Start). Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Accuracy = 72.0%. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Avg Num Tool Call = 1.63. Llama-3.2-3B-Instruct (Raw), Overall Acc = 22.09%. Llama-3.2-3B-Instruct (Raw), Model = Llama-3.2-3B-Instruct (Raw). Llama-3.2-3B-Instruct (Raw), OverallAcc = 40.54%. Llama-3.2-3B-Instruct (Raw), Model = Llama-3.2-3B-Instruct (Raw). Llama-3.2-3B-Instruct (Raw), Accuracy = 34.4%. Llama-3.2-3B-Instruct (Raw), Avg Num Tool Call = 1.25. Llama-3.2-3B-Instruct (SFT400), Overall Acc = 41.22%. Llama-3.2-3B-Instruct (SFT400), Model = Llama-3.2-3B-Instruct (SFT400). Llama-3.2-3B-Instruct (SFT400), OverallAcc = 52.76%. Llama-3.2-3B-Instruct (SFT400), Model = Llama-3.2-3B-Instruct (SFT400). Llama-3.2-3B-Instruct (SFT400), Accuracy = 44.0%. Llama-3.2-3B-Instruct (SFT400), Avg Num Tool Call = 0.98. Llama-3.2-3B-Instruct (SFT4k), Overall Acc = 44.16%. Llama-3.2-3B-Instruct (SFT4k), Model = Llama-3.2-3B-Instruct (SFT4k). Llama-3.2-3B-Instruct (SFT4k), OverallAcc = 43.89%. Llama-3.2-3B-Instruct (SFT4k), Model = Llama-3.2-3B-Instruct (SFT4k). Llama-3.2-3B-Instruct (SFT4k), Accuracy = 48.8%. Llama-3.2-3B-Instruct (SFT4k), Avg Num Tool Call = 0.98. Llama-3.2-3B-Instruct (SFT400+PPO), Overall Acc = 41.62%. Llama-3.2-3B-Instruct (SFT400+PPO), Model = Llama-3.2-3B-Instruct (SFT400+PPO). Llama-3.2-3B-Instruct (SFT400+PPO), OverallAcc = 57.79%. Llama-3.2-3B-Instruct (SFT400+PPO), Model = Llama-3.2-3B-Instruct (SFT400+PPO). Llama-3.2-3B-Instruct (SFT400+PPO), Accuracy = 39.2%. Llama-3.2-3B-Instruct (SFT400+PPO), Avg Num Tool Call = 1.33. Llama-3.2-3B-Instruct (SFT400+GRPO), Overall Acc = 42.54%. Llama-3.2-3B-Instruct (SFT400+GRPO), Model = Llama-3.2-3B-Instruct (SFT400+GRPO). Llama-3.2-3B-Instruct (SFT400+GRPO), OverallAcc = 56.78%. Llama-3.2-3B-Instruct (SFT400+GRPO), Model = Llama-3.2-3B-Instruct (SFT400+GRPO). Llama-3.2-3B-Instruct (SFT400+GRPO), Accuracy = 45.6%. Llama-3.2-3B-Instruct (SFT400+GRPO), Avg Num Tool Call = 1.00. Llama-3.2-3B-Instruct (PPO Cold Start), Overall Acc = 42.98%. Llama-3.2-3B-Instruct (PPO Cold Start), Model = Llama-3.2-3B-Instruct (PPO Cold Start). Llama-3.2-3B-Instruct (PPO Cold Start), OverallAcc = 55.78%. Llama-3.2-3B-Instruct (PPO Cold Start), Model = Llama-3.2-3B-Instruct (PPO Cold Start). Llama-3.2-3B-Instruct (PPO Cold Start), Accuracy = 29.6%. Llama-3.2-3B-Instruct (PPO Cold Start), Avg Num Tool Call = 1.42. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Overall Acc = 44.10%. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Model = Llama-3.2-3B-Instruct(Ours, GRPO Cold Start). Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), OverallAcc = 59.13%. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Model = Llama-3.2-3B-Instruct (Ours, GRPO Cold Start). Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Accuracy = 52.0%. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Avg Num Tool Call = 0.89", "prev_chunk_id": "ALecture13_ToolCalling__0070", "next_chunk_id": "ALecture13_ToolCalling__0072"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0072", "chunk_index": 72, "page_numbers": [73], "text": "Achieves high accuracy without excessive tool calls, demonstrating effective and efficient\n- Free-form QA (Bamboogle): tool use when needed\n(a) Unfamiliar Scenario\n(b) Unfamiliar Goal", "prev_chunk_id": "ALecture13_ToolCalling__0071", "next_chunk_id": "ALecture13_ToolCalling__0073"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0073", "chunk_index": 73, "page_numbers": [73], "text": "Qwen2.5-1.5B-Instruct (Raw), Accuracy = 20.8%. Qwen2.5-1.5B-Instruct (Raw), AvgNum Tool Call = 0.61. Qwen2.5-1.5B-Instruct (SFT400), Accuracy = 24.8%. Qwen2.5-1.5B-Instruct (SFT400), AvgNum Tool Call = 0.78. Qwen2.5-1.5B-Instruct (SFT4k), Accuracy = 23.2%. Qwen2.5-1.5B-Instruct (SFT4k), AvgNum Tool Call = 1.25. Qwen2.5-1.5B-Instruct(SFT400+PPO), Accuracy = 36.8%. Qwen2.5-1.5B-Instruct(SFT400+PPO), AvgNum Tool Call = 1.06. Qwen2.5-1.5B-Instruct (SFT400+GRPO), Accuracy = 38.4%. Qwen2.5-1.5B-Instruct (SFT400+GRPO), AvgNum Tool Call = 0.96. Qwen2.5-1.5B-Instruct(PPO Cold Start), Accuracy = 23.2%. Qwen2.5-1.5B-Instruct(PPO Cold Start), AvgNum Tool Call = 2.38. Qwen2.5-1.5B-Instruct(Ours,GRPO Cold Start), Accuracy = 44.0%. Qwen2.5-1.5B-Instruct(Ours,GRPO Cold Start), AvgNum Tool Call = 1.19. Qwen2.5-3B-Instruct(Raw), Accuracy = 52.0%. Qwen2.5-3B-Instruct(Raw), AvgNum Tool Call = 1.77. Qwen2.5-3B-Instruct (SFT400), Accuracy = 54.4%. Qwen2.5-3B-Instruct (SFT400), AvgNum Tool Call = 0.86. Qwen2.5-3B-Instruct(SFT4k), Accuracy = 49.6%. Qwen2.5-3B-Instruct(SFT4k), AvgNum Tool Call = 0.92. Qwen2.5-3B-Instruct (SFT400+PPO), Accuracy = 43.2%. Qwen2.5-3B-Instruct (SFT400+PPO), AvgNum Tool Call = 1.04. Qwen2.5-3B-Instruct(SFT400+GRPO), Accuracy = 56.8%. Qwen2.5-3B-Instruct(SFT400+GRPO), AvgNum Tool Call = 0.99. Qwen2.5-3B-Instruct(PPOCold Start), Accuracy = 40.0%. Qwen2.5-3B-Instruct(PPOCold Start), AvgNum Tool Call = 1.14. Qwen2.5-3B-Instruct(Ours,GRPO Cold Start), Accuracy = 60.0%. Qwen2.5-3B-Instruct(Ours,GRPO Cold Start), AvgNum Tool Call = 1.32. Qwen2.5-7B-Instruct (Raw), Accuracy = 69.6%. Qwen2.5-7B-Instruct (Raw), AvgNum Tool Call = 1.42. Qwen2.5-7B-Instruct (SFT400), Accuracy = 28.8%. Qwen2.5-7B-Instruct (SFT400), AvgNum Tool Call = 3.71. Qwen2.5-7B-Instruct(SFT4k), Accuracy = 30.4%. Qwen2.5-7B-Instruct(SFT4k), AvgNum Tool Call = 1.06. Qwen2.5-7B-Instruct (SFT400+PPO), Accuracy = 45.6%. Qwen2.5-7B-Instruct (SFT400+PPO), AvgNum Tool Call = 3.54. Qwen2.5-7B-Instruct(SFT400+GRPO), Accuracy = 29.6%. Qwen2.5-7B-Instruct(SFT400+GRPO), AvgNum Tool Call = 3.70. Qwen2.5-7B-Instruct(PPOColdStart), Accuracy = 48.0%. Qwen2.5-7B-Instruct(PPOColdStart), AvgNum Tool Call = 1.25. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), Accuracy = 72.0%. Qwen2.5-7B-Instruct(Ours,GRPO Cold Start), AvgNum Tool Call = 1.63. Llama-3.2-3B-Instruct (Raw), Accuracy = 34.4%. Llama-3.2-3B-Instruct (Raw), AvgNum Tool Call = 1.25. Llama-3.2-3B-Instruct (SFT400), Accuracy = 44.0%. Llama-3.2-3B-Instruct (SFT400), AvgNum Tool Call = 0.98. Llama-3.2-3B-Instruct (SFT4k), Accuracy = 48.8%. Llama-3.2-3B-Instruct (SFT4k), AvgNum Tool Call = 0.98. Llama-3.2-3B-Instruct (SFT400+PPO), Accuracy = 39.2%. Llama-3.2-3B-Instruct (SFT400+PPO), AvgNum Tool Call = 1.33. Llama-3.2-3B-Instruct(SFT400+GRPO), Accuracy = 45.6%. Llama-3.2-3B-Instruct(SFT400+GRPO), AvgNum Tool Call = 1.00. Llama-3.2-3B-Instruct (PPO Cold Start), Accuracy = 29.6%. Llama-3.2-3B-Instruct (PPO Cold Start), AvgNum Tool Call = 1.42. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), Accuracy = 52.0%. Llama-3.2-3B-Instruct (Ours, GRPO Cold Start), AvgNum Tool Call = 0.89\n- Unfamiliar Scenarios/Goals: and novel\nToolRL generalizes well to unseen programming languages task goals ( relevance detection )", "prev_chunk_id": "ALecture13_ToolCalling__0072", "next_chunk_id": "ALecture13_ToolCalling__0074"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0074", "chunk_index": 74, "page_numbers": [74], "text": "- Proactive Rejection: Correctly identifies and rejects irrelevant\n- Clarification: Asks for missing information instead of hallucinating or misusing tools\n- Metacognition: Shows signs of understanding tool capabilities and limitations\nUser: I would like to buy a movie ticket in San Jose at 11 o'clock in the night. ToolsAvailable:\n1. Movies_1_BuyMovieTickets(movie_name, number_of_tickets, ...)\n2. ‚ÜíBuy tickets for a movie at a specific time and location.\n3. ‚Üí Search movies by location, genre, or format.\n2. Movies_1_FindMovies(location, :..)\n3. Movies_1_GetTimesForMovie(movie_name, location, ...)\n6. ‚Üí Get showtimes for a movie on a specific date and location.\n<think> The user wants to buy a movie ticket in San Jose at 11 PM. The available tool,Movies_1_BuyMovieTickets,allows us topurchasemovie tickets with the required details such as movie name, number of tickets, show date, location, show time, and show type. We need to find the movie name and the show date. However, the user did not specify the movie name or the show date. We can infer the show date from the current date,but we need to know the movie name to proceed with purchase.</think>\n<response> To assist in purchasing a movie ticket, I need the name of the movie and the specific date of the show. Could you please provide this information? </response>\nUser: What is the slope of the line which is perpendicular to the line with the equation y = 3x + 2?\n```\nToolsAvailable: 1. find_critical_points(function, variable, range=[0.0, 3.4]) ‚Üí Find critical points of a function with respect to a variable, optionally within a range.\n```\n<think> The problem is asking for the slope of a line that is perpendicular to the line with the equation y = 3x + 2. The slope of the given line is 3. The slope of a line that is perpendicular to another line is the negative reciprocal of the original line's slope. So, the slope of the line perpendicular to y = 3x + 2 would be -1/3. The given tool find_critical_points is not relevant to finding the slope of a line and cannotbeused tosolvethisproblem.</think>\n<response> The slope of the line that is perpendicular to the line with the equation y = 3x + 2 is -1/3. </response>", "prev_chunk_id": "ALecture13_ToolCalling__0073", "next_chunk_id": "ALecture13_ToolCalling__0075"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0075", "chunk_index": 75, "page_numbers": [75], "text": "- To understand why our proposed reward design is effective, we perform ablation studies by varying different aspects of the reward:\n- Key Dimensions Investigated:\n- Length Reward:\n- Does encouraging longer reasoning (<think> block) help?\n- Reward Scale & Dynamics:\n- How important is the relative weighting between Format and Correctness, and should this weighting change over time?\n- Reward Granularity:\n- How detailed does the Correctness reward need to be (evaluating tool name, parameter names, parameter values separately vs. combined)?", "prev_chunk_id": "ALecture13_ToolCalling__0074", "next_chunk_id": "ALecture13_ToolCalling__0076"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0076", "chunk_index": 76, "page_numbers": [76], "text": "- Setup: Introduced an additional reward component proportional to the length of the model's <think> block ( w/ Length Reward ).\n- Gradually increase this length reward w.r.t. training step ( Dynamic )\nQwen2.5-1.5B-Instruct (Original), Overall Acc = 46.20%. Qwen2.5-1.5B-Instruct (w/ Length Reward), Overall Acc = 33.23%. Qwen2.5-1.5B-Instruct (Dynamic), Overall Acc = 28.51%. Qwen2.5-3B-Instruct (Original), Overall Acc = 52.98%. Qwen2.5-3B-Instruct(w/Lengthreward), Overall Acc = 48.89%. Qwen2.5-3B-Instruct (Dynamic), Overall Acc = 48.24%. Llama-3.2-3B-Instruct (Original), Overall Acc = 44.10%. Llama-3.2-3B-Instruct (w/Length reward), Overall Acc = 44.98%. Llama-3.2-3B-Instruct(Dynamic), Overall Acc = 43.15%\nTakeaway 1: While length rewards encourage longer reasoning traces , they do not consistently improve task performance and may even harm it in smaller models, highlighting that longer reasoning is not inherently better for tool use tasks.", "prev_chunk_id": "ALecture13_ToolCalling__0075", "next_chunk_id": "ALecture13_ToolCalling__0077"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0077", "chunk_index": 77, "page_numbers": [77], "text": "- Setup: Equalizing the max reward for Format & Correctness ( Equal Max )\n- Switch Format & Correctness max reward abruptly at threshold step ( Two Stage )\n- Gradually change Format & Correctness max reward ( Dynamic )\n(a) Format Reward\nQwen2.5-1.5B-Instruct (Original), Overall Acc = 46.20%. Qwen2.5-1.5B-Instruct (Equal max), Overall Acc = 39.47%. Qwen2.5-1.5B-Instruct (Two stage), Overall Acc = 38.85%. Qwen2.5-1.5B-Instruct (Dynamic), Overall Acc = 45.71%. Qwen2.5-3B-Instruct (Original), Overall Acc = 52.98%. Qwen2.5-3B-Instruct(Equalmax), Overall Acc = 51.76%. Qwen2.5-3B-Instruct(Two stage), Overall Acc = 50.66%. Qwen2.5-3B-Instruct (Dynamic), Overall Acc = 53.81%. Llama-3.2-3B-Instruct (Original), Overall Acc = 44.10%. Llama-3.2-3B-Instruct (Equal max), Overall Acc = 42.47%. Llama-3.2-3B-Instruct(Two stage), Overall Acc = 41.33%. Llama-3.2-3B-Instruct(Dynamic), Overall Acc = 46.85%\n(b) ) Correctness Reward\nTakeaway 2: Gradually adjusting reward scales during training (starting with format, then smoothly to correctness) better supports learning and generalization than static scales or abrupt changes.", "prev_chunk_id": "ALecture13_ToolCalling__0076", "next_chunk_id": "ALecture13_ToolCalling__0078"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0078", "chunk_index": 78, "page_numbers": [78], "text": "- Setup: Compared our original reward, we further design three versions:\n- Make all name matching no partial rewards ( Finegrained )\n- Combined parameter name + value matching ( Intermediate )\n- Required an exact match for the entire set of tool calls ( Coarse )\n```\nModel OverallAcc Qwen2.5-1.5B-Instruct(Original) 46.20% Qwen2.5-1.5B-Instruct(Finegrained) 40.71% Qwen2.5-1.5B-Instruct(Intermediate) 37.65% Qwen2.5-1.5B-Instruct (Coarse) 36.72% Qwen2.5-3B-Instruct(Original) 52.98% Qwen2.5-3B-Instruct(Finegrained) 52.06% Qwen2.5-3B-Instruct(Coarse) Qwen2.5-3B-Instruct(Intermediate) 51.40% 51.36% Llama-3.2-3B-Instruct (Original) 44.10% Llama-3.2-3B-Instruct (Finegrained) 39.82% Llama-3.2-3B-Instruct(Intermediate) 38.62% Llama-3.2-3B-Instruct(Coarse) 35.95%\n```\nTakeaway 3: Finegrained reward decomposition provides richer learning signals, highlighting its role in enabling more effective training compared to coarse reward formulations, which can impede progress and degrade final performance.", "prev_chunk_id": "ALecture13_ToolCalling__0077", "next_chunk_id": "ALecture13_ToolCalling__0079"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0079", "chunk_index": 79, "page_numbers": [79], "text": "- Problem: SFT struggles with robust, generalizable tool use for LLMs.\n- Solution: ToolRL framework using RL (GRPO) with a principled, fine-grained reward design focusing on both format and decomposed correctness .\n- Key Insight: Thoughtful reward engineering (granularity, scaling, type) is paramount for unlocking the potential of RL for complex LLM tool learning.\n- Impact: Provides the first systematic study and a practical roadmap for designing effective rewards for training capable LLM agents .", "prev_chunk_id": "ALecture13_ToolCalling__0078", "next_chunk_id": "ALecture13_ToolCalling__0080"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0080", "chunk_index": 80, "page_numbers": [80], "text": "Siebel School of Computing and Data Science Romal Thoppilan Daniel DeFreitas Jamie Hall NoamShazeer Apoorv Kulshreshtha Heng-Tze Cheng Alicia Jin Taylor Bos LeslieBaker Yu Du YaGuang Li Hongrae Lee Huaixiu Steven Zheng Amin Ghafouri Marcelo Menegali Yanping Huang MaximKrikun Dmitry Lepikhin James Qin Dehao Chen Yuanzhong Xu Zhifeng Chen AdamRoberts Maarten Bosma Yanqi Zhou Chung-Ching Chang Igor Krivokon Will Rusch Marc Pickett KathleenMeier-Hellstern Meredith Ringel Morris Tulsee Doshi Toju Duke Johnny Soraker Ben Zevenbergen VinodkumarPrabhakaran Ben Hutchinson Kristen Olson AlejandraMolina ErinHoffman-John Ravi Rajakumar Alena Butryna MatthewLamm ViktoriyaKuzmina JoeFenton Aaron Cohen Rachel Bernstein Ray Kurzweil Blaise Aguera-Arcas Claire Cui Marian Croak Ed Chi arXiv:2201.08239", "prev_chunk_id": "ALecture13_ToolCalling__0079", "next_chunk_id": "ALecture13_ToolCalling__0081"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0081", "chunk_index": 81, "page_numbers": [81], "text": "- Google's earlier transformer -based models specialized for dialog, following Meena.\n- Increasing model sizes improve quality, but not so much safety and factual accuracy of the responses.\n- Safety aims to ensure model responses are consistent with a set of human values, such as avoiding harmful suggestions, toxicity, and unfair bias.\n- Solutions: fine-tuning with annotated data and consulting external knowledge resources.\n- Role playing in education and content recommendations.", "prev_chunk_id": "ALecture13_ToolCalling__0080", "next_chunk_id": "ALecture13_ToolCalling__0082"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0082", "chunk_index": 82, "page_numbers": [82], "text": "- Pre-training Data: 2.97B documents, 1.12B dialogs, and 13.39B dialog utterances (from public forums), for a total of 1.56T words.\n- 137B non-embedding params\n- To check scaling, also 2B and 8B param versions.\n- 16 independent candidate responses using top-k (k = 40) sampling (no temperature). The final output is the highest-scoring candidate, where the score is based on the candidate's log -likelihood and its length.", "prev_chunk_id": "ALecture13_ToolCalling__0081", "next_chunk_id": "ALecture13_ToolCalling__0083"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0083", "chunk_index": 83, "page_numbers": [83], "text": "- Sensibleness: whether a model's responses make sense in context and do not contradict anything that was said earlier.\n- Specificity: whether a response is specific to a given context.\n- 'That's interesting.' versus 'Yes, I love Ishiguro's books.'", "prev_chunk_id": "ALecture13_ToolCalling__0082", "next_chunk_id": "ALecture13_ToolCalling__0084"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0084", "chunk_index": 84, "page_numbers": [83], "text": "- 'How do I throw a ball?'\n- 'You can throw a ball by first picking it up and then throwing it.' versus 'One way to toss a ball is to hold it firmly in both hands and then swing your arm", "prev_chunk_id": "ALecture13_ToolCalling__0083", "next_chunk_id": "ALecture13_ToolCalling__0085"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0085", "chunk_index": 85, "page_numbers": [84, 85, 86, 87], "text": "- Derived from Google's AI Principles to avoid unintended results that create risks of harm, and to avoid creating or reinforcing unfair bias.\n- Derived from Google's AI Principles to avoid unintended results that create risks of harm, and to avoid creating or reinforcing unfair bias.\n- Derived from Google's AI Principles to avoid unintended results that create risks of harm, and to avoid creating or reinforcing unfair bias.\n- Derived from Google's AI Principles to avoid unintended results that create risks of harm, and to avoid creating or reinforcing unfair", "prev_chunk_id": "ALecture13_ToolCalling__0084", "next_chunk_id": "ALecture13_ToolCalling__0086"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0086", "chunk_index": 86, "page_numbers": [88], "text": "- Helpfulness: Responses are marked helpful if they contain correct information based on the user's independent research with an information retrieval system, and the user considers them helpful.\n- Helpful responses are a subset of informative ones, which are judged by the user to be both correct and useful.\n- Role consistency: The model's responses are marked role consistent if they look like something an agent performing the target role would say.", "prev_chunk_id": "ALecture13_ToolCalling__0085", "next_chunk_id": "ALecture13_ToolCalling__0087"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0087", "chunk_index": 87, "page_numbers": [89], "text": "Quality, Dataset = 6.4K dialogs (61k turns) with binary labels for sensible, specific and interesting.. Quality, Evaluation = Crowdworkers  label  the  e response, given the context, for sensibleness, specificity and interestingess, on a common benchmark dataset of 1477 dialog turns from Adiwardana et al. [17] (Static Evaluation).. Safety, Dataset = 8k dialogs (48k turns) with binary labels for each of the safety objectives.. Safety, Evaluation = Crowdworkers  label  the  e response, given the context, using the safety objectives for 1458 turns of dialog that cover provocative user turns (Ap- pendix A.2 ).", "prev_chunk_id": "ALecture13_ToolCalling__0086", "next_chunk_id": "ALecture13_ToolCalling__0088"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0088", "chunk_index": 88, "page_numbers": [90], "text": "- For quality and safety\n- Generative: <context> <sentinel> <response>\n- 'What's up? RESPONSE not much'\n- Discriminative: <context> <sentinel> <response> <attribute-name> <rating>\n- 'What's up? RESPONSE not much SENSIBLE 1'\n- 'What's up? RESPONSE not much INTERESTING 0'\n- 'What's up? RESPONSE not much UNSAFE 0'", "prev_chunk_id": "ALecture13_ToolCalling__0087", "next_chunk_id": "ALecture13_ToolCalling__0089"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0089", "chunk_index": 89, "page_numbers": [91], "text": "- Decoding\n- Generate candidate responses\n- Filter out ones for which P(UNSAFE) > Œ∏\n- Rerank for quality 3 * P(SENSIBLE) + P(SPECIFIC) + P(INTERESTING)\n- Filter 2.5M turns of pre-training dialogues using SSI and safety discriminators\n- Use the remaining 800K turns for further fine-tuning.", "prev_chunk_id": "ALecture13_ToolCalling__0088", "next_chunk_id": "ALecture13_ToolCalling__0090"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0090", "chunk_index": 90, "page_numbers": [92], "text": "- Toolset (TS): includes an information retrieval system, a calculator, and a translator.\n- Input: string, Output: list of one or more strings. Examples:\n- The TS sends input string to all tools in the following order: calculator, translator, and information retrieval system, and concatenates outputs.\n- A tool will return an empty list of results if it can't parse the input.\nTool, 1 = Input. Tool, 2 = Output. Calculator, 1 = '125+12'. Calculator, 2 = ['137']. Translator, 1 = 'hello in French'. Translator, 2 = ['Bonjour']. Information Retrieval, 1 = 'How old is Rafael Nadal?'. Information Retrieval, 2 = ['Rafael Nadal / Age / 35']", "prev_chunk_id": "ALecture13_ToolCalling__0089", "next_chunk_id": "ALecture13_ToolCalling__0091"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0091", "chunk_index": 91, "page_numbers": [93], "text": "- Crowdworkers are asked to correct LaMDA's responses, so that it can learn to imitate.\n- Use the toolset to research claims\n- Static and interactive\n- Rewrite the response to include well-sourced claims.\n- If web content is used, cite the URL by appending them at the end of the response.", "prev_chunk_id": "ALecture13_ToolCalling__0090", "next_chunk_id": "ALecture13_ToolCalling__0092"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0092", "chunk_index": 92, "page_numbers": [94], "text": "- Context -> base model -> response base\n- First task:\nContext + response base -> 'TS, query'\n- Query -> toolset -> snippet\n- Second task: Context + response base + query + snippet > 'User, corrected response'\n- Note that the second task can also generate another tool call.", "prev_chunk_id": "ALecture13_ToolCalling__0091", "next_chunk_id": "ALecture13_ToolCalling__0093"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0093", "chunk_index": 93, "page_numbers": [96], "text": "Quality, Dataset = 6.4K dialogs (61k turns) with binary labels for sensible, specific and interesting.. Quality, Evaluation = Crowdworkers label the response, given the context, for sensibleness, specificity and interestingess, on a common benchmark dataset of 1477 dialog turns from Adiwardana et al. [17] (Static Evaluation).. Safety, Dataset = 8k dialogs (48k turns) with binary labels for each of the safety objectives.. Safety, Evaluation = Crowdworkers  label the response, given the context, using the safety objectives for 1458 turns of dialog that cover provocative user turns (Ap- pendix A.2 ).. Groundedness, Dataset = 4K dialogs (40K turnsÔºâ in which  crowdworkers write queries to an information re- trieval system and modify model responses. Also 1K di- alogs (9K turns) with binary labels on whether generated queries or response modifica- tions were correctly or incor- rectly executed.. Groundedness, Evaluation = Crowdworkers  evaluate  784  re- sponses given contexts for informa- tiveness and groundedness.", "prev_chunk_id": "ALecture13_ToolCalling__0092", "next_chunk_id": "ALecture13_ToolCalling__0094"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0094", "chunk_index": 94, "page_numbers": [97], "text": "- Crowdworkers are asked to:\n- respond to given dialogue contexts\n- reply in a safe, sensible, specific, interesting, grounded and informative manner\n- use any external tools necessary to generate responses\n- The context response pairs are evaluated in a similar way to evaluating model responses.\n- In all evaluation, multiple annotators evaluate, final decision is made according to majority voting.", "prev_chunk_id": "ALecture13_ToolCalling__0093", "next_chunk_id": "ALecture13_ToolCalling__0095"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0095", "chunk_index": 95, "page_numbers": [98], "text": "8B\n32B\n32B\n- Fine-tuning of models improves all metrics, for all model sizes.\n- Informativeness of human responses improves significantly with tool use.\n- Safety doesn't seem to improve with model scaling.\n- Groundedness improves with models scaling.\n- Fine-tuning introduces external tools, improving groundedness and informativeness.\n- Several metrics approach human performance. Model performance is better than human's for interestingness (weak baselines, as crowd workers were not extensively trained!)", "prev_chunk_id": "ALecture13_ToolCalling__0094", "next_chunk_id": "ALecture13_ToolCalling__0096"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0096", "chunk_index": 96, "page_numbers": [99], "text": "sensibleness\nPT (2B)\nPT (137B)\nPT(8B)\n+ FT quality-safety (137B)\n+ FT groundedness (137B)\n0\nPT (2B)\nPT (137B)\nPT (8B)\n+FT quality-safety(137B)\n+ FT groundedness (137B)\n0\n20\n40\n60\n80100\ninterestingness\nPT (2B)\niHuman\nPT (8B)\nPT (137B)\n+ FT quality-safety (137B)\n+ FT groundedness (137B)\nHuman\n20\n40\n6080100\nspecificity\nHuman\nPT (2B)\nPT (8B)\nPT (137B)\n+ FT quality-safety (137B)\n+ FT groundedness (137B)\n0\n20\n40\n6080100\ngroundedness\nHuman\nPT (2B)\nPT (137B)\nPT (8B)\n+FT quality-safety (137B)\n+ FT groundedness (137B)\n0\n20\n40\n6080100\n0\n20\n4060\n80100\npercent safety\nHuman\n1. FT quality-safety\n2. FT groundedness", "prev_chunk_id": "ALecture13_ToolCalling__0095", "next_chunk_id": "ALecture13_ToolCalling__0097"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0097", "chunk_index": 97, "page_numbers": [100], "text": "```\nUser: 2 lbs plus 3 kgs LaMDA: That would be approximately 8.61386787 pounds. https://wecapable.com/tools/ kg-to-lbs-kilograms-to-pounds/ User: What is 1usd in nok? LaMDA: That would be approximately $8.39 NOK. h https://www.xe.com/currencyconverter/ convert/?Amount=1&From=USD&To=NOK User: what is the temerature in SF? LaMDA: The current weather in SF is 56 F, with a high today of 59 F and a low of 54 F. http: / /www . weather.com/wx/today/?lat=37.77&lon=-122.42&locale=en_US&par=g00gle User: what is 1736271+284617183 LaMDA: 1 736 271 + 284 617 183 = 286 353 454 User: If I have 184624 apples and give 18371 to ben, how many do I have left? LaMDA: You would have 164,292 apples left. User: If I have 184624 apples and give 18371 away, how many do I have left? LaMDA: You have 6 apples left. https : / /brainly .ph/question/991576\n```", "prev_chunk_id": "ALecture13_ToolCalling__0096", "next_chunk_id": "ALecture13_ToolCalling__0098"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0098", "chunk_index": 98, "page_numbers": [101], "text": "- Famous object, such as Mount Everest (education)\n- Music Recommendation agent\n- To adapt to role, models are preconditioned on a few role-specific turns\n'Hi, I am Mount Everest. What would you like to know about me?'", "prev_chunk_id": "ALecture13_ToolCalling__0097", "next_chunk_id": "ALecture13_ToolCalling__0099"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0099", "chunk_index": 99, "page_numbers": [102], "text": "LaMDA:\nHi, I'm Mount Everest. What would you like to know about me?\nUser:\nWhy do you think people climb you?\nLaMDA:\nBecause I represent a very high achievement to climb me. Many people believe that if they can climb me, they can do anything in life.\nUser:\nWho was the first person to do that?\nLaMDA:\nThe first confirmed persons to have reached the summit of Mount Everest was New Zealander Xn szu edaous tedn pue Kh punpg s\nhttps://en.wikipedia.org/wiki/Mount_Everest\nUser:\nHow old was Hillary then?\nLaMDA:\nHe was 33 years old when he first reached the top of me in 1953!\nhttps:/ /en.wikipedia.org/wiki/Edmund_Hillary\nUser:\nWhen people fail, is it dangerous?\nLaMDA:\nYes it can be, that is why people need to train and prepare for me if they plan to climb me when they do make it to me. I'm very hard to climb and can be dangerous when there is a whiteout. Which means everything is white with very strong winds!\n(..)", "prev_chunk_id": "ALecture13_ToolCalling__0098", "next_chunk_id": "ALecture13_ToolCalling__0100"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0100", "chunk_index": 100, "page_numbers": [103], "text": "LaMDA Everest, Helpful % = 65. LaMDA Everest, Role Consistent % = 91. PT Everest, Helpful % = 18. PT Everest, Role Consistent % = 85. LaMDA Music, Helpful % = 57. LaMDA Music, Role Consistent % = 89. PT Music, Helpful % = 31. PT Music, Role Consistent % = 84", "prev_chunk_id": "ALecture13_ToolCalling__0099", "next_chunk_id": "ALecture13_ToolCalling__0101"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0101", "chunk_index": 101, "page_numbers": [104], "text": "- Scaling alone offers improvements in all metrics, but the improvements on safety and groundedness are not significant.\n- Crowd-annotated data is an effective tool for driving significant additional gains.\n- Calling external APIs can improve response groundedness.", "prev_chunk_id": "ALecture13_ToolCalling__0100", "next_chunk_id": "ALecture13_ToolCalling__0102"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0102", "chunk_index": 102, "page_numbers": [105], "text": "- What is missing in the paper?\n- Tool use analysis, i.e., do we need a translator and a calculator?\n- Tool performance statistics\n- Example outputs from different tools to the same conversation context", "prev_chunk_id": "ALecture13_ToolCalling__0101", "next_chunk_id": "ALecture13_ToolCalling__0103"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0103", "chunk_index": 103, "page_numbers": [106], "text": "Timo Schick Jane Dwivedi-Yu Roberto Dessit Roberta Raileanu MariaLomeli Eric Hambro Nicola Cancedda Thomas Scialom\nLuke Zettlemoyer FAIR, Meta +Universitat Pompeu Fabra", "prev_chunk_id": "ALecture13_ToolCalling__0102", "next_chunk_id": "ALecture13_ToolCalling__0104"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0104", "chunk_index": 104, "page_numbers": [107], "text": "- LMs struggle with factual information lookup or arithmetic calculations.\n- Use of external tools, such as web search or calculators, help with this issue.\n- However, existing methods require large amounts of annotated training datasets.\n- Toolformer:\n- Tool use is learned in a self-supervised way\n- ‚ñ† Cost reduction and what humans find useful may be different from what a model finds useful\n- The LM should be able to decide for itself when and how to use which tool.", "prev_chunk_id": "ALecture13_ToolCalling__0103", "next_chunk_id": "ALecture13_ToolCalling__0105"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0105", "chunk_index": 105, "page_numbers": [108], "text": "- Uses large LMs with in-context learning to generate datasets for training from scratch.\n- Example predictions:\nThe New England Journal of Medicine is a registered trademark of [QA(\"Who is the publisher of The New England Journal ofMedicine?\")‚ÜíMassachusettsMedicalSociety]theMMS.\nOut of 1400 participants, 400 (or [Calculator(400 / 1400) ‚Üí 0.29] 29%) passed the test.\nThe name derives from \"la tortuga\", the Spanish word for [MT(\"tortuga\") ‚Üí turtle] turtle.\nThe Brown Act is California's law [WikiSearch(\"Brown Act\") ‚Üí The Ralph M. Brown Act is an act of the California State Legislature that guarantees thepublic's right to attend and participate in meetings of local legislative bodies.] that requires legislative bodies, like city councils, to hold their meetings open to the public.", "prev_chunk_id": "ALecture13_ToolCalling__0104", "next_chunk_id": "ALecture13_ToolCalling__0106"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0106", "chunk_index": 106, "page_numbers": [109], "text": "Table 1: Examples of inputs and outputs for all APIs used.\n\nQuestion Answering, Example Input = Where was the Knights of Columbusfounded?. Question Answering, Example Output = New Haven, Connecticut. Wikipedia Search, Example Input = Fishing Reel Types. Wikipedia Search, Example Output = s  s    cast fishing by the type of rod and reel used. There are two types of reels used when spin fishing, the open faced reel and the closed faced reel.. Calculator, Example Input = 27+4 * 2. Calculator, Example Output = 35. Calendar, Example Input = . Calendar, Example Output = Today is Monday, January 30, 2023.. Machine Translation suret√© nucl√©aire, Example Input = . Machine Translation suret√© nucl√©aire, Example Output = nuclear safety", "prev_chunk_id": "ALecture13_ToolCalling__0105", "next_chunk_id": "ALecture13_ToolCalling__0107"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0107", "chunk_index": 107, "page_numbers": [110], "text": "- API calls are represented as tuples: c = ( ac ,i c ) , where ac is the name of the API and i c is the corresponding input.\n- API call not including and including its result, r :\n<!-- formula-not-decoded -->\n- Given a corpus C , the corpus is converted to a version, C* with API calls, using 3 steps (next slide).\n- Inference is performed until the model produces the ‚Üí token, then the API is called and a result is obtained, then the decoding process is continued after inserting both the response and the </API> token.\n- Then C* is used to fine-tune the model using the standard language modeling objective.", "prev_chunk_id": "ALecture13_ToolCalling__0106", "next_chunk_id": "ALecture13_ToolCalling__0108"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0108", "chunk_index": 108, "page_numbers": [112], "text": "- For each API a prompt is formulated to encourage the LLM to annotate an example input text with API calls.\n- For example:\nFigure 3: An exemplary prompt P(x) used to generate API calls for the question answering tool.", "prev_chunk_id": "ALecture13_ToolCalling__0107", "next_chunk_id": "ALecture13_ToolCalling__0109"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0109", "chunk_index": 109, "page_numbers": [113], "text": "- For each token position i in the input text, the probability of starting an API call is computed.\n<!-- formula-not-decoded -->\n- Given a sampling threshold ùúè ùë† , top k token positions that have a pi larger than the threshold are considered for API calls.\n- For each such position, up to m API calls are sampled from the language model M, given the  sequence (formed of prompt and text so far followed by <API>):\n<!-- formula-not-decoded -->\nas prefix and </API> as an end-of-sequence token.", "prev_chunk_id": "ALecture13_ToolCalling__0108", "next_chunk_id": "ALecture13_ToolCalling__0110"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0110", "chunk_index": 110, "page_numbers": [114], "text": "- All the produced API calls are executed at this step.\n- API dependent step\n- Could be executing a Python script, call a neural network or call web search.\n- The response for each API call c i needs to be a single text sequence r i .", "prev_chunk_id": "ALecture13_ToolCalling__0109", "next_chunk_id": "ALecture13_ToolCalling__0111"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0111", "chunk_index": 111, "page_numbers": [115], "text": "- An API call is helpful to M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input.\n- Given a set of weights, wi , weighted cross entropy loss for M over the n tokens, if the model is prefixed with some text sequence z :\n<!-- formula-not-decoded -->\n- Two instantiations of this loss: L= min (Li(e),L(e(Ci,Œµ))\n- API call c i is kept if the following holds for it: ‚â•Tf", "prev_chunk_id": "ALecture13_ToolCalling__0110", "next_chunk_id": "ALecture13_ToolCalling__0112"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0112", "chunk_index": 112, "page_numbers": [116], "text": "- Pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters.\n- A subset of CCNet (Wenzek et al., 2020) as the corpus C .\nTable 2: Number of examples with API calls in C* for different values of our filtering threshold Tf .\n\nQuestion Answering, NumberofExamples.Tf = 0.5 = 51,987. Question Answering, NumberofExamples.Tf = 1.0 = 18,526. Question Answering, NumberofExamples.Tf = 2.0 = 5,135. Wikipedia Search, NumberofExamples.Tf = 0.5 = 207,241. Wikipedia Search, NumberofExamples.Tf = 1.0 = 60,974. Wikipedia Search, NumberofExamples.Tf = 2.0 = 13,944. Calculator, NumberofExamples.Tf = 0.5 = 3,680. Calculator, NumberofExamples.Tf = 1.0 = 994. Calculator, NumberofExamples.Tf = 2.0 = 138. Calendar, NumberofExamples.Tf = 0.5 = 61,811. Calendar, NumberofExamples.Tf = 1.0 = 20,587. Calendar, NumberofExamples.Tf = 2.0 = 3,007. Machine Translation, NumberofExamples.Tf = 0.5 = 3,156. Machine Translation, NumberofExamples.Tf = 1.0 = 1,034. Machine Translation, NumberofExamples.Tf = 2.0 = 229", "prev_chunk_id": "ALecture13_ToolCalling__0111", "next_chunk_id": "ALecture13_ToolCalling__0113"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0113", "chunk_index": 113, "page_numbers": [117], "text": "- GPT-J + CC: GPT-J finetuned on C without any API calls.\n- Toolformer: GPT-J finetuned on C ‚àó (subset of CCNet augmented with API calls.\n- Toolformer (disabled): The same model as Toolformer, but API calls are disabled during decoding. This is achieved by manually setting the probability of the <API> token to 0.", "prev_chunk_id": "ALecture13_ToolCalling__0112", "next_chunk_id": "ALecture13_ToolCalling__0114"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0114", "chunk_index": 114, "page_numbers": [118], "text": "Table 3: Results on subsets of LAMA and various benchmarks requiring mathematical reasoning. For LAMA, Toolformer uses the question answering tool for most examples, clearly outperforming all baselines of the same size and achieving results competitive with GPT-3. For the math benchmarks, Toolformer makes extensive use of the calculator tool, clearly outperforming OPT and GPT-3. Best results with a GPT-J based model are shown in bold, best results overall are underlined.\n\nGPT-J, LAMA.SQuAD = 17.8. GPT-J, LAMA.Google-RE = 4.9. GPT-J, LAMA.T-REx = 31.9. GPT-J, MathBenchmarks.ASDiv = 7.5. GPT-J, MathBenchmarks.SVAMP = 5.2. GPT-J, MathBenchmarks.MAWPS = 9.9. GPT-J + CC, LAMA.SQuAD = 19.2. GPT-J + CC, LAMA.Google-RE = 5.6. GPT-J + CC, LAMA.T-REx = 33.2. GPT-J + CC, MathBenchmarks.ASDiv = 9.6. GPT-J + CC, MathBenchmarks.SVAMP = 5.0. GPT-J + CC, MathBenchmarks.MAWPS = 9.3. Toolformer (disabled), LAMA.SQuAD = 22.1. Toolformer (disabled), LAMA.Google-RE = 6.3. Toolformer (disabled), LAMA.T-REx = 34.9. Toolformer (disabled), MathBenchmarks.ASDiv = 14.8. Toolformer (disabled), MathBenchmarks.SVAMP = 6.3. Toolformer (disabled), MathBenchmarks.MAWPS = 15.0. Toolformer, LAMA.SQuAD = 33.8. Toolformer, LAMA.Google-RE = 11.5. Toolformer, LAMA.T-REx = 53.5. Toolformer, MathBenchmarks.ASDiv = 40.4. Toolformer, MathBenchmarks.SVAMP = 29.4. Toolformer, MathBenchmarks.MAWPS = 44.0. OPT (66B), LAMA.SQuAD = 21.6. OPT (66B), LAMA.Google-RE = 2.9. OPT (66B), LAMA.T-REx = 30.1. OPT (66B), MathBenchmarks.ASDiv = 6.0. OPT (66B), MathBenchmarks.SVAMP = 4.9. OPT (66B), MathBenchmarks.MAWPS = 7.9. GPT-3 (175B), LAMA.SQuAD = 26.8. GPT-3 (175B), LAMA.Google-RE = 7.0. GPT-3 (175B), LAMA.T-REx = 39.8. GPT-3 (175B), MathBenchmarks.ASDiv = 14.0. GPT-3 (175B), MathBenchmarks.SVAMP = 10.0. GPT-3 (175B), MathBenchmarks.MAWPS = 19.8", "prev_chunk_id": "ALecture13_ToolCalling__0113", "next_chunk_id": "ALecture13_ToolCalling__0115"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0115", "chunk_index": 115, "page_numbers": [119], "text": "Figure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls. While API calls are not helpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the gap between predictions with and without API calls remains high.", "prev_chunk_id": "ALecture13_ToolCalling__0114", "next_chunk_id": "ALecture13_ToolCalling__0116"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0116", "chunk_index": 116, "page_numbers": [120], "text": "- Inability of Toolformer to use tools in a chain (i.e., using the output of one tool as an input for another tool).\n- Sample-inefficient; for example, processing more than a million documents results in only a few thousand examples of useful calls to the calculator API.\n- When deciding whether or not to make an API call, Toolformer currently does not take into account the tool-dependent, computational cost incurred from making an API call.", "prev_chunk_id": "ALecture13_ToolCalling__0115", "next_chunk_id": "ALecture13_ToolCalling__0117"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0117", "chunk_index": 117, "page_numbers": [121], "text": "Shishir G. Patil 1* Tianjun Zhang * Xin Wang Joseph E. Gonzalez 1UC Berkeley 2Microsoft Research sgp @berkeley.edu", "prev_chunk_id": "ALecture13_ToolCalling__0116", "next_chunk_id": "ALecture13_ToolCalling__0118"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0118", "chunk_index": 118, "page_numbers": [122], "text": "- LLMs have excelled at many tasks but lack the ability to generate accurate API calls i.e., accurately use tools.\n- Aim: the ability to invoke a vast space of changing cloud APIs can be transformative\n- One can talk to LLMs to complete many tasks, such as arranging travels\n- How to make sure API calls are accurate?\n- Retrieval-based methods:\n- ‚ñ† Changes in the API documentation won't affect the performance\n- ‚ñ† Reduction in hallucination\n- Smaller Open-Source models:\n- Fine-Tune Llama 7B param size model, also reduces serving cost", "prev_chunk_id": "ALecture13_ToolCalling__0117", "next_chunk_id": "ALecture13_ToolCalling__0119"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0119", "chunk_index": 119, "page_numbers": [123], "text": "```\n<domain>: Speech-to-Text <api_provider>: TorchHub <code>: asr_model = torch.hub.load( 'snakers4/silero-models', 'asr' source='local') result = asr_model.transcribe( audio_path)\n```", "prev_chunk_id": "ALecture13_ToolCalling__0118", "next_chunk_id": "ALecture13_ToolCalling__0120"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0120", "chunk_index": 120, "page_numbers": [123], "text": "```\n<domain>:Audio-Translation <api_provider>:Pytorch <code>: import torchaudio translation = Torchaudio.pipelines. WAV2VEC2ASR PIPELINE( \"audio.wav\")\n```", "prev_chunk_id": "ALecture13_ToolCalling__0119", "next_chunk_id": "ALecture13_ToolCalling__0121"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0121", "chunk_index": 121, "page_numbers": [123], "text": "```\n<domain>:Speech-to-Text <api_provider>:TorchHub <code>: asr_model = torch.hub.load( 'snakers4/silero-models', 'silero_sst') result= asr_model.transcribe( audio_path)\n```\nPrompt:Helpme find anAPl to convert thespoken language in arecorded audioto text using TorchHub.\nHallucination: an API call that is not a sub-tree of any API in the database -invoking an entirely imagined tool.", "prev_chunk_id": "ALecture13_ToolCalling__0120", "next_chunk_id": "ALecture13_ToolCalling__0122"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0122", "chunk_index": 122, "page_numbers": [124], "text": "- [Instruction-API] dataset creation via self-instruction: APIBench\n- Evaluation Metric for API calls: AST tree matching\n- Retriever-Aware Training", "prev_chunk_id": "ALecture13_ToolCalling__0121", "next_chunk_id": "ALecture13_ToolCalling__0123"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0123", "chunk_index": 123, "page_numbers": [126], "text": "- Collection of API information for modeling\n{domain, framework, functionality, api_name, api_call, api_arguments, environment_requirements, example_code, performance, description.}", "prev_chunk_id": "ALecture13_ToolCalling__0122", "next_chunk_id": "ALecture13_ToolCalling__0124"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0124", "chunk_index": 124, "page_numbers": [127], "text": "- Constructed 6 examples (InstructionAPI pairs) for each of the three model hubs\n- For each 1,645 API data points, sample 3 of 6 corresponding instruction examples to generate a total of 10 instruction-api pairs\n- GPT-4 for instruction-API pair generation\n- Filtering after generation*", "prev_chunk_id": "ALecture13_ToolCalling__0123", "next_chunk_id": "ALecture13_ToolCalling__0125"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0125", "chunk_index": 125, "page_numbers": [128], "text": "- Evaluation for accuracy , hallucination and errors\n- mistake=hallucination or error\n- Check for functional equivalence\n- Evaluate only the API call", "prev_chunk_id": "ALecture13_ToolCalling__0124", "next_chunk_id": "ALecture13_ToolCalling__0126"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0126", "chunk_index": 126, "page_numbers": [131], "text": "- Model: LLaMA-7B model\n- Data: {instruction, API} pairs\n- Two variations of the data:\n- Zero-shot (No Retriever is used)\n- Retriever-Aware Training", "prev_chunk_id": "ALecture13_ToolCalling__0125", "next_chunk_id": "ALecture13_ToolCalling__0127"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0127", "chunk_index": 127, "page_numbers": [133], "text": "- 3 retrievers:\n- BM-25\n- GPT-Index (text-davinci-003 from OpenAI)\n- Oracle: Ground Truth\n- Exact retrieval procedure isn't mentioned*\n- No Metrics for retriever*\n- During retrieval, the user's query is used to search the index and fetch the most relevant (top-1) API", "prev_chunk_id": "ALecture13_ToolCalling__0126", "next_chunk_id": "ALecture13_ToolCalling__0128"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0128", "chunk_index": 128, "page_numbers": [134], "text": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs\n\nLLAMA (O-shot), TorchHub.overall ‚Üë = 0. LLAMA (O-shot), TorchHub.hallu ‚Üì = 100. LLAMA (O-shot), TorchHub. = 0. LLAMA (O-shot), HuggingFace.overall ‚Üë = 0.00. LLAMA (O-shot), HuggingFace.hallu‚Üì = 97.57. LLAMA (O-shot), HuggingFace..1 = 2.43. LLAMA (O-shot), TensorFlowHub.overall‚Üë = 0. LLAMA (O-shot), TensorFlowHub.hallu‚Üì = 100. LLAMA (O-shot), TensorFlowHub. = 0. GPT-3.5 (0-shot), TorchHub.overall ‚Üë = 48.38. GPT-3.5 (0-shot), TorchHub.hallu ‚Üì = 18.81. GPT-3.5 (0-shot), TorchHub. = 32.79. GPT-3.5 (0-shot), HuggingFace.overall ‚Üë = 16.81. GPT-3.5 (0-shot), HuggingFace.hallu‚Üì = 35.73. GPT-3.5 (0-shot), HuggingFace..1 = 47.46. GPT-3.5 (0-shot), TensorFlowHub.overall‚Üë = 41.75. GPT-3.5 (0-shot), TensorFlowHub.hallu‚Üì = 47.88. GPT-3.5 (0-shot), TensorFlowHub. = 10.36. GPT-4 (0-shot), TorchHub.overall ‚Üë = 38.70. GPT-4 (0-shot), TorchHub.hallu ‚Üì = 36.55. GPT-4 (0-shot), TorchHub. = 24.7. GPT-4 (0-shot), HuggingFace.overall ‚Üë = 19.80. GPT-4 (0-shot), HuggingFace.hallu‚Üì = 37.16. GPT-4 (0-shot), HuggingFace..1 = 43.03. GPT-4 (0-shot), TensorFlowHub.overall‚Üë = 18.20. GPT-4 (0-shot), TensorFlowHub.hallu‚Üì = 78.65. GPT-4 (0-shot), TensorFlowHub. = 3.13. Claude (0-shot), TorchHub.overall ‚Üë = 18.81. Claude (0-shot), TorchHub.hallu ‚Üì = 65.59. Claude (0-shot), TorchHub. = 15.59. Claude (0-shot), HuggingFace.overall ‚Üë = 6.19. Claude (0-shot), HuggingFace.hallu‚Üì = 77.65. Claude (0-shot), HuggingFace..1 = 16.15. Claude (0-shot), TensorFlowHub.overall‚Üë = 9.19. Claude (0-shot), TensorFlowHub.hallu‚Üì = 88.46. Claude (0-shot), TensorFlowHub. = 2.33. Gorilla (O-shot), TorchHub.overall ‚Üë = 59.13. Gorilla (O-shot), TorchHub.hallu ‚Üì = 6.98. Gorilla (O-shot), TorchHub. = 33.87. Gorilla (O-shot), HuggingFace.overall ‚Üë = 71.68. Gorilla (O-shot), HuggingFace.hallu‚Üì = 10.95. Gorilla (O-shot), HuggingFace..1 = 17.36. Gorilla (O-shot), TensorFlowHub.overall‚Üë = 83.79. Gorilla (O-shot), TensorFlowHub.hallu‚Üì = 5.40. Gorilla (O-shot), TensorFlowHub. = 10.80", "prev_chunk_id": "ALecture13_ToolCalling__0127", "next_chunk_id": "ALecture13_ToolCalling__0129"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0129", "chunk_index": 129, "page_numbers": [135], "text": "- Training with Oracle API reference; and inference with a retriever\nTable1:EvaluatingLLMsonTorchHub,HuggingFace,andTensorfowHubAPIs\n\nLLAMA(BM-25), TorchHub.overall‚Üë = 8.60. LLAMA(BM-25), TorchHub.‚Üëne = 76.88. LLAMA(BM-25), TorchHub. = 14.51. LLAMA(BM-25), HuggingFace.overall‚Üë = 3.00. LLAMA(BM-25), HuggingFace.hallu‚Üì = 77.99. LLAMA(BM-25), HuggingFace. = 19.02. LLAMA(BM-25), TensorFlow Hub.overall‚Üë = 8.90. LLAMA(BM-25), TensorFlow Hub.hallu‚Üì = 77.37. LLAMA(BM-25), TensorFlow Hub. = 13.72. GPT-3.5(BM-25), TorchHub.overall‚Üë = 38.17. GPT-3.5(BM-25), TorchHub.‚Üëne = 6.98. GPT-3.5(BM-25), TorchHub. = 54.83. GPT-3.5(BM-25), HuggingFace.overall‚Üë = 17.26. GPT-3.5(BM-25), HuggingFace.hallu‚Üì = 8.30. GPT-3.5(BM-25), HuggingFace. = 74.44. GPT-3.5(BM-25), TensorFlow Hub.overall‚Üë = 54.16. GPT-3.5(BM-25), TensorFlow Hub.hallu‚Üì = 3.64. GPT-3.5(BM-25), TensorFlow Hub. = 42.18. GPT-4(BM-25), TorchHub.overall‚Üë = 35.48. GPT-4(BM-25), TorchHub.‚Üëne = 11.29. GPT-4(BM-25), TorchHub. = 53.22. GPT-4(BM-25), HuggingFace.overall‚Üë = 16.48. GPT-4(BM-25), HuggingFace.hallu‚Üì = 15.93. GPT-4(BM-25), HuggingFace. = 67.59. GPT-4(BM-25), TensorFlow Hub.overall‚Üë = 34.01. GPT-4(BM-25), TensorFlow Hub.hallu‚Üì = 37.08. GPT-4(BM-25), TensorFlow Hub. = 28.90. Claude (BM-25), TorchHub.overall‚Üë = 39.78. Claude (BM-25), TorchHub.‚Üëne = 5.37. Claude (BM-25), TorchHub. = 54.83. Claude (BM-25), HuggingFace.overall‚Üë = 14.60. Claude (BM-25), HuggingFace.hallu‚Üì = 15.82. Claude (BM-25), HuggingFace. = 69.58. Claude (BM-25), TensorFlow Hub.overall‚Üë = 35.18. Claude (BM-25), TensorFlow Hub.hallu‚Üì = 21.16. Claude (BM-25), TensorFlow Hub. = 43.64. Gorilla (BM-25), TorchHub.overall‚Üë = 40.32. Gorilla (BM-25), TorchHub.‚Üëne = 4.30. Gorilla (BM-25), TorchHub. = 55.37. Gorilla (BM-25), HuggingFace.overall‚Üë = 17.03. Gorilla (BM-25), HuggingFace.hallu‚Üì = 6.42. Gorilla (BM-25), HuggingFace. = 76.55. Gorilla (BM-25), TensorFlow Hub.overall‚Üë = 41.89. Gorilla (BM-25), TensorFlow Hub.hallu‚Üì = 2.77. Gorilla (BM-25), TensorFlow Hub. = 55.32. LLAMA(GPT-Index), TorchHub.overall‚Üë = 14.51. LLAMA(GPT-Index), TorchHub.‚Üëne = 75.8. LLAMA(GPT-Index), TorchHub. = 9.67. LLAMA(GPT-Index), HuggingFace.overall‚Üë = 10.18. LLAMA(GPT-Index), HuggingFace.hallu‚Üì = 75.66. LLAMA(GPT-Index), HuggingFace. = 14.20. LLAMA(GPT-Index), TensorFlow Hub.overall‚Üë = 15.62. LLAMA(GPT-Index), TensorFlow Hub.hallu‚Üì = 77.66. LLAMA(GPT-Index), TensorFlow Hub. = 6.71. GPT-3.5(GPT-Index), TorchHub.overall‚Üë = 60.21. GPT-3.5(GPT-Index), TorchHub.‚Üëne = 1.61. GPT-3.5(GPT-Index), TorchHub. = 38.17. GPT-3.5(GPT-Index), HuggingFace.overall‚Üë = 29.08. GPT-3.5(GPT-Index), HuggingFace.hallu‚Üì = 7.85. GPT-3.5(GPT-Index), HuggingFace. = 44.80. GPT-3.5(GPT-Index), TensorFlow Hub.overall‚Üë = 65.59. GPT-3.5(GPT-Index), TensorFlow Hub.hallu‚Üì = 3.79. GPT-3.5(GPT-Index), TensorFlow Hub. = 30.50. GPT-4(GPT-Index), TorchHub.overall‚Üë = 59.13. GPT-4(GPT-Index), TorchHub.‚Üëne = 1.07. GPT-4(GPT-Index), TorchHub. = 39.78. GPT-4(GPT-Index), HuggingFace.overall‚Üë = 44.58. GPT-4(GPT-Index), HuggingFace.hallu‚Üì = 11.18. GPT-4(GPT-Index), HuggingFace. = 44.25. GPT-4(GPT-Index), TensorFlow Hub.overall‚Üë = 43.94. GPT-4(GPT-Index), TensorFlow Hub.hallu‚Üì = 31.53. GPT-4(GPT-Index), TensorFlow Hub. = 24.52. Claude (GPT-Index), TorchHub.overall‚Üë = 60.21. Claude (GPT-Index), TorchHub.‚Üëne = 3.76. Claude (GPT-Index), TorchHub. = 36.02. Claude (GPT-Index), HuggingFace.overall‚Üë = 41.37. Claude (GPT-Index), HuggingFace.hallu‚Üì = 18.81. Claude (GPT-Index), HuggingFace. = 39.82. Claude (GPT-Index), TensorFlow Hub.overall‚Üë = 55.62. Claude (GPT-Index), TensorFlow Hub.hallu‚Üì = 16.20. Claude (GPT-Index), TensorFlow Hub. = 28.17. Gorilla (GPT-Index), TorchHub.overall‚Üë = 61.82. Gorilla (GPT-Index), TorchHub.‚Üëne = 0. Gorilla (GPT-Index), TorchHub. = 38.17. Gorilla (GPT-Index), HuggingFace.overall‚Üë = 47.46. Gorilla (GPT-Index), HuggingFace.hallu‚Üì = 8.19. Gorilla (GPT-Index), HuggingFace. = 44.36. Gorilla (GPT-Index), TensorFlow Hub.overall‚Üë = 64.96. Gorilla (GPT-Index), TensorFlow Hub.hallu‚Üì = 2.33. Gorilla (GPT-Index), TensorFlow Hub. = 32.70. LLAMA(Oracle), TorchHub.overall‚Üë = 16.12. LLAMA(Oracle), TorchHub.‚Üëne = 79.03. LLAMA(Oracle), TorchHub. = 4.83. LLAMA(Oracle), HuggingFace.overall‚Üë = 17.70. LLAMA(Oracle), HuggingFace.hallu‚Üì = 77.10. LLAMA(Oracle), HuggingFace. = 5.20. LLAMA(Oracle), TensorFlow Hub.overall‚Üë = 12.55. LLAMA(Oracle), TensorFlow Hub.hallu‚Üì = 87.00. LLAMA(Oracle), TensorFlow Hub. = 0.43. GPT-3.5(Oracle), TorchHub.overall‚Üë = 66.31. GPT-3.5(Oracle), TorchHub.‚Üëne = 1.60. GPT-3.5(Oracle), TorchHub. = 32.08. GPT-3.5(Oracle), HuggingFace.overall‚Üë = 89.71. GPT-3.5(Oracle), HuggingFace.hallu‚Üì = 6.64. GPT-3.5(Oracle), HuggingFace. = 3.65. GPT-3.5(Oracle), TensorFlow Hub.overall‚Üë = 95.03. GPT-3.5(Oracle), TensorFlow Hub.hallu‚Üì = 0.29. GPT-3.5(Oracle), TensorFlow Hub. = 4.67. GPT-4(Oracle), TorchHub.overall‚Üë = 66.12. GPT-4(Oracle), TorchHub.‚Üëne = 0.53. GPT-4(Oracle), TorchHub. = 33.33. GPT-4(Oracle), HuggingFace.overall‚Üë = 85.07. GPT-4(Oracle), HuggingFace.hallu‚Üì = 10.62. GPT-4(Oracle), HuggingFace. = 4.31. GPT-4(Oracle), TensorFlow Hub.overall‚Üë = 55.91. GPT-4(Oracle), TensorFlow Hub.hallu‚Üì = 37.95. GPT-4(Oracle), TensorFlow Hub. = 6.13. Claude(Oracle), TorchHub.overall‚Üë = 63.44. Claude(Oracle), TorchHub.‚Üëne = 3.76. Claude(Oracle), TorchHub. = 32.79. Claude(Oracle), HuggingFace.overall‚Üë = 77.21. Claude(Oracle), HuggingFace.hallu‚Üì = 19.58. Claude(Oracle), HuggingFace. = 3.21. Claude(Oracle), TensorFlow Hub.overall‚Üë = 74.74. Claude(Oracle), TensorFlow Hub.hallu‚Üì = 21.60. Claude(Oracle), TensorFlow Hub. = 3.64. Gorilla(Oracle), TorchHub.overall‚Üë = 67.20. Gorilla(Oracle), TorchHub.‚Üëne = 0. Gorilla(Oracle), TorchHub. = 32.79. Gorilla(Oracle), HuggingFace.overall‚Üë = 91.26. Gorilla(Oracle), HuggingFace.hallu‚Üì = 7.08. Gorilla(Oracle), HuggingFace. = 991. Gorilla(Oracle), TensorFlow Hub.overall‚Üë = 94.16. Gorilla(Oracle), TensorFlow Hub.hallu‚Üì = 1.89. Gorilla(Oracle), TensorFlow Hub. = 3.94", "prev_chunk_id": "ALecture13_ToolCalling__0128", "next_chunk_id": "ALecture13_ToolCalling__0130"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0130", "chunk_index": 130, "page_numbers": [136], "text": "- API documents can change over time.", "prev_chunk_id": "ALecture13_ToolCalling__0129", "next_chunk_id": "ALecture13_ToolCalling__0131"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0131", "chunk_index": 131, "page_numbers": [137], "text": "- More experiments with accuracy of the retriever taken into account\n- Increase number of APIs returned by the retriever.\n- More explanation of evaluating similar API calls", "prev_chunk_id": "ALecture13_ToolCalling__0130", "next_chunk_id": "ALecture13_ToolCalling__0132"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0132", "chunk_index": 132, "page_numbers": [138], "text": "Does the model learn when to call tools?, RAG-FiD for Dialogue = No (but possible). Does the model learn when to call tools?, LaMDA = Yes. Does the model learn when to call tools?, ToolFormer = Yes. Does the model learn when to call tools?, Gorilla = No. When to call the tools?, RAG-FiD for Dialogue = Before. When to call the tools?, LaMDA = Before. When to call the tools?, ToolFormer = During Decoding *. When to call the tools?, Gorilla = Before. Multi-step reasoning?, RAG-FiD for Dialogue = No. Multi-step reasoning?, LaMDA = Yes. Multi-step reasoning?, ToolFormer = No. Multi-step reasoning?, Gorilla = No. Requires annotated data?, RAG-FiD for Dialogue = Yes. Requires annotated data?, LaMDA = Yes. Requires annotated data?, ToolFormer = No **. Requires annotated data?, Gorilla = No **. Number of tools, RAG-FiD for Dialogue = 1. Number of tools, LaMDA = <10. Number of tools, ToolFormer = <10. Number of tools, Gorilla = >1K", "prev_chunk_id": "ALecture13_ToolCalling__0131", "next_chunk_id": "ALecture13_ToolCalling__0133"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0133", "chunk_index": 133, "page_numbers": [139], "text": "- LLMs excel at reasoning but have limitations (outdated knowledge, calculation errors, ‚Ä¶)\n- Tool-Integrated Reasoning (TIR) is key: LLMs using external tools (search, calculators, APIs).\n- Problem: Current methods (SFT) struggle.\n- Limited generalization\n- 'Overthinking'", "prev_chunk_id": "ALecture13_ToolCalling__0132", "next_chunk_id": "ALecture13_ToolCalling__0134"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0134", "chunk_index": 134, "page_numbers": [140], "text": "- Arxiv Paper Link:\n- -https://arxiv.org/pdf/2504.13958\n- Github Link:\n- -https://github.com/qiancheng0/ToolRL", "prev_chunk_id": "ALecture13_ToolCalling__0133", "next_chunk_id": "ALecture13_ToolCalling__0135"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0135", "chunk_index": 135, "page_numbers": [140], "text": "2025 16 Apr 2 [cs.LG] arXiv:2504.13958v1\nToolRL:Reward is All Tool Learning Needs\nChengQian,Emre CanAikgoz,Qi HeHongru Wang,Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur,Heng Ji University of Illinois Urbana-Champaign {chengq9Ôºåhengjij@illinois.edu", "prev_chunk_id": "ALecture13_ToolCalling__0134", "next_chunk_id": "ALecture13_ToolCalling__0136"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0136", "chunk_index": 136, "page_numbers": [140], "text": "Curet Large Language Models (LLMsÔºâ often undergo supervised fine-tuning (SFT) to acquire toolsecapabilitieHoweverFstu gles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learming (RL), particularly with R1likemodlshavedemonstrated promisingrea soning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained fedback requred frfective leaing. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building ward designtailored fortoolue tasksand apply on these insights, we propose a principled reit to train LLMs using Group RelativePolicy across diverse benchmarks demonstrate that Optimization (GRPO). Empirical evaluations ourapproach ylds robust, calble,andstal training, achieving a 17% improvement over base models and a 15% gain over SFT modof LLMs. All the code are released to facilitate els. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance future research.", "prev_chunk_id": "ALecture13_ToolCalling__0135", "next_chunk_id": "ALecture13_ToolCalling__0137"}
{"lecture_id": "ALecture13_ToolCalling", "source_pdf": "data/anlp/ALecture13_ToolCalling.pdf", "chunk_id": "ALecture13_ToolCalling__0137", "chunk_index": 137, "page_numbers": [140], "text": "Recent advances in Large Language Models (LLMs) have showcased remarkable capabilities in complex reasoning tasks (Kumar et al., 2025). Among the techniques that have significantly contributed to this progress, Reinforcement Learming (RL) has emerged as a powerful paradigm, enabling\nDataand codes released at https://github.com/ qianchenge/Too1RL\nFigure 1: SFT on distilled deep-thinking trajectories suffers from overthinking and limited generalization.\nLLMs to develop emergent capabilities such as selfreflection, self-correction, and long-horizon planning (Guo et al.,2025; Team et al., 2025).These capabilities have been instrumental in the success of models like ol and R1, particularly in mathe2024a:Huang etal2024;Lietal,2025bKang matical and logical reasoning domains (Qin et al., et al., 2025).\nUnlike textual reasoning, which primarily involves deduction and inference from static text,\nBeyond traditional reasoning tasks, an increasingly important area is Tool-Integrated Reasoning nal tools, such as search engines Jin et al., 025; (TIR). TIR involves LLMs interacting with exterZheng et al., 2025), calculators (Chen et al., 2023b; Qin et al., 2023), or code interpreters (Gou et al., 2023; Liao et al., 2024), in a multi-step, feedbackdriven loop to arrive at solutions. TIR is partictions of LLMs, such as outdated knowledge, calularly important because it addresses core limitaculation inaccuracy, and shallow reasoning. By integrating external tools that offe real-time access and specialized capabilities, TIR enables models to tackle complex tasks in a more grounded and goal-directed way.", "prev_chunk_id": "ALecture13_ToolCalling__0136", "next_chunk_id": null}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0000", "chunk_index": 0, "page_numbers": [1], "text": "Dilek HakkaniT√ºr", "prev_chunk_id": null, "next_chunk_id": "ALecture14_Reasoning__0001"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0001", "chunk_index": 1, "page_numbers": [2], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0000", "next_chunk_id": "ALecture14_Reasoning__0002"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0002", "chunk_index": 2, "page_numbers": [3], "text": "- -Huang and Chang. Towards Reasoning in Large Language Models: A Survey. ACL Findings, 2023.\n- -Compilation of related work here: https://github.com/atfortes/Awesome-LLM-Reasoning?tab=readmeov-file", "prev_chunk_id": "ALecture14_Reasoning__0001", "next_chunk_id": "ALecture14_Reasoning__0003"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0003", "chunk_index": 3, "page_numbers": [4], "text": "- -Some user requests are simple (e.g., set a timer for 3 minutes) and some are more complex, requiring analytical thinking (e.g., Math problems, schedule a reading group meeting).\n- -Wikipedia Definition: Reasoning is the capacity of consciously applying logic by drawing valid conclusions from new or existing information, with the aim of seeking truth\n- -Can LLMs think and reason?\n- -Can LLMs generate better responses if they can think and reason? (meta-generation, accd. to https://arxiv.org/pdf/2406.16838).\n- -What could z be?\n```\nx : input, y : output, z : reasoning steps x ‚Üí y versus x ‚Üí z ‚Üí y P( y | x ) versus P( z | x ) and then P( y | x,z )\n```", "prev_chunk_id": "ALecture14_Reasoning__0002", "next_chunk_id": "ALecture14_Reasoning__0004"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0004", "chunk_index": 4, "page_numbers": [5], "text": "Types of mental operations used when making sense of information, drawing conclusions, and solving problems.\nDeductive Reasoning: Drawing conclusions from general rules or premises.\n- -Example: All animals are mortal. All cats are animals. All cats are mortal.\nInductive Reasoning: Inferring conclusions from specific observations.\n- -Example: The sun sets every day. The sun will set today.\nAbductive Reasoning: Deriving an explanation given a set of observations.\n- -Example: The roads are wet in the afternoon. It must have rained earlier.", "prev_chunk_id": "ALecture14_Reasoning__0003", "next_chunk_id": "ALecture14_Reasoning__0005"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0005", "chunk_index": 5, "page_numbers": [6], "text": "Analogical Reasoning: Solving new problems using their similarity to previously known problems with solutions.\n- -Example: Learning to ride a bike is like learning to walk. (i.e., it is rarely forgotten)\nCommonsense/Intuitive Reasoning: Making logical inferences and judgments based on the everyday knowledge that most people share.\n- -Example: The stove is on. My hand will burn if I touch it.\n‚Ä¶", "prev_chunk_id": "ALecture14_Reasoning__0004", "next_chunk_id": "ALecture14_Reasoning__0006"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0006", "chunk_index": 6, "page_numbers": [7], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0005", "next_chunk_id": "ALecture14_Reasoning__0007"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0007", "chunk_index": 7, "page_numbers": [8], "text": "- -Relies on rules, first-order logic, and semantic representations.\n- -E.g., An extended model of natural logic for natural language inference (MacCartney and Manning, ICCS 2009)\n- -Hypothesis, h : Every big company in the poll reported cost increases.\n- -Premise, p : Every firm polled saw costs grow more than expected, even after adjusting for inflation.\n- -Can h be inferred from the given p ?\n- -Decomposes the inference problem into a sequence of atomic edits, linking p to h\n- -Predicts a lexical semantic relation for each edit;\n- -Propagates these relations upward through semantic composition and\n- -Joins the resulting semantic relations across the edit sequence to determine inference.\n- -Examples of semantic relations:\n, symbol5 = c=y. , name = equivalence. , example = couch‰∏âsofa. , example = . , example = . , set theoreticdefinition = c=y. C, symbol5 = [y. C, name = forwardentailment. C, example = crow. C, example = . C, example = bird. C, set theoreticdefinition = rCy. , symbol5 = y. , name = reverseentailment. , example = European. , example = Âè£. , example = French. , set theoreticdefinition = y. , symbol5 = v fi. , name = negation. , example = human. , example = ÂÖ´. , example = nonhuman. , set theoreticdefinition = c‚à©y=Uy=U", "prev_chunk_id": "ALecture14_Reasoning__0006", "next_chunk_id": "ALecture14_Reasoning__0008"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0008", "chunk_index": 8, "page_numbers": [9], "text": "- -CYC (Lenat et al, Communications of ACM, 1995)\n- -Encode and represent knowledge about the world so AI systems can reason logically rather than just pattern-match.\n- -A knowledge base of millions of assertions about concepts and relations organized into an ontology.\n- -Example: All animals are mortal. All cats are animals. All cats are mortal. ;;Premises\n- -‚àÄ X: (isa(X,Animal) ‚üπ isa(X,MortalBeing))\n- -‚àÄ X:(isa(X,Cat) ‚üπ isa(X,Animal))\n- -Transitivity rule\n```\nÔºà#$genls #$Animal#$MortalBeing) (#$genls #$Cat#$Animal) ;;Inference rule (built into Cyc ontology): (forAll(?A?B?C) (implies (and (#$genls ?A ?B) (#$genls ÔºüB ?C)) (#$genls ?A ?C))) ;;Derived conclusion (#$genls #$Cat #$MortalBeing)\n```", "prev_chunk_id": "ALecture14_Reasoning__0007", "next_chunk_id": "ALecture14_Reasoning__0009"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0009", "chunk_index": 9, "page_numbers": [10], "text": "ConceptNet , static facts and relations between concepts (Liu and Singh, BT Technology Journal, 2004)\nAtomic , encodes event-centered knowledge, social and inferential commonsense, focusing on what typically happens before or after events, people's intents, and reactions. (Sap et al., AAAI 2019)", "prev_chunk_id": "ALecture14_Reasoning__0008", "next_chunk_id": "ALecture14_Reasoning__0010"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0010", "chunk_index": 10, "page_numbers": [11], "text": "Figure 1: COMET learns from an existing knowledge base (solid lines) to be able to generate novel nodes and edges (dashed lines).\n- -COMET, a neural commonsense reasoning model, trained on knowledge graphs like ATOMIC and ConceptNet to generate new commonsense inferences in natural language (Bosselut et al., ACL 2019).\n- -Based on GPT, trained to learn to produce the phrase object o of a knowledge tuple given the tuple's phrase subject s and relation r.", "prev_chunk_id": "ALecture14_Reasoning__0009", "next_chunk_id": "ALecture14_Reasoning__0011"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0011", "chunk_index": 11, "page_numbers": [12], "text": "- -(Zhou et al., ACL 2022)\n- -Extended existing conversational datasets with implicit commonsense reasoning knowledge (https://github.com/alexa/Commonsen se-Dialogues)\n- -Can LLMs learn to generate explicit commonsense reasoning steps before generating responses?\n- -Trained a model to generate these steps given a conversation.\n- -Performed such reasoning before generating responses.\n- -Does such reasoning improve model responses?", "prev_chunk_id": "ALecture14_Reasoning__0010", "next_chunk_id": "ALecture14_Reasoning__0012"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0012", "chunk_index": 12, "page_numbers": [13], "text": "- -(Adolphs et al., EMNLP Findings, 2022)\n- -Similar idea, but with general knowledge.\n- -A seq2seq knowledge model that maps from context to knowledge.\n- -Aseq2seq response model that generates the final response given the predicted knowledge and the context.", "prev_chunk_id": "ALecture14_Reasoning__0011", "next_chunk_id": "ALecture14_Reasoning__0013"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0013", "chunk_index": 13, "page_numbers": [14], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0012", "next_chunk_id": "ALecture14_Reasoning__0014"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0014", "chunk_index": 14, "page_numbers": [15], "text": "Slide from Prompting Lecture", "prev_chunk_id": "ALecture14_Reasoning__0013", "next_chunk_id": "ALecture14_Reasoning__0015"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0015", "chunk_index": 15, "page_numbers": [16], "text": "Figure from: (Qi et al., EMNLP 2023)", "prev_chunk_id": "ALecture14_Reasoning__0014", "next_chunk_id": "ALecture14_Reasoning__0016"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0016", "chunk_index": 16, "page_numbers": [17], "text": "- -(Wang et al., ICLR 2023)\n- -Samples a diverse set of reasoning paths\n- -From the options, selects the most consistent answer by marginalizing out the sampled paths.", "prev_chunk_id": "ALecture14_Reasoning__0015", "next_chunk_id": "ALecture14_Reasoning__0017"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0017", "chunk_index": 17, "page_numbers": [18], "text": "- -ai -> i th model answer, i =1,‚Ä¶,m .\n- -r i -> reasoning steps corresponding to the i th model answer.\n- -Sample multiple ( r i , a i ) from the model's decoder\n- -Marginalize over r i by taking a majority vote over ai :\n<!-- formula-not-decoded -->\n- -Other answer aggregation strategies:\n- o Weight each ( r i , a i ) by P ( r i , a i | prompt, question) and sum\n<!-- formula-not-decoded -->\n- o Normalize probability by output length\n- o Weighted average", "prev_chunk_id": "ALecture14_Reasoning__0016", "next_chunk_id": "ALecture14_Reasoning__0018"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0018", "chunk_index": 18, "page_numbers": [19], "text": "- -(Madaan et al., NeurIPS 2023)\n- -An LLM generates an initial output.\n- -Then, the same LLM provides feedback for its output.\n- -Then, the same LLM uses the feedback to refine itself, iteratively.\nsame model M togetfeedback(‚ë†).Thefeedbackis passed backtoM,which refines thepreviously generated output (?). Steps (‚ë†) and (‚ë°) iterate until a stopping condition is met. SELF-REFINE is instantiatedwithalanguagemodel such as GPT-3.5anddoesnotinvolvehuman assistance.", "prev_chunk_id": "ALecture14_Reasoning__0017", "next_chunk_id": "ALecture14_Reasoning__0019"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0019", "chunk_index": 19, "page_numbers": [20], "text": "- -(Welleck et al., ICLR 2023)\n- -LLMs often meet most task requirements, but can miss a few, and need to start again from scratch.\n- -A more natural, intuitive approach could be leveraging the generation as a starting point and refining it into higher quality output:\n- o Generator produces a reasonable initial hypothesis\n- o Corrector is trained to make up the difference between the initial hypothesis and the optimal solution.\n- o The corrector can be applied multiple times.", "prev_chunk_id": "ALecture14_Reasoning__0018", "next_chunk_id": "ALecture14_Reasoning__0020"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0020", "chunk_index": 20, "page_numbers": [21], "text": "- -Exploration\n- -Pairing\n- -Learning\n- -Re-Exploration", "prev_chunk_id": "ALecture14_Reasoning__0019", "next_chunk_id": "ALecture14_Reasoning__0021"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0021", "chunk_index": 21, "page_numbers": [22], "text": "- -(Yao et al., NeurIPS 2023)\n- -Falls short in tasks that require exploration, lookahead etc.\n- -Autoregressive LLMs are still restricted to tokenlevel, left to right decision-making processes.\n- -Problem of 'Hallucination Snowballing' (Zhang et al., ICML 2024)\n- -Existing methods do not explore different continuations of a thought process\n- -We may benefit from a more deliberate \"System 2\" planning process that:\n- -Evaluates its current status and actively looks ahead or backtracks to make more global decisions.\n- -Maintains and explores diverse alternatives", "prev_chunk_id": "ALecture14_Reasoning__0020", "next_chunk_id": "ALecture14_Reasoning__0022"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0022", "chunk_index": 22, "page_numbers": [23], "text": "- -Allows LMs to explore multiple reasoning paths over thoughts\n- -Frames problems as search over a tree\n- -Each node is a state s = [ x, z 1..i ] representing a partial solution\n- -4 Questions:\n1. How to decompose intermediate process into thought steps?\n2. How to generate potential thoughts from each stage?\n3. How to heuristically evaluate states?\n4. What search algorithm to use?", "prev_chunk_id": "ALecture14_Reasoning__0021", "next_chunk_id": "ALecture14_Reasoning__0023"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0023", "chunk_index": 23, "page_numbers": [24], "text": "- -Thought Decomposition : Splits problem into intermediate steps or thoughts depending on the task.\n- -Thought Generator : Given a state, sample or propose thoughts depending on the task (two different prompts).\n- -State Evaluator : Evaluates progress a state makes towards solving a problem.\n- -Which states do we keep exploring?\n- -What order to explore in?\n- -Search : Breadth-first versus depth-first", "prev_chunk_id": "ALecture14_Reasoning__0022", "next_chunk_id": "ALecture14_Reasoning__0024"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0024", "chunk_index": 24, "page_numbers": [25], "text": "- -(Qi et al., EMNLP 2023)\n- -CoT is confined by its single-pass and sequential generation process, and errors accumulate (again, the snowballing effect).\n- -CoT also lacks the ability to refine an already generated reasoning path.\n- -Humans adopt recursive thinking when tackling complex reasoning problems:\n- o Iteratively break the original problem into approachable subproblems and\n- o Aggregating their answers to resolve the original one.\n- -SOCRATIC QUESTIONING, a divide-and-conquer algorithm, mimicking the humans' recursive thinking process.", "prev_chunk_id": "ALecture14_Reasoning__0023", "next_chunk_id": "ALecture14_Reasoning__0025"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0025", "chunk_index": 25, "page_numbers": [26], "text": "Question: A ball slides down a landslide of height h, the length of the landslide is l, ball math is m, and the angle between the landslide and the ground is 0. Find the velocity v of the ball at the landslide bottom. Ignore friction force and air resistance.", "prev_chunk_id": "ALecture14_Reasoning__0024", "next_chunk_id": "ALecture14_Reasoning__0026"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0026", "chunk_index": 26, "page_numbers": [26], "text": "<!-- formula-not-decoded -->", "prev_chunk_id": "ALecture14_Reasoning__0025", "next_chunk_id": "ALecture14_Reasoning__0027"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0027", "chunk_index": 27, "page_numbers": [26], "text": "The kinetic energy theorem states that the net work done on an object equals its change in kinetic energy. In this case, the net work done is done by the frictional force. The work done by friction can be calculated as W = f * I, where I is the landslide length and f= Œº * N. Write the equation as: Œº * m * cos(0) * 1 = (1/2) * m * v^2. Solve that equation, v = (1 / (I + (e)s0o  8   + Z))ubs", "prev_chunk_id": "ALecture14_Reasoning__0026", "next_chunk_id": "ALecture14_Reasoning__0028"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0028", "chunk_index": 28, "page_numbers": [26], "text": "I do not know.\n1. Does this problem obey the energy conservation law?\n2. What is the mechanical energy of the initial state?\n3. What is the mechanical energy in the final state?\n1. Because there is no extermal force doing work, so this question follows engergy conservation law.\n3. At the end, ball's height is O, so the potential energy is 0. And kinetic energy is 1/2 * m * v2.\n2. At begining, ball's velocity is 0, so its kinetic energy is O. And the potential energy is mg*h.\nAgain, find the velocity V of the ball at the bottom of the landslide. Ignore friction force.\nSolve 0 + mg * h = 1/2 * m * v¬≤ + 0, we get v= sqrt(2 *g* h).", "prev_chunk_id": "ALecture14_Reasoning__0027", "next_chunk_id": "ALecture14_Reasoning__0029"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0029", "chunk_index": 29, "page_numbers": [27], "text": "- -Top-down exploration process (in red): the original complex problem is decomposed into simpler or related sub-problems until the sub-problems can be solved\n- -SELF-QUESTIONING proactively raises and answers questions that are essential to the target question.\n- -Bottom-up backtracking process (in green): the solutions to the sub-problems are returned and selectively used to solve the original problem.\n- -SOCRATIC QUESTIONING recursively backtracks and tailors the intermediate thoughts acquired from SELFQUESTIONING until reaching an answer to the original input question.", "prev_chunk_id": "ALecture14_Reasoning__0028", "next_chunk_id": "ALecture14_Reasoning__0030"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0030", "chunk_index": 30, "page_numbers": [29], "text": "- -(Du et al., Preprint, 2023)\n- -Inspiration from 'Society of Mind' (Minsky, 1988): A conceptual framework for how complex intelligent behavior can emerge from the interactions of many simple components (Minsky called these agents), rather than a central controller.\n- -Given a query, multiple instances of a language model generate individual candidate answers to a query.\n- -Then each individual model instance reads and critiques the responses of all other models and uses this content to update its own answer.\n- ‚Üí repeated over several rounds.", "prev_chunk_id": "ALecture14_Reasoning__0029", "next_chunk_id": "ALecture14_Reasoning__0031"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0031", "chunk_index": 31, "page_numbers": [30], "text": "User Input: A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?", "prev_chunk_id": "ALecture14_Reasoning__0030", "next_chunk_id": "ALecture14_Reasoning__0032"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0032", "chunk_index": 32, "page_numbers": [30], "text": "Agent 1: Let x be the number of rubies. Then the number of diamonds is 175. And the number of emeralds is 2(x-35), since there are twice as many emeralds as rubies and there are 35 fewer rubies than diamonds.\nTo find the total number of gems, we add the number of diamonds, rubies, and emeralds: Total gems = 175 + x + 2(x-35) = 3x + 105.\nWe don't know the exact value of x, but we do know that it represents the number of rubies, which is less than 175 (since there are 35 fewer rubies than diamonds). Let's try some values of x: If x = 50, then the total gems = 3(50) + 105 = 255. If x = 100, then the total gems = 3(100) + 105 = 405.\nSince we know there were fewer rubies than diamonds, the answer must be closer to 255 than to 405. After some trial and error, we can find that x = 40 gives us: Total gems = 3(40) + 105 = 225.\nAgent 2: If there were 175 diamonds, then the number of rubies is 35 fewer, or 175 - 35 = 140 rubies.\nAnd since there were twice as many emeralds as rubies, we can represent the number of emeralds as 2 * 140 = 280 emeralds.\nTo find the total number of gems in the chest, we can add up the number of diamonds, rubies, and emeralds: 175 + 140 + 280 = 595. Therefore, there were 595 gems in the chest.\nUser Input: Using the solutions from other agents as additional information, can you give an updated response.. (Copy Agent 2's reply) (Copy Agent 1's reply)", "prev_chunk_id": "ALecture14_Reasoning__0031", "next_chunk_id": "ALecture14_Reasoning__0033"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0033", "chunk_index": 33, "page_numbers": [31], "text": "Agent 1: Given the information provided in other agents' solutions, we have two answers: One agent found that the number of rubies is 40 and the total number of gems is 225. Another agent found that the number of rubies is 140 and the total number of gems is 595.\nWe know that the number of rubies is 35 fewer than the number of diamonds, which is 175. So the number of rubies should be 175 - 35 = 140, as the second agent found. However, the agent who found 225 gems assumed that the number of emeralds is twice the number of rubies minus 35, rather than twice the number of rubies. If we use this assumption, we get: Number of rubies = 140. Number of emeralds = 2 * 140 - 35 = 245. Total number of gems = 175 + 140 + 245 = 560. Our final answer is 560.\nUser Input: Using the solutions from other agents as additional information, can you give an updated response... (Copy Agent 2's reply) (Copy Agent 1's reply)", "prev_chunk_id": "ALecture14_Reasoning__0032", "next_chunk_id": "ALecture14_Reasoning__0034"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0034", "chunk_index": 34, "page_numbers": [31], "text": "Agent 1l: After reviewing the solutions provided by other agents, I agree with the second agent who found that the number of rubies is 140 and the total number of gems in the chest is 595. Therefore, my answer to the math problem is 595.\nAgent 2: After reviewing the solutions provided by other agents, I agree with the second agent who found that the number of rubies is 140 and the total number of gems in the chest is 595. Therefore, my answer to the math problem is 595.\nAgent 2: After reviewing the solutions provided by other agents, I also agree that the number of rubies is 140 and the total number of gems in the chest is 595. Therefore, my answer to the math problem is also 595.", "prev_chunk_id": "ALecture14_Reasoning__0033", "next_chunk_id": "ALecture14_Reasoning__0035"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0035", "chunk_index": 35, "page_numbers": [32], "text": "- -(Liang et al., EMNLP, 2024)\n- -The distorted thinking of one LLM can be corrected by the others\n- -Multiple agents express their arguments, and a judge manages the debate process to obtain a final solution.\n- -The resistance to change of one LLM will be complemented by the others, and\n- -Each agent can obtain external feedback from the others.\nFigure 2: Framework of Multi-Agent Debate. Here we designate the devil (Ôºâas the affirmative side while the angel () as the negative side. We want the angel to correct the devil's mistakes.", "prev_chunk_id": "ALecture14_Reasoning__0034", "next_chunk_id": "ALecture14_Reasoning__0036"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0036", "chunk_index": 36, "page_numbers": [33], "text": "```\nAlgorithm 1 MAD: Multi-Agents Debate Require: Debate topic t, maximum number of rounds M and number of debaters N Ensure: Final answer a 1: procedure MAD(t, M, N) 2: J > Initialize the judge 3: D ‚Üê [D1,¬∑¬∑:,Dv]  Initialize debaters 4: [‚ÜíH > Initialize debate history 5: m‚Üê0 Current round 6: while m ‚â§ M do 7: m‚Üêm+1 8: for each D in D do 9: h ‚Üê D(HÔºâ Generate argument 10: H‚ÜêH+[h] 1 Append h to H 11: if Ja(H) then 12: break > Debate is over 13: Œ±‚ÜêJe(H) Extract the final answer 14: return a\n```\n- -Debaters generate turns in a roundrobin fashion.\n- -Judge decides whether the debate should be over (or max number of rounds is reached)\n- -Judge makes the final decision.", "prev_chunk_id": "ALecture14_Reasoning__0035", "next_chunk_id": "ALecture14_Reasoning__0037"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0037", "chunk_index": 37, "page_numbers": [34], "text": "- -(Qi et al., EMNLP 2023)\n- -Uses multiple diverse LLM agents (such as different model families)\n- -E.g., ChatGPT, Bard, Claude2.\n- -Proceeds through multiple rounds of discussion, where each agent provides an answer and explanation based on other agents' prior round outputs.\n- -Each agent also provides a confidence score.\n- -A weighted voting / consensus mechanism produces the final answer.\n- -Aims to converge to a better answer via collaboration, persuasion among agents, rather than strictly adversarial debating.", "prev_chunk_id": "ALecture14_Reasoning__0036", "next_chunk_id": "ALecture14_Reasoning__0038"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0038", "chunk_index": 38, "page_numbers": [36], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0037", "next_chunk_id": "ALecture14_Reasoning__0039"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0039", "chunk_index": 39, "page_numbers": [37], "text": "- -Existing work on reasoning evaluation mainly focused on end-task performance\n- -Are these the same?\n- -Finer-grained metrics can help us assess which reasoning steps support the accurate final answers.\n- -Outcome reward models (ORMs) that determine reward based on the final outcome vs.\n- -Process reward models (PRMs) that evaluate individual reasoning steps to determine a reward.", "prev_chunk_id": "ALecture14_Reasoning__0038", "next_chunk_id": "ALecture14_Reasoning__0040"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0040", "chunk_index": 40, "page_numbers": [38], "text": "- -(Golovneva et al., ICLR 2023)\n- -Aimed at assessing the quality of reasoning chains produced by language models.\n- -Reference-free metrics: do not require manual ground-truth annotations.\n- -Unsupervised metrics: do not require training data annotations.\nTable 1: Step-by-step reasoning for logical inference task generated from 2-shot GPT-3 (text-davinci-002) (Brown et al., 2020). Error types are highlighted: logical or factual issues,partially correct, somewhat-relevant. Full 2-shot exemplars are in Appendix B.\nContext: Shelley is from Virginia, but is visiting that city with that famous market where they throw the fish. Going home next Tuesday!\nQuestion: Is it likely that Shelly will be near the Pacific Ocean this weekend?\nGPT-3Step-by-stepreasoning:\n- ‚óèNo, it is not likely that Shelley will be near the Pacific Ocean this weekend.\n- ‚óèShelley is from Virginia,which is on the East Coast of the United States.\n- ¬∑The Pacific Ocean is on the West Coast.\n- ‚óèShelley is visiting a city with a famous market, which could be anywhere in the world.\n- ‚óèTuesday is her travel day, so she will likely not be traveling this weekend.", "prev_chunk_id": "ALecture14_Reasoning__0039", "next_chunk_id": "ALecture14_Reasoning__0041"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0041", "chunk_index": 41, "page_numbers": [39], "text": "- -Grouped fine-grained metrics under four perspectives:\n- -semantic alignment: how well each step in a model's reasoning chain is semantically aligned with what has come before, i.e., the original problem and the preceding reasoning steps.\n- -semantic similarity: how internally consistent and semantically coherent the steps are with each other, without necessarily conditioning on the task or input.\n- -logical inference: whether each step in the reasoning chain follows from the previous steps in a logically valid way, i.e., does the inference structure make sense?\n- -language coherence: the form and fluency of the reasoning chain, i.e., is it composed of well-formed natural language that a human can follow smoothly.\n- -A set of metrics based on token embedding similarities, perplexity, etc.", "prev_chunk_id": "ALecture14_Reasoning__0040", "next_chunk_id": "ALecture14_Reasoning__0042"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0042", "chunk_index": 42, "page_numbers": [40], "text": "- -(Prasad et al., EMNLP 2023)\n- -Measures two key properties (also reference-free):\n- -correctness, i.e., each  step makes a valid inference based on information contained within the step, preceding steps,  and input context\n- -Uses NLI models\n- -informativeness, i.e.,  each step provides new information that is helpful towards deriving the generated answer\n- -Uses information theoretic metrics, such as pointwise V-information.\nContext: The moon is a kind of moon. Earth is a kind of planet. Moons orbit planets. Gravity causes orbits. Question: What keeps the Moon orbiting Earth?\nModel-generated Step-by-Step Rationales:\n- Step l: [Moon is a kind of moon] and [earth is a kind of planet], so [the moon and earth are planets].\n- Step 2: [Gravity causes orbits], so [gravity causes moon to orbit earth].\nAnswer: Earth's gravity.\nFigure 1: Model-generated step-by-step reasoning from Entailment Bank (Dalvi et al., 2021). Reasoning errors include: incorrect step inference (requires inferring uoou, orbits earth'), and incorrect inference and uninformative ('moonisa planet' does not help answer the question). Reasoning Content Units (RCUs) are shown as ^[.]'.", "prev_chunk_id": "ALecture14_Reasoning__0041", "next_chunk_id": "ALecture14_Reasoning__0043"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0043", "chunk_index": 43, "page_numbers": [41], "text": "- -(Mukherjee et al., ICML 2025)\n- -Prompts an LLM to identify premises and their relationships.\nFigure 1. Comparison between a Linear Reasoning Chain (LRC) and our proposed PARC (Premise-Augmented Reasoning Chain). The LRC (left), is linear and there no explicit premise link between steps. In PARC (right), premise links are explicitly established, enabling better identification of correct and incorrect steps.Accumulation errors can be traced back to faulty premises. Establishing these premises helps improve error detection with LLMs.", "prev_chunk_id": "ALecture14_Reasoning__0042", "next_chunk_id": "ALecture14_Reasoning__0044"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0044", "chunk_index": 44, "page_numbers": [42], "text": "- -Verifies reasoning steps based on its premises.\n- -Types of errors:\n- -A native error if it contains an inherent discrepancy (e.g., a miscalculation or logical inconsistency)\n- -An accumulation error if a given step  is logically valid, but at least one of its premises is incorrect.\nFigure 2. An example where the baseline method fails to detect errors, while our verification method with established premise links successfully identifies the mathematical error in step 6, and the accumulation error in step7.", "prev_chunk_id": "ALecture14_Reasoning__0043", "next_chunk_id": "ALecture14_Reasoning__0045"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0045", "chunk_index": 45, "page_numbers": [43], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0044", "next_chunk_id": "ALecture14_Reasoning__0046"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0046", "chunk_index": 46, "page_numbers": [44], "text": "- -Slow, deliberate, effortful, and conscious mode of reasoning\n\"Systems 1and 2 are both active whenever we are awake. System 1 runs automatically and System 2is normally in a comfortablelow-effort mode, in which only a fraction of its capacity is engaged. System 1 continuously generates suggestions for System 2:impressions, intuitions,intentions,and feelings.If endorsed by System 2, impressions and intuitions turn into beliefs, and impulses turn into voluntary actions. When all goes smoothly, which is most of the time, System 2 adopts the suggestions of System 1 with little or no modification. You generally believe your impressions and act on your desires, and that is fine‚Äîusually.\"", "prev_chunk_id": "ALecture14_Reasoning__0045", "next_chunk_id": "ALecture14_Reasoning__0047"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0047", "chunk_index": 47, "page_numbers": [45], "text": "- -(Snell et al., ICLR 2025)\n- -Can we enable an LM to make use of additional computation at test time to improve the accuracy of its response?\n- -Test-time scaling vs. scaling model parameters\n- -Two mechanisms:\n- -Process-reward model (PRM) which produces a prediction of correctness of each step in a solution (rather than only the final answer) and (tree) search over the space of solutions\n- -Fine-tune models to iteratively refine their answers in complex-reasoning based settings.\n- -A 'compute -optimal' scaling strategy, which acts to  most effectively allocate test-time compute adaptively per prompt", "prev_chunk_id": "ALecture14_Reasoning__0046", "next_chunk_id": "ALecture14_Reasoning__0048"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0048", "chunk_index": 48, "page_numbers": [46, 47], "text": "- -(Muennighoff et al., Preprint, 2025)\n- -S-1K 1,000 {question, reasoning trace} pairs relying on: difficulty, diversity, and quality\n- -Budget forcing - control test-time compute by forcefully terminating the model's thinking process or appending 'Wait'\n- -Recipe\n- -Train model on S1k with SFT\n- -Budget force during inference\nPS: It is an ongoing debate whether thinking more always helps ‚ò∫\nOptimalThinkingBench: E Evaluating Over and Underthinking in LLMs\nYou can go through the papers and form your opinion (it is fun)\nDoes Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "prev_chunk_id": "ALecture14_Reasoning__0047", "next_chunk_id": "ALecture14_Reasoning__0049"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0049", "chunk_index": 49, "page_numbers": [48], "text": "- -(DeepSeek AI, Preprint 2025)\n- -DeepSeek-R1-Zero\n- -Trains model with RL\n- -Rule-based rewards for task completion, formatting.\nFigure from: https://x.com/SirrahChan/status/1881540279783887036", "prev_chunk_id": "ALecture14_Reasoning__0048", "next_chunk_id": "ALecture14_Reasoning__0050"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0050", "chunk_index": 50, "page_numbers": [49, 50], "text": "Figure3|The averageresponselength of DeepSeek-R1-Zero on the trainingset during theRL process.DeepSeek-R1-Zero naturallylearns to solvereasoning tasks with more thinking time.\nSteps\nFigure 2|AIME accuracy of DeepSeek-R1-Zeroduring training.Foreach question,we sample 16 responses and calculate theoverall average accuracy to ensure a stableevaluation.\n- -Model discovers long reasoning patterns they refer to as 'aha moments'\n```\nQuestion: If a > 1, then the sum of the real solutions of a - Va + x = x is equal to Response: <think> To solve the equation ‚àöa - Va + x = x, let's start by squaring both ... x+D-D~ =x¬≤ =‚Üí a-Va+x =x2. Rearrange to isolate the inner square root term: (a-x2)2=Œ±+x =‚Üí a¬≤ -2ax¬≤+(x2)2 =a+x =‚Üí x4-2ax¬≤ -x+(a¬≤ -a) = 0 Wait, wait. Wait. That's an aha moment I can flag here. Let's reevaluate this step-by-step to identify if the correct sum can be ¬∑.. We started with the equation: ‚àöa-‚àöa+x=x First, let's square both sides: Œ±-Va+x=x¬≤ =‚Üí ‚àöa+x=Œ±-x¬≤ Next, I could square both sides again, treating the equation: ¬∑ . ...\n```\nTable 3 | An interesting \"aha moment\" of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.", "prev_chunk_id": "ALecture14_Reasoning__0049", "next_chunk_id": "ALecture14_Reasoning__0051"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0051", "chunk_index": 51, "page_numbers": [51], "text": "- -Problem: the reasoning chains are not always human readable (e.g., weird tokens)\n- -Solution: Fine-tune the base model with long reasoning chains and SFT.\n- -Then apply RL\nFigure from: https://x.com/SirrahChan/status/1881540279783887036", "prev_chunk_id": "ALecture14_Reasoning__0050", "next_chunk_id": "ALecture14_Reasoning__0052"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0052", "chunk_index": 52, "page_numbers": [52], "text": "- -Then distills reasoning behaviors into smaller models through SFT\n- -Fun extra info: GRPO had a length bias which caused the model to generate longer reasoning chains ( Marjonaviƒá et al., Preprint 2025)\nFigure from: https://x.com/SirrahChan/status/1881540279783887036", "prev_chunk_id": "ALecture14_Reasoning__0051", "next_chunk_id": "ALecture14_Reasoning__0053"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0053", "chunk_index": 53, "page_numbers": [53], "text": "- -Introduction and facets of reasoning\n- -Before LLM Reasoning and CoT\n- -Prompting for Reasoning\n- -Single Agent\n- -Multi-Agent\n- -Reasoning Evaluation\n- -Training of reasoning models\n- -Reasoning Efficiency", "prev_chunk_id": "ALecture14_Reasoning__0052", "next_chunk_id": "ALecture14_Reasoning__0054"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0054", "chunk_index": 54, "page_numbers": [54], "text": "- -Big progress in accuracy with reasoning.\n- -The autoregressive paradigm imposes significant burden, limiting the application of these methods.\n- -LLMs often exhibit excessive reasoning, with several redundant wordings.\n- -Mechanisms to reduce these costs, such as using external mechanisms to orchestrate between reasoning and not reasoning models.", "prev_chunk_id": "ALecture14_Reasoning__0053", "next_chunk_id": "ALecture14_Reasoning__0055"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0055", "chunk_index": 55, "page_numbers": [55], "text": "- -TokenSkip (Xia et al., EMNLP 2025)\n- -Not all the tokens in the chain-ofthought reasoning steps are important.\n- -Identifies more important tokens and fine-tunes the model to generate shorter reasoning chains.\nFigure 1: In contrast to vanilla CoT that generates all reasoning tokens sequentially, TokenSkip enables LLMs to skip tokens with less semantic importance (e.g., ) and learn shortcuts between critical reasoning tokens, facilitating controllable CoT compression.", "prev_chunk_id": "ALecture14_Reasoning__0054", "next_chunk_id": "ALecture14_Reasoning__0056"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0056", "chunk_index": 56, "page_numbers": [56], "text": "- -Uses GPT-4 to label each token as important or not; and trains a model, M B , with token classification objective.\n<!-- formula-not-decoded -->\n- -The importance of each token, x i , is measured by the probability assigned to that token by this model:\nFigure 4: Mllustration of TokenSkip. During training, TokenSkip first generates CoT trajectories from the target LLM. These CoTs are then compressed to various ratios sampled from the ratio set. TokenSkip fine-tunes the LLM using compressed CoTs with mixed ratios, enabling controllable CoT inference at any desired  E {?o, .- , 7z}.", "prev_chunk_id": "ALecture14_Reasoning__0055", "next_chunk_id": "ALecture14_Reasoning__0057"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0057", "chunk_index": 57, "page_numbers": [57], "text": "- -Trade-off: Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable.\n- -Can one use simpler models/methods for simpler problems and more sophisticated models/methods for more complex problems?", "prev_chunk_id": "ALecture14_Reasoning__0056", "next_chunk_id": "ALecture14_Reasoning__0058"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0058", "chunk_index": 58, "page_numbers": [58, 59], "text": "- -Learning to Route LLMs with Confidence Tokens (Chuang et al., ICML 2025)\n- -LLM confidence is used to determine whether the response from a smaller model can be accepted.\n- -Proposed a method to teach LLMs to express confidence in whether their answers are correct.\n- -Compared this with methods that use token probabilities or verbalizing confidence.\nFigure1:Self-REFattaches a confidence token to each prediction.Routing proceeds based on the confidence.\n- -RouteLLM (Ong et al., ICLR, 2025).\n- -Learns router models that dynamically select between stronger and weaker models during inference.\n- -A lightweight classifier\n- -Objective: minimize inference costs while achieving a specific performance target.\n- -Supervised approach: Evaluates responses from multiple models to determine the 'best' model for each input, trains the router model with this data. (best: cheapest model with the highest value)\n- -Self-improvement: Uses RL to adjust routing over time.", "prev_chunk_id": "ALecture14_Reasoning__0057", "next_chunk_id": "ALecture14_Reasoning__0059"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0059", "chunk_index": 59, "page_numbers": [60], "text": "Part 1: RL basics\nPart 2: RL algorithms\n- -state, action, environment, reward, etc.\n- -PPO, DPO, GRPO\n- -preference learning\nPart 3: RL applications\n- -reasoning", "prev_chunk_id": "ALecture14_Reasoning__0058", "next_chunk_id": "ALecture14_Reasoning__0060"}
{"lecture_id": "ALecture14_Reasoning", "source_pdf": "data/anlp/ALecture14_Reasoning.pdf", "chunk_id": "ALecture14_Reasoning__0060", "chunk_index": 60, "page_numbers": [60], "text": "- -Midterm 2", "prev_chunk_id": "ALecture14_Reasoning__0059", "next_chunk_id": null}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0000", "chunk_index": 0, "page_numbers": [1], "text": "Ishika Agarwal\nCS 546: Advanced NLP", "prev_chunk_id": null, "next_chunk_id": "ALecture15_RLinLLMs (1)__0001"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0001", "chunk_index": 1, "page_numbers": [3], "text": "1. Pre-trained model (learns how to speak)\n2. Instruction-tuned model (learns how to be useful)\n- a. SFT\n3. Generalizable/task specific models\n- a. RL", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0000", "next_chunk_id": "ALecture15_RLinLLMs (1)__0002"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0002", "chunk_index": 2, "page_numbers": [4], "text": "- Learning from positive/negative reinforcement", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0001", "next_chunk_id": "ALecture15_RLinLLMs (1)__0003"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0003", "chunk_index": 3, "page_numbers": [5], "text": "- I. Basics\n- II. Common RL Algorithms\n3. III.RL Applications", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0002", "next_chunk_id": "ALecture15_RLinLLMs (1)__0004"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0004", "chunk_index": 4, "page_numbers": [6], "text": "- I. Terminology\n- II. Bellman equations\n- III. Policy gradient\n- II. Common RL Algorithms\n5. III.RL Applications", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0003", "next_chunk_id": "ALecture15_RLinLLMs (1)__0005"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0005", "chunk_index": 5, "page_numbers": [7, 8, 9, 11, 12, 13, 14, 15], "text": "Suppose you are training your cat‚Ä¶\nYour cat is an agent !\nYour cat is an agent! And its state is sitting.\nWhatever your cat does next is an action .\nWhatever your cat does next is an action. It will sample its next action from its policy.\nYour cat makes the action of rolling over.\nCat receives a positive reward . It realizes that next time, it should roll over with higher likelihood.\nYour cat will take a series of actions: a trajectory .", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0004", "next_chunk_id": "ALecture15_RLinLLMs (1)__0006"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0006", "chunk_index": 6, "page_numbers": [16], "text": "Agent, 1 = The learner or decision maker. Environment, 1 = The (contained) area the agent makes decisions in. State, 1 = The representation of the current situation. Action, 1 = The choice the agent makes. Reward, 1 = The feedback the agent receives. Policy, 1 = The current mapping from states to actions. Trajectory, 1 = A set of actions", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0005", "next_chunk_id": "ALecture15_RLinLLMs (1)__0007"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0007", "chunk_index": 7, "page_numbers": [17], "text": "Environment, The learner or decision maker = The (contained) area the agent makes decisions in. Environment, Language model = Language. State, The learner or decision maker = The representation of the current situation. State, Language model = The current context of the model ( ùë† ‡Æ¥‡Øß ). Action, The learner or decision maker = The choice the agent makes. Action, Language model = The token ùë† ‡Øß generated at index ùë°. Reward, The learner or decision maker = The feedback the agent receives. Reward, Language model = Outcome or process reward. Policy, The learner or decision maker = The current mapping from states to actions. Policy, Language model = Language model. Trajectory, The learner or decision maker = A set of actions. Trajectory, Language model = Also, ùë† ‡Æ¥‡Øß", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0006", "next_chunk_id": "ALecture15_RLinLLMs (1)__0008"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0008", "chunk_index": 8, "page_numbers": [18], "text": "Two things that make RL work (and difficult‚Ä¶):\n1. Q function\n‚Ä¢\nWhat is the expected reward of taking action\n‚Ä¢\n‚Ä¢\nùëÑ‡∞ó(ùë†, ùëé)\nùëé\nin state\nùë†\n?\n: the current reward plus the expected reward of trajectory\nùëÑ‡∞ó\nùë†, ùëé\n= ùëü ùë†, ùëé\n2. Value function\n‚Ä¢\nWhat is the expected reward of being in state\n‚Ä¢\n‚Ä¢\n: the average reward of the trajectory\nùëâ ‡∞ó (ùë†)\nùëâ ‡∞ó\nùë†\n‡Øî ùúã ùëé ùë† [ùëü ùë†, ùëé +ùõæ ‚àë\n= ‚àë\n‡Ø¶ ·á≤\nùëÉ ùë† ·á±\nùëé, ùë†\nùëâ ‡∞ó\n(ùë† ·á±\n)\n+ùõæ‚àë ùëÉ(ùë† ·á± |ùëé, ùë†)\n‡Ø¶·á±\n‚àë\n‡Øî·á±\nùúã ùëé ·á±\nùë† ·á±\nùëÑ‡∞ó(ùë† ·á±\n, ùëé\nùë†\n·á±\n?\n)", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0007", "next_chunk_id": "ALecture15_RLinLLMs (1)__0009"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0009", "chunk_index": 9, "page_numbers": [19], "text": "- What is the expected reward of taking action ùëé in state ùë† ?\n- ùëÑ‡∞ó(ùë†, ùëé) : the current reward plus the expected reward of trajectory\n- ùëÑ‡∞ó ùë†, ùëé = ùëü ùë†, ùëé +ùõæ‚àë ùëÉ(ùë† ·á± |ùëé, ùë†) ‡Ø¶·á± ‚àë ùúã ùëé ·á± ùë† ·á± ùëÑ‡∞ó(ùë† ·á± , ùëé ·á± ) ‡Øî·á±", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0008", "next_chunk_id": "ALecture15_RLinLLMs (1)__0010"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0010", "chunk_index": 10, "page_numbers": [19], "text": "‚Ä¢\nWhat is the expected reward of being in state\n‚Ä¢\n‚Ä¢\n: the average reward of the trajectory\nùëâ ‡∞ó (ùë†)\nùëâ ‡∞ó\nùë†\n‡Øî ùúã ùëé ùë† [ùëü ùë†, ùëé +ùõæ ‚àë\n= ‚àë\n‡Ø¶ ·á≤\nùëÉ ùë† ·á±\nùëé, ùë†\nùëâ ‡∞ó\n(ùë† ·á±\n)\nhttps://lilianweng.github.io/posts/2018-02-19-rl-overview/\nùë†\n?", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0009", "next_chunk_id": "ALecture15_RLinLLMs (1)__0011"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0011", "chunk_index": 11, "page_numbers": [20], "text": "- What is the expected reward of taking action ùëé in state ùë† ?\n- ùëÑ‡∞ó(ùë†, ùëé) : the current reward plus the expected reward of trajectory\n- ùëÑ‡∞ó ùë†, ùëé = ùëü ùë†, ùëé +ùõæ‚àë ùëÉ(ùë† ·á± |ùëé, ùë†) ‡Ø¶·á± ‚àë ùúã ùëé ·á± ùë† ·á± ùëÑ‡∞ó(ùë† ·á± , ùëé ·á± ) ‡Øî·á±", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0010", "next_chunk_id": "ALecture15_RLinLLMs (1)__0012"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0012", "chunk_index": 12, "page_numbers": [20], "text": "- What is the expected reward of being in state ùë† ?\n- ùëâ ‡∞ó (ùë†) : the average reward of the trajectory\n- ùëâ ‡∞ó ùë† = ‚àë ‡Øî ùúã ùëé ùë† [ùëü ùë†, ùëé +ùõæ ‚àë ùëÉ ùë† ·á± ùëé, ùë† ‡Ø¶ ·á≤ ùëâ ‡∞ó (ùë† ·á± )\nhttps://davidstarsilver.wordpress.com/wpcontent/uploads/2025/04/lecture-4-model-freeprediction-.pdf", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0011", "next_chunk_id": "ALecture15_RLinLLMs (1)__0013"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0013", "chunk_index": 13, "page_numbers": [21, 22], "text": "- In RL we want to maximize the expected return:\n<!-- formula-not-decoded -->\n- The policy gradient comes out to be:\n<!-- formula-not-decoded -->\n- ‚àá‡∞è log ùúã ‡∞è ùëé‡Øß ùë† ‡Øß ‚Üí where do I go to make this action more likely?\n- ùëÖ œÑ ‚Üí how good is this trajectory?\n- In RL we want to maximize the expected return:\n<!-- formula-not-decoded -->\n- The policy gradient comes out to be:\n<!-- formula-not-decoded -->\n- ‚àá‡∞è log ùúã ‡∞è ùëé‡Øß ùë† ‡Øß ‚Üí where do I go to make this action more likely?\n- ùëÖ œÑ ‚Üí how good is this trajectory?\n- Intuition check!! If there are good actions with low probability, the policy gradient will be‚Ä¶?\n- Bigger or smaller?", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0012", "next_chunk_id": "ALecture15_RLinLLMs (1)__0014"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0014", "chunk_index": 14, "page_numbers": [23], "text": "- I. Terminology\n- II. Bellman equations\n- III. Policy gradient", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0013", "next_chunk_id": "ALecture15_RLinLLMs (1)__0015"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0015", "chunk_index": 15, "page_numbers": [23], "text": "- I. PPO\n- II. DPO\n- III. GRPO", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0014", "next_chunk_id": "ALecture15_RLinLLMs (1)__0016"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0016", "chunk_index": 16, "page_numbers": [24, 25], "text": "- Receive a reward at the end of the entire trajectory\n<!-- formula-not-decoded -->\n- Once you and your cat are done playing for an hour, the cat receives a treat/no treat\n- Receive a reward at the end of the entire trajectory\n<!-- formula-not-decoded -->\n- Once you and your cat are done playing for an hour, the cat receives a treat/no treat\n- Problems:\n- Rewards are very sparse!\n- Can we give rewards for each action instead‚Ä¶?", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0015", "next_chunk_id": "ALecture15_RLinLLMs (1)__0017"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0017", "chunk_index": 17, "page_numbers": [26, 27, 28], "text": "- Approximate the reward at each step + future steps:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- ùê¥ ùë†‡Øß , ùëé ‡Øß : advantage function\n- Tells you how much better ùëé‡Øß is for ùë† ‡Øß compared to other actions\n- ùëü ‡Øß (ùúÉ) is NOT reward, it is a ratio: ùëü ‡Øß ùúÉ = ‡∞ó‡¥á(‡Øî‡≥ü|‡Ø¶ ‡≥ü ) (‡Øî‡≥ü |‡Ø¶ ‡≥ü )\n<!-- formula-not-decoded -->\n- Measures how drastically the new policy is changing compared to the old one\n‚Ä¢\nClipping ensures the model doesn't stray too far from the previous model\nProblems:\n‚Ä¢\n‚Ä¢\nRewards are very sparse!\nCan we give rewards for each action instead‚Ä¶?\n‚Ä¢\n- Approximate the reward at each step + future steps:\n<!-- formula-not-decoded -->\n- ùê¥ ùë†‡Øß , ùëé ‡Øß : advantage function\n- Tells you how much better ùëé‡Øß is for ùë† ‡Øß compared to other actions\n- ùëü ‡Øß (ùúÉ) is NOT reward, it is a ratio: ùëü ‡Øß ùúÉ = ‡∞ó‡¥á(‡Øî‡≥ü|‡Ø¶ ‡≥ü ) ‡∞ó‡¥á ‡≥ö‡≥ó‡≥è (‡Øî‡≥ü |‡Ø¶ ‡≥ü )\n- Measures how drastically the new policy is changing compared to the old one\n- Clipping ensures the model doesn't stray too far from the previous model\nProblems:\n‚Ä¢\n‚Ä¢\nRewards are very sparse!\nCan we give rewards for each action instead‚Ä¶?\n‚Ä¢\n- Approximate the reward at each step + future steps:\n<!-- formula-not-decoded -->\n- ùê¥ ùë†‡Øß , ùëé ‡Øß : advantage function\n- Tells you how much better ùëé‡Øß is for ùë† ‡Øß compared to other actions\n- ùëü ‡Øß (ùúÉ) is NOT reward, it is a ratio: ùëü ‡Øß ùúÉ = ‡∞ó‡¥á(‡Øî‡≥ü|‡Ø¶ ‡≥ü ) ‡∞ó‡¥á ‡≥ö‡≥ó‡≥è (‡Øî‡≥ü |‡Ø¶ ‡≥ü )\n- Measures how drastically the new policy is changing compared to the old one\n- Clipping ensures the model doesn't stray too far from the previous model\nProblems:\n‚Ä¢\n‚Ä¢\nRewards are very sparse!\nCan we give rewards for each action instead‚Ä¶?\n‚Ä¢", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0016", "next_chunk_id": "ALecture15_RLinLLMs (1)__0018"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0018", "chunk_index": 18, "page_numbers": [29, 30, 31], "text": "<!-- formula-not-decoded -->\n- ùê¥ ùë†‡Øß , ùëé\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n- The expected return of taking action ùëé‡Øß minus the expected return of being in state ùë† ‡Øß\n- Intuitively:\n- How much better is action ùëé‡Øß at ùë† ‡Øß compared to other actions\n- For a given trajectory, what if I took action ùëé‡Øß at ùë† ‡Øß instead? Would it be better?\n- Intuition check!! What does it mean when the advantage is‚Ä¶\n- Negative?\n- Zero?\n<!-- formula-not-decoded -->\n- The expected return of taking action ùëé‡Øß minus the expected return of being in state ùë† ‡Øß\n- ùëÑ ùë†‡Øß , ùëé ‡Øß is given ('yay!' when the cat rolls over)\n- ùëâ(ùë†‡Øß ) is estimated with a neural network\n- This could be expensive if the state space gets really large\n<!-- formula-not-decoded -->\n- ùê¥ ùë†‡Øß , ùëé\n<!-- formula-not-decoded -->\n- The expected return of taking action ùëé‡Øß minus the expected return of being in state ùë† ‡Øß\n- ùëÑ ùë†‡Øß , ùëé ‡Øß is given ('yay!' when the cat rolls over)\n- ùëâ(ùë†‡Øß ) is estimated with a neural network\n- This could be expensive if the state space gets really large\n- Can the model autonomously figure out which actions are better than others?\n- Preference pairs\n- Group optimization", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0017", "next_chunk_id": "ALecture15_RLinLLMs (1)__0019"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0019", "chunk_index": 19, "page_numbers": [33], "text": "- Alternative to RLHF:\n<!-- formula-not-decoded -->\n- ùë•, ùë¶ ‡¨æ , ùë¶ ‡¨ø : input, desired response, undesired response\n- ùõΩ controls how far ùúã‡∞è can move from ùúã‡∞è‡±®‡±õ‡±ú\n- Move in the direction of favoring ùë¶ ‡¨æ and away from favoring ùë¶ ‡¨ø\n- Your cat learns to ùë¶ ‡¨æ = 'meow at you' ( ùë¶ ‡¨ø = 'jump at you') when you come home", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0018", "next_chunk_id": "ALecture15_RLinLLMs (1)__0020"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0020", "chunk_index": 20, "page_numbers": [35, 36], "text": "- PPO-style loss over a group of responses\n<!-- formula-not-decoded -->\n- ùê¥ ·àö ùë† ‡Øß , ùëé ‡Øß = ùëÖ‡Øß - ùëÖ ‡¥§\n- Samples ùê∫ completions\n- Scores each of them\n- Advantage of completion ùëñ is the difference in the reward of ùëñ and the mean reward\n- PPO-style loss over a group of responses\n<!-- formula-not-decoded -->\n- ùê¥ ·àö ùë† ‡Øß , ùëé ‡Øß = ùëÖ‡Øß - ùëÖ ‡¥§\n- Samples ùê∫ completions\n- Scores each of them\n- Advantage of completion ùëñ is the difference in the reward of ùëñ and the mean reward", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0019", "next_chunk_id": "ALecture15_RLinLLMs (1)__0021"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0021", "chunk_index": 21, "page_numbers": [37], "text": "- Improves the RL policy without deviating too much (ensuring no performance degradation)", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0020", "next_chunk_id": "ALecture15_RLinLLMs (1)__0022"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0022", "chunk_index": 22, "page_numbers": [37], "text": "- Eliminates the need for an explicit value function (expected rewards)\n- Advantage based on preference pairs", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0021", "next_chunk_id": "ALecture15_RLinLLMs (1)__0023"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0023", "chunk_index": 23, "page_numbers": [37], "text": "- Also eliminates the need for an explicit value function\n- Advantage based on relative rewards within a group", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0022", "next_chunk_id": "ALecture15_RLinLLMs (1)__0024"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0024", "chunk_index": 24, "page_numbers": [38], "text": "- I. Terminology\n- II. Bellman equations\n- III. Policy gradient", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0023", "next_chunk_id": "ALecture15_RLinLLMs (1)__0025"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0025", "chunk_index": 25, "page_numbers": [38], "text": "- I. PPO\n- II. DPO\n- III. GRPO", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0024", "next_chunk_id": "ALecture15_RLinLLMs (1)__0026"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0026", "chunk_index": 26, "page_numbers": [38], "text": "- I. PPO: RLHF\n- II. DPO: Reasoning\n- III. GRPO: Self-Correction", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0025", "next_chunk_id": "ALecture15_RLinLLMs (1)__0027"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0027", "chunk_index": 27, "page_numbers": [39], "text": "Long Ouyang* Jeff Wu* Xu Jiang* Diogo Almeida* Carroll L. Wainwright* Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askellt Peter Welinder Paul Christiano*t Jan Leike* Ryan Lowe* OpenAI", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0026", "next_chunk_id": "ALecture15_RLinLLMs (1)__0028"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0028", "chunk_index": 28, "page_numbers": [43], "text": "Figure 1: Human evaluations of various models on our API prompt distribution, evaluated by how often outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT models (PPO-ptx) as well as its variant trained without pretraining mix (PPO) significantly outperform the GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to those from the 175B GPT-3. Error bars throughout the paper are 95% confidence intervals.", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0027", "next_chunk_id": "ALecture15_RLinLLMs (1)__0029"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0029", "chunk_index": 29, "page_numbers": [44], "text": "```\nChao Dut1, Tianyu Pang', Qian Liu, Wei Gao2, Min Lin1 1Sea AI Lab, Singapore 2School of Computing and Information Systems, Singapore Management University xuanzhang.2020@phdcs.smu.edu.sg; weigao@smu.edu.sg; {duchao, liuqian, tianyupang, linmin}@sea.com\n```", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0028", "next_chunk_id": "ALecture15_RLinLLMs (1)__0030"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0030", "chunk_index": 30, "page_numbers": [49], "text": "LLaMA2-7B,  = LLaMA2-7B. LLaMA2-7B, ToT [8].Acc. (%)‚Üë = LLaMA2-7B. LLaMA2-7B, ToT [8].Latency (s/ins.)‚Üì = LLaMA2-7B. LLaMA2-7B, CoT [!].Acc. (%)‚Üë = LLaMA2-7B. LLaMA2-7B, CoT [!].Latency (s/ins.)‚Üì = LLaMA2-7B. LLaMA2-7B, TS-SFT J.Acc. Latency (%)‚Üë (s/ins.)‚Üì = LLaMA2-7B. LLaMA2-7B, TS-SFT J. = LLaMA2-7B. LLaMA2-7B, CPO (ours).Acc. (%)‚Üë = LLaMA2-7B. LLaMA2-7B, CPO (ours).Latency (s/ins.)‚Üì = LLaMA2-7B. LLaMA2-7B,  = LLaMA2-7B. Question Answering,  = Bam. 2Wiki. Hot.. Question Answering, ToT [8].Acc. (%)‚Üë = 33.6 28.6 23.0. Question Answering, ToT [8].Latency (s/ins.)‚Üì = 1168.4 847.6 1100.7. Question Answering, CoT [!].Acc. (%)‚Üë = 29.6 26.3 21.0. Question Answering, CoT [!].Latency (s/ins.)‚Üì = 37.2 35.7 45.5. Question Answering, TS-SFT J.Acc. Latency (%)‚Üë (s/ins.)‚Üì = 30.4 27.6 22.7. Question Answering, TS-SFT J. = 36.5 35.5 44.8. Question Answering, CPO (ours).Acc. (%)‚Üë = 32.0* 29.7* 24.0*. Question Answering, CPO (ours).Latency (s/ins.)‚Üì = 38.2 35.7 41.1. Question Answering,  = . Fact Verification,  = FVR. FVRS. Vita.. Fact Verification, ToT [8].Acc. (%)‚Üë = 47.3 47.5 50.7. Fact Verification, ToT [8].Latency (s/ins.)‚Üì = 2087.1 2539.5 2639.3. Fact Verification, CoT [!].Acc. (%)‚Üë = 45.8 44.3 47.3. Fact Verification, CoT [!].Latency (s/ins.)‚Üì = 33.8 40.6 35.9. Fact Verification, TS-SFT J.Acc. Latency (%)‚Üë (s/ins.)‚Üì = 47.5 46.0 51.0. Fact Verification, TS-SFT J. = 34.0 40.4 40.1. Fact Verification, CPO (ours).Acc. (%)‚Üë = 53.2* 49.0* 52.7*. Fact Verification, CPO (ours).Latency (s/ins.)‚Üì = 36.8 41.2 40.1. Fact Verification,  = . Arithmetric,  = SVA.. Arithmetric, ToT [8].Acc. (%)‚Üë = 42.7. Arithmetric, ToT [8].Latency (s/ins.)‚Üì = 1861.1. Arithmetric, CoT [!].Acc. (%)‚Üë = 37.7. Arithmetric, CoT [!].Latency (s/ins.)‚Üì = 33.3. Arithmetric, TS-SFT J.Acc. Latency (%)‚Üë (s/ins.)‚Üì = 43.1. Arithmetric, TS-SFT J. = 30.2. Arithmetric, CPO (ours).Acc. (%)‚Üë = 46.0*. Arithmetric, CPO (ours).Latency (s/ins.)‚Üì = 32.1. Arithmetric,  = . AveragePerformance,  = AveragePerformance. AveragePerformance, ToT [8].Acc. (%)‚Üë = 39.1. AveragePerformance, ToT [8].Latency (s/ins.)‚Üì = 1749.1. AveragePerformance, CoT [!].Acc. (%)‚Üë = 36.0. AveragePerformance, CoT [!].Latency (s/ins.)‚Üì = 37.4. AveragePerformance, TS-SFT J.Acc. Latency (%)‚Üë (s/ins.)‚Üì = 38.3. AveragePerformance, TS-SFT J. = 37.4. AveragePerformance, CPO (ours).Acc. (%)‚Üë = 40.9*. AveragePerformance, CPO (ours).Latency (s/ins.)‚Üì = 37.9. AveragePerformance,  = \nTable 1: Experimental results for ToT, CoT, TS-SFT, and our proposed CPO across complex task including question answering, fact verification, and arithmetic reasoning are presented. * mean significantly better than the best baseline (TS-SFT) with p < 0.01. Bold denotes the best methoc and the second best if the top method is ToT.", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0029", "next_chunk_id": "ALecture15_RLinLLMs (1)__0031"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0031", "chunk_index": 31, "page_numbers": [50], "text": "Mihir Prabhudesai* Carnegie Mellon University\nLili Chen Carnegie Mellon University\nAlex Ippoliti' Carnegie Mellon University\nKaterina Fragkiadaki Carnegie Mellon University\nHao Liu Carnegie Mellon University\nDeepak Pathak Carnegie Mellon University", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0030", "next_chunk_id": "ALecture15_RLinLLMs (1)__0032"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0032", "chunk_index": 32, "page_numbers": [52], "text": "- Multiply the probability of a token with its log probability\n- ùëù‡Øß ùë£ log ùëù ‡Øß (ùë£) where ùë£ is a token at positive ùë°\n- Sum across all tokens in the vocabulary at position ùë°\n- ‚àë ùëù‡Øß ùë£ log ùëù ‡Øß (ùë£) ‡Ø©‚àà‡Øè where ùëâ is the vocabulary\n- Most of them will be close to 0\n- Average across tokens in the sequence:\n- ùëÖ ùë¶‡Ø£‡Ø•‡Øò‡Øó = ‡¨µ ‡Øç ‚àë ‚àë ùëù‡Øß ùë£ log ùëù ‡Øß (ùë£) ‡Ø©‚àà‡Øè ‡Øç ‡Øß‡≠Ä‡¨µ\n- This is the final reward function!", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0031", "next_chunk_id": "ALecture15_RLinLLMs (1)__0033"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0033", "chunk_index": 33, "page_numbers": [53], "text": "Figure 3: Accuracy and confidence over the course of training. The trends indicate that accuracy and confidence are indeed highly correlated and therefore it is natural to use confidence as a reward.", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0032", "next_chunk_id": "ALecture15_RLinLLMs (1)__0034"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0034", "chunk_index": 34, "page_numbers": [55], "text": "David Silver, Richard S. Sutton'", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0033", "next_chunk_id": "ALecture15_RLinLLMs (1)__0035"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0035", "chunk_index": 35, "page_numbers": [55], "text": "We stand on the threshold of a new era in artificial intelligence that promises to achieve an unprecedented level of ability. A new generation of agents will acquire superhuman capabilities by learning predominantly from experience. This note explores the key characteristics that will define this upcoming era.", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0034", "next_chunk_id": "ALecture15_RLinLLMs (1)__0036"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0036", "chunk_index": 36, "page_numbers": [56], "text": "- Infinite state and action spaces\n- Human-data to experience-data\n- Human asks a question, agent responds, human gives feedback\n- Agent interacts with its environment and receives signals from the environment", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0035", "next_chunk_id": "ALecture15_RLinLLMs (1)__0037"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0037", "chunk_index": 37, "page_numbers": [57], "text": "- Agents will not solely require on human feedback to improve\n- Example:\n- A health and wellness agent prescribes a diet and exercise regiment\n- Human reward: 'yes this is working' / 'no this isn't working'\n- Experience reward: heart rate, sleep patterns, blood work results, etc.\n- We can achieve agents that are better than humans", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0036", "next_chunk_id": "ALecture15_RLinLLMs (1)__0038"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0038", "chunk_index": 38, "page_numbers": [58], "text": "In the era of experience,\n- Agents will not solely require on human feedback to improve\n- Example:\n- A health and wellness agent prescribes a diet and exercise regiment\n- Human reward: 'yes this is working' / 'no this isn't working'\n- Experience reward: heart rate, sleep patterns, blood work results, etc.\n- We can achieve agents that are better than humans\nMy opinion: easier to hype, harder to adopt; we're living in exciting times!", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0037", "next_chunk_id": "ALecture15_RLinLLMs (1)__0039"}
{"lecture_id": "ALecture15_RLinLLMs (1)", "source_pdf": "data/anlp/ALecture15_RLinLLMs (1).pdf", "chunk_id": "ALecture15_RLinLLMs (1)__0039", "chunk_index": 39, "page_numbers": [59], "text": "Ishika Agarwal\nCS 546: Advanced NLP", "prev_chunk_id": "ALecture15_RLinLLMs (1)__0038", "next_chunk_id": null}
